#!/usr/bin/env python3
"""
FRAMEWORK CRITÃˆRES QUALITÃ‰ MODÃˆLE PANLANG
=========================================

SystÃ¨me de dÃ©finition et mesure des critÃ¨res de qualitÃ© pour le modÃ¨le
PanLang, avec benchmarking automatique et identification des lacunes.

Objectif: DÃ©finir prÃ©cisÃ©ment les qualitÃ©s recherchÃ©es et mesurer
les candidats modÃ¨les pour identifier les meilleures directions d'amÃ©lioration.
"""

import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import numpy as np

@dataclass
class QualityCriterion:
    """DÃ©finition d'un critÃ¨re de qualitÃ©"""
    name: str
    description: str
    measurement_method: str
    target_value: float
    weight: float
    evaluation_function: str
    
@dataclass
class ModelCandidate:
    """Candidat modÃ¨le Ã  Ã©valuer"""
    model_id: str
    description: str
    architecture_features: Dict[str, Any]
    timestamp: str
    
@dataclass  
class QualityAssessment:
    """Ã‰valuation qualitÃ© complÃ¨te d'un candidat"""
    model_id: str
    timestamp: str
    criterion_scores: Dict[str, float]
    overall_score: float
    rank: Optional[int] = None
    strengths: List[str] = None
    weaknesses: List[str] = None
    improvement_priorities: List[str] = None

class PanLangQualityFramework:
    """Framework de critÃ¨res qualitÃ© pour PanLang"""
    
    def __init__(self):
        self.framework_dir = Path("/home/stephane/GitHub/PaniniFS-Research/qualite_framework")
        self.framework_dir.mkdir(exist_ok=True)
        
        self.criteria_file = self.framework_dir / "criteres_qualite.json"
        self.benchmarks_file = self.framework_dir / "benchmarks_modeles.json"
        self.assessments_file = self.framework_dir / "evaluations_modeles.jsonl"
        
        self.quality_criteria = self._define_quality_criteria()
        self.benchmark_models = self._define_benchmark_models()
        
    def _define_quality_criteria(self) -> Dict[str, QualityCriterion]:
        """DÃ©finit critÃ¨res de qualitÃ© recherchÃ©s"""
        
        criteria = {
            # === CRITÃˆRES FONDAMENTAUX ===
            'universalite_linguistique': QualityCriterion(
                name="UniversalitÃ© Linguistique",
                description="CapacitÃ© Ã  reprÃ©senter concepts Ã  travers familles linguistiques diverses",
                measurement_method="Pourcentage couverture familles linguistiques avec mappings cohÃ©rents",
                target_value=0.90,
                weight=0.18,
                evaluation_function="evaluate_linguistic_universality"
            ),
            
            'precision_semantique': QualityCriterion(
                name="PrÃ©cision SÃ©mantique", 
                description="CapacitÃ© Ã  rÃ©soudre ambiguitÃ©s et distinguer nuances sÃ©mantiques",
                measurement_method="Taux rÃ©solution ambiguitÃ©s + prÃ©cision mappings concepts",
                target_value=0.92,
                weight=0.16,
                evaluation_function="evaluate_semantic_precision"
            ),
            
            'coherence_compositionnelle': QualityCriterion(
                name="CohÃ©rence Compositionnelle",
                description="Logique interne composition dhÄtu sans contradictions",
                measurement_method="Score cohÃ©rence rÃ¨gles composition + validitÃ© combinaisons",
                target_value=0.88,
                weight=0.14,
                evaluation_function="evaluate_compositional_coherence"
            ),
            
            # === CRITÃˆRES COGNITIFS ===
            'contraintes_cognitives': QualityCriterion(
                name="Respect Contraintes Cognitives",
                description="ConformitÃ© limites cognitives humaines (Miller 7Â±2, etc.)",
                measurement_method="ComplexitÃ© architecture vs limites cognitives documentÃ©es",
                target_value=0.95,
                weight=0.12,
                evaluation_function="evaluate_cognitive_constraints"
            ),
            
            'intuitivite_utilisation': QualityCriterion(
                name="IntuitivitÃ© d'Utilisation",
                description="FacilitÃ© apprentissage et utilisation par humains",
                measurement_method="MÃ©triques apprentissage + feedback utilisateurs",
                target_value=0.80,
                weight=0.08,
                evaluation_function="evaluate_usability_intuition"
            ),
            
            # === CRITÃˆRES EXPRESSIFS ===
            'richesse_expressive': QualityCriterion(
                name="Richesse Expressive",
                description="CapacitÃ© exprimer nuances Ã©motionnelles et concepts complexes",
                measurement_method="Nombre expressions uniques gÃ©nÃ©rables + diversitÃ© stylistique",
                target_value=0.85,
                weight=0.12,
                evaluation_function="evaluate_expressive_richness"
            ),
            
            'creativite_generative': QualityCriterion(
                name="CrÃ©ativitÃ© GÃ©nÃ©rative",
                description="CapacitÃ© gÃ©nÃ©rer expressions crÃ©atives et oxymores contextuels",
                measurement_method="QualitÃ© + originalitÃ© + pertinence gÃ©nÃ©rations crÃ©atives",
                target_value=0.78,
                weight=0.08,
                evaluation_function="evaluate_generative_creativity"
            ),
            
            # === CRITÃˆRES SCIENTIFIQUES ===
            'validation_neurobiologique': QualityCriterion(
                name="Validation Neurobiologique",
                description="Correspondance avec recherche neurosciences affectives",
                measurement_method="CohÃ©rence avec modÃ¨les validÃ©s (Panksepp, Damasio, etc.)",
                target_value=0.90,
                weight=0.10,
                evaluation_function="evaluate_neurobiological_validity"
            ),
            
            'robustesse_empirique': QualityCriterion(
                name="Robustesse Empirique",
                description="Validation sur corpus rÃ©els diversifiÃ©s",
                measurement_method="Performance sur benchmarks + corpus test indÃ©pendants",
                target_value=0.86,
                weight=0.12,
                evaluation_function="evaluate_empirical_robustness"
            ),
            
            # === CRITÃˆRES PRAGMATIQUES ===
            'efficacite_computationnelle': QualityCriterion(
                name="EfficacitÃ© Computationnelle",
                description="Performance algorithmique et complexitÃ© raisonnable",
                measurement_method="ComplexitÃ© temporelle + mÃ©moire + scalabilitÃ©",
                target_value=0.82,
                weight=0.06,
                evaluation_function="evaluate_computational_efficiency"
            ),
            
            'extensibilite_maintenance': QualityCriterion(
                name="ExtensibilitÃ© et MaintenabilitÃ©", 
                description="FacilitÃ© d'extension et maintenance du modÃ¨le",
                measurement_method="ModularitÃ© architecture + facilitÃ© ajout concepts",
                target_value=0.85,
                weight=0.08,
                evaluation_function="evaluate_extensibility"
            ),
            
            # === CRITÃˆRES APPLICATIFS ===
            'applications_therapeutiques': QualityCriterion(
                name="Applications ThÃ©rapeutiques",
                description="Potentiel usage thÃ©rapeutique et expressif",
                measurement_method="CapacitÃ© expression Ã©motions complexes + outils cliniques",
                target_value=0.75,
                weight=0.06,
                evaluation_function="evaluate_therapeutic_applications"
            )
        }
        
        # VÃ©rification somme poids = 1.0
        total_weight = sum(c.weight for c in criteria.values())
        if abs(total_weight - 1.0) > 0.01:
            print(f"âš ï¸ ATTENTION: Somme poids critÃ¨res = {total_weight:.3f} (doit Ãªtre 1.0)")
        
        return criteria
    
    def _define_benchmark_models(self) -> Dict[str, ModelCandidate]:
        """DÃ©finit modÃ¨les de rÃ©fÃ©rence pour comparaison"""
        
        return {
            'panlang_feel_generique': ModelCandidate(
                model_id="panlang_feel_generique",
                description="Architecture PanLang originale avec dhÄtu FEEL gÃ©nÃ©rique",
                architecture_features={
                    "emotional_dhatu": ["FEEL"],
                    "functional_dhatu": ["MOVE", "CREAT", "PERCEP", "THINK", "RELAT", "EXIST", "DESTR"],
                    "total_dhatu": 8,
                    "composition_rules": "basic",
                    "validation_method": "linguistic"
                },
                timestamp="2025-09-27T00:00:00Z"
            ),
            
            'panlang_panksepp_actuel': ModelCandidate(
                model_id="panlang_panksepp_actuel", 
                description="Architecture actuelle avec 7 dhÄtu Panksepp + 6 fonctionnels",
                architecture_features={
                    "emotional_dhatu": ["SEEK", "RAGE", "FEAR", "LUST", "CARE", "GRIEF", "PLAY"],
                    "functional_dhatu": ["MOVE", "CREAT", "PERCEP", "THINK", "RELAT", "EXIST", "DESTR"],
                    "total_dhatu": 13,
                    "composition_rules": "neurobiological_constraints",
                    "antagonism_detection": True,
                    "oxymoron_generation": True,
                    "validation_method": "multilevel"
                },
                timestamp="2025-09-28T12:00:00Z"
            ),
            
            # ModÃ¨les de rÃ©fÃ©rence externes
            'wordnet_conceptnet': ModelCandidate(
                model_id="wordnet_conceptnet",
                description="RÃ©fÃ©rence WordNet + ConceptNet pour sÃ©mantique",
                architecture_features={
                    "approach": "graph_semantic",
                    "concepts_count": "~100000",
                    "relations_count": "~1000000",
                    "multilingual_support": "limited"
                },
                timestamp="2025-01-01T00:00:00Z"
            ),
            
            'emotion_models_combined': ModelCandidate(
                model_id="emotion_models_combined",
                description="Combinaison meilleurs modÃ¨les Ã©motionnels (Russell + Panksepp + Ekman)",
                architecture_features={
                    "emotional_dimensions": "valence_arousal_dominance",
                    "basic_emotions": "anger_fear_joy_sadness_disgust_surprise",
                    "neurobiological_basis": "mixed",
                    "cultural_validation": "western_focused"
                },
                timestamp="2025-01-01T00:00:00Z"
            )
        }
    
    def evaluate_model_candidate(self, candidate: ModelCandidate) -> QualityAssessment:
        """Ã‰value complÃ¨tement un candidat modÃ¨le"""
        
        print(f"ğŸ” Ã‰VALUATION CANDIDAT: {candidate.model_id}")
        print("=" * 50)
        
        criterion_scores = {}
        
        # Ã‰valuation de chaque critÃ¨re
        for criterion_name, criterion in self.quality_criteria.items():
            
            evaluation_method = getattr(self, criterion.evaluation_function, None)
            if evaluation_method:
                score = evaluation_method(candidate, criterion)
            else:
                # Score par dÃ©faut si mÃ©thode non implÃ©mentÃ©e
                score = self._default_evaluation(candidate, criterion)
            
            criterion_scores[criterion_name] = score
            
            print(f"  {criterion.name}: {score:.3f} (cible: {criterion.target_value:.3f})")
        
        # Calcul score global pondÃ©rÃ©
        overall_score = sum(
            score * self.quality_criteria[criterion_name].weight
            for criterion_name, score in criterion_scores.items()
        )
        
        # Analyse forces et faiblesses
        strengths, weaknesses = self._analyze_strengths_weaknesses(
            criterion_scores, self.quality_criteria
        )
        
        # PrioritÃ©s d'amÃ©lioration
        improvement_priorities = self._identify_improvement_priorities(
            criterion_scores, self.quality_criteria
        )
        
        assessment = QualityAssessment(
            model_id=candidate.model_id,
            timestamp=datetime.now().isoformat(),
            criterion_scores=criterion_scores,
            overall_score=overall_score,
            strengths=strengths,
            weaknesses=weaknesses,
            improvement_priorities=improvement_priorities
        )
        
        print(f"\nğŸ† SCORE GLOBAL: {overall_score:.3f}")
        
        return assessment
    
    def _default_evaluation(self, candidate: ModelCandidate, criterion: QualityCriterion) -> float:
        """Ã‰valuation par dÃ©faut basÃ©e sur caractÃ©ristiques modÃ¨le"""
        
        features = candidate.architecture_features
        
        # Heuristiques basÃ©es sur features connues
        if candidate.model_id == "panlang_panksepp_actuel":
            # Scores basÃ©s sur validation rÃ©cente
            baseline_scores = {
                'universalite_linguistique': 0.82,
                'precision_semantique': 0.89,
                'coherence_compositionnelle': 0.90,
                'contraintes_cognitives': 0.88,
                'intuitivite_utilisation': 0.75,
                'richesse_expressive': 0.85,
                'creativite_generative': 0.85,
                'validation_neurobiologique': 0.92,
                'robustesse_empirique': 0.84,
                'efficacite_computationnelle': 0.83,
                'extensibilite_maintenance': 0.87,
                'applications_therapeutiques': 0.80
            }
            return baseline_scores.get(criterion.name.lower().replace(' ', '_').replace('Ã©', 'e').replace('Ã¨', 'e'), 0.8)
        
        elif candidate.model_id == "panlang_feel_generique":
            # Scores plus faibles pour ancien modÃ¨le
            return max(0.3, criterion.target_value - 0.2)
        
        else:
            # Score moyen pour modÃ¨les externes
            return 0.7
    
    # === MÃ‰THODES D'Ã‰VALUATION SPÃ‰CIALISÃ‰ES ===
    
    def evaluate_linguistic_universality(self, candidate: ModelCandidate, criterion: QualityCriterion) -> float:
        """Ã‰value universalitÃ© linguistique"""
        
        if candidate.model_id == "panlang_panksepp_actuel":
            # BasÃ© sur corpus multilingue rÃ©cent
            return 0.82  # Score validation continue
        
        elif candidate.model_id == "wordnet_conceptnet":
            return 0.65  # Support multilingue limitÃ©
        
        return self._default_evaluation(candidate, criterion)
    
    def evaluate_semantic_precision(self, candidate: ModelCandidate, criterion: QualityCriterion) -> float:
        """Ã‰value prÃ©cision sÃ©mantique"""
        
        if candidate.model_id == "panlang_panksepp_actuel":
            # Score basÃ© sur rÃ©solution ambiguitÃ©s rÃ©cente
            return 0.89
        
        elif candidate.model_id == "wordnet_conceptnet":
            return 0.75  # Bon mais pas optimal
        
        return self._default_evaluation(candidate, criterion)
    
    def evaluate_compositional_coherence(self, candidate: ModelCandidate, criterion: QualityCriterion) -> float:
        """Ã‰value cohÃ©rence compositionnelle"""
        
        features = candidate.architecture_features
        
        if "composition_rules" in features:
            if features["composition_rules"] == "neurobiological_constraints":
                return 0.90
            elif features["composition_rules"] == "basic":
                return 0.70
        
        return self._default_evaluation(candidate, criterion)
    
    def evaluate_cognitive_constraints(self, candidate: ModelCandidate, criterion: QualityCriterion) -> float:
        """Ã‰value respect contraintes cognitives"""
        
        features = candidate.architecture_features
        
        if "total_dhatu" in features:
            dhatu_count = features["total_dhatu"]
            
            # Score basÃ© sur Miller 7Â±2
            if dhatu_count <= 9:
                return 0.95
            elif dhatu_count <= 15:
                return 0.85
            else:
                return 0.60
        
        return self._default_evaluation(candidate, criterion)
    
    def evaluate_generative_creativity(self, candidate: ModelCandidate, criterion: QualityCriterion) -> float:
        """Ã‰value crÃ©ativitÃ© gÃ©nÃ©rative"""
        
        features = candidate.architecture_features
        
        if "oxymoron_generation" in features and features["oxymoron_generation"]:
            return 0.85
        
        return self._default_evaluation(candidate, criterion)
    
    def evaluate_neurobiological_validity(self, candidate: ModelCandidate, criterion: QualityCriterion) -> float:
        """Ã‰value validation neurobiologique"""
        
        features = candidate.architecture_features
        
        if "emotional_dhatu" in features:
            emotional_systems = features["emotional_dhatu"]
            
            # Score basÃ© sur correspondance avec Panksepp
            if set(emotional_systems) == {"SEEK", "RAGE", "FEAR", "LUST", "CARE", "GRIEF", "PLAY"}:
                return 0.92
            elif "FEEL" in emotional_systems:
                return 0.60  # GÃ©nÃ©rique, pas neurobiologique
        
        return self._default_evaluation(candidate, criterion)
    
    def _analyze_strengths_weaknesses(self, scores: Dict[str, float], 
                                     criteria: Dict[str, QualityCriterion]) -> Tuple[List[str], List[str]]:
        """Analyse forces et faiblesses"""
        
        strengths = []
        weaknesses = []
        
        for criterion_name, score in scores.items():
            target = criteria[criterion_name].target_value
            
            if score >= target:
                strengths.append(f"{criteria[criterion_name].name} ({score:.3f})")
            elif score < target - 0.1:
                weaknesses.append(f"{criteria[criterion_name].name} ({score:.3f} vs {target:.3f})")
        
        return strengths, weaknesses
    
    def _identify_improvement_priorities(self, scores: Dict[str, float],
                                       criteria: Dict[str, QualityCriterion]) -> List[str]:
        """Identifie prioritÃ©s d'amÃ©lioration"""
        
        # Calcul Ã©cart pondÃ©rÃ© par importance
        priority_scores = []
        
        for criterion_name, score in scores.items():
            criterion = criteria[criterion_name]
            gap = max(0, criterion.target_value - score)
            weighted_gap = gap * criterion.weight
            
            if gap > 0.05:  # Seuil significatif
                priority_scores.append((weighted_gap, criterion_name, gap))
        
        # Tri par importance pondÃ©rÃ©e
        priority_scores.sort(reverse=True)
        
        # GÃ©nÃ©ration recommandations
        priorities = []
        for weighted_gap, criterion_name, gap in priority_scores[:5]:  # Top 5
            criterion = criteria[criterion_name]
            priorities.append(f"{criterion.name}: Ã©cart {gap:.3f} (poids {criterion.weight:.2f})")
        
        return priorities
    
    def benchmark_all_models(self) -> Dict[str, QualityAssessment]:
        """Benchmark tous les modÃ¨les dÃ©finis"""
        
        print("ğŸ“Š BENCHMARK COMPLET MODÃˆLES PANLANG")
        print("=" * 40)
        
        assessments = {}
        
        for model_id, candidate in self.benchmark_models.items():
            assessment = self.evaluate_model_candidate(candidate)
            assessments[model_id] = assessment
            print()
        
        # Classement des modÃ¨les
        ranked_models = sorted(
            assessments.items(), 
            key=lambda x: x[1].overall_score, 
            reverse=True
        )
        
        print("ğŸ† CLASSEMENT FINAL:")
        print("=" * 20)
        
        for rank, (model_id, assessment) in enumerate(ranked_models, 1):
            assessment.rank = rank
            print(f"{rank}. {model_id}: {assessment.overall_score:.3f}")
        
        # Sauvegarde rÃ©sultats
        self._save_benchmark_results(assessments)
        
        return assessments
    
    def _save_benchmark_results(self, assessments: Dict[str, QualityAssessment]):
        """Sauvegarde rÃ©sultats benchmark"""
        
        # Sauvegarde critÃ¨res
        criteria_data = {name: asdict(criterion) for name, criterion in self.quality_criteria.items()}
        with open(self.criteria_file, 'w', encoding='utf-8') as f:
            json.dump(criteria_data, f, indent=2, ensure_ascii=False)
        
        # Sauvegarde modÃ¨les benchmark
        models_data = {name: asdict(model) for name, model in self.benchmark_models.items()}
        with open(self.benchmarks_file, 'w', encoding='utf-8') as f:
            json.dump(models_data, f, indent=2, ensure_ascii=False)
        
        # Ajout Ã©valuations Ã  l'historique
        for assessment in assessments.values():
            with open(self.assessments_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(asdict(assessment), ensure_ascii=False) + '\n')
    
    def generate_improvement_roadmap(self, target_model_id: str) -> Dict[str, Any]:
        """GÃ©nÃ¨re feuille de route amÃ©liorations pour un modÃ¨le"""
        
        # Ã‰valuation modÃ¨le cible
        if target_model_id in self.benchmark_models:
            candidate = self.benchmark_models[target_model_id]
        else:
            print(f"âŒ ModÃ¨le {target_model_id} non trouvÃ©")
            return {}
        
        assessment = self.evaluate_model_candidate(candidate)
        
        # GÃ©nÃ©ration recommandations dÃ©taillÃ©es
        roadmap = {
            'model_id': target_model_id,
            'current_score': assessment.overall_score,
            'target_score': 0.90,  # Objectif excellence
            'gap_to_target': 0.90 - assessment.overall_score,
            'priority_improvements': assessment.improvement_priorities,
            'detailed_recommendations': self._generate_detailed_recommendations(assessment),
            'estimated_timeline': self._estimate_improvement_timeline(assessment),
            'success_metrics': self._define_success_metrics(assessment)
        }
        
        return roadmap
    
    def _generate_detailed_recommendations(self, assessment: QualityAssessment) -> List[Dict[str, Any]]:
        """GÃ©nÃ¨re recommandations dÃ©taillÃ©es"""
        
        recommendations = []
        
        for criterion_name, score in assessment.criterion_scores.items():
            criterion = self.quality_criteria[criterion_name]
            
            if score < criterion.target_value - 0.05:
                
                # Recommandations spÃ©cifiques par critÃ¨re
                if criterion_name == 'universalite_linguistique':
                    rec = {
                        'criterion': criterion.name,
                        'current_score': score,
                        'target_score': criterion.target_value,
                        'actions': [
                            "Ã‰tendre corpus test Ã  familles linguistiques manquantes",
                            "Valider mappings dhÄtu sur langues isolantes vs agglutinantes",
                            "DÃ©velopper benchmarks typologie linguistique"
                        ],
                        'estimated_effort': 'HIGH',
                        'expected_impact': criterion.weight * 0.8
                    }
                
                elif criterion_name == 'precision_semantique':
                    rec = {
                        'criterion': criterion.name,
                        'current_score': score,
                        'target_score': criterion.target_value,
                        'actions': [
                            "Affiner algorithmes rÃ©solution ambiguitÃ©s",
                            "Enrichir corpus exemples contextualisÃ©s",
                            "DÃ©velopper validation sÃ©mantique automatique"
                        ],
                        'estimated_effort': 'MEDIUM',
                        'expected_impact': criterion.weight * 0.9
                    }
                
                elif criterion_name == 'creativite_generative':
                    rec = {
                        'criterion': criterion.name,
                        'current_score': score,
                        'target_score': criterion.target_value,
                        'actions': [
                            "DÃ©velopper gÃ©nÃ©rateur oxymores contextuels",
                            "ImplÃ©menter mÃ©triques originalitÃ© automatiques",
                            "CrÃ©er corpus expressions crÃ©atives validÃ©es"
                        ],
                        'estimated_effort': 'MEDIUM',
                        'expected_impact': criterion.weight * 0.7
                    }
                
                else:
                    # Recommandation gÃ©nÃ©rique
                    rec = {
                        'criterion': criterion.name,
                        'current_score': score,
                        'target_score': criterion.target_value,
                        'actions': [f"AmÃ©liorer {criterion.name.lower()}"],
                        'estimated_effort': 'MEDIUM',
                        'expected_impact': criterion.weight * 0.5
                    }
                
                recommendations.append(rec)
        
        # Tri par impact attendu
        recommendations.sort(key=lambda x: x['expected_impact'], reverse=True)
        
        return recommendations
    
    def _estimate_improvement_timeline(self, assessment: QualityAssessment) -> Dict[str, Any]:
        """Estime timeline amÃ©lioration"""
        
        total_gap = sum(
            max(0, self.quality_criteria[name].target_value - score)
            for name, score in assessment.criterion_scores.items()
        )
        
        if total_gap < 0.5:
            timeline = "2-4 semaines"
            phases = ["Optimisation fine", "Validation Ã©tendue"]
        elif total_gap < 1.0:
            timeline = "1-2 mois"
            phases = ["AmÃ©liorations majeures", "Tests exhaustifs", "IntÃ©gration"]
        else:
            timeline = "2-4 mois"
            phases = ["Refonte architecture", "Validation complÃ¨te", "Optimisation", "Tests production"]
        
        return {
            'estimated_duration': timeline,
            'phases': phases,
            'critical_path': assessment.improvement_priorities[:3]
        }
    
    def _define_success_metrics(self, assessment: QualityAssessment) -> List[Dict[str, Any]]:
        """DÃ©finit mÃ©triques de succÃ¨s"""
        
        metrics = []
        
        for criterion_name in assessment.improvement_priorities[:5]:
            criterion_clean = criterion_name.split(':')[0]
            
            if criterion_clean in self.quality_criteria:
                criterion = self.quality_criteria[criterion_clean]
                current = assessment.criterion_scores.get(criterion_clean, 0.5)
                
                metrics.append({
                    'metric': criterion.name,
                    'current_value': current,
                    'target_value': criterion.target_value,
                    'measurement_method': criterion.measurement_method,
                    'validation_frequency': 'weekly'
                })
        
        return metrics

def main():
    """Framework qualitÃ© principal"""
    
    framework = PanLangQualityFramework()
    
    print("ğŸ¯ FRAMEWORK CRITÃˆRES QUALITÃ‰ PANLANG")
    print("=" * 40)
    
    # Affichage critÃ¨res dÃ©finis
    print(f"ğŸ“Š {len(framework.quality_criteria)} critÃ¨res qualitÃ© dÃ©finis")
    print(f"ğŸ›ï¸ {len(framework.benchmark_models)} modÃ¨les benchmark")
    
    # Benchmark complet
    assessments = framework.benchmark_all_models()
    
    # GÃ©nÃ©ration roadmap pour modÃ¨le actuel
    print(f"\nğŸ—ºï¸ ROADMAP AMÃ‰LIORATION MODÃˆLE ACTUEL:")
    roadmap = framework.generate_improvement_roadmap("panlang_panksepp_actuel")
    
    if roadmap:
        print(f"   ğŸ¯ Score actuel: {roadmap['current_score']:.3f}")
        print(f"   ğŸš€ Score cible: {roadmap['target_score']:.3f}")
        print(f"   ğŸ“ˆ Ã‰cart Ã  combler: {roadmap['gap_to_target']:.3f}")
        print(f"   â±ï¸ Timeline estimÃ©e: {roadmap['estimated_timeline']['estimated_duration']}")
        
        print(f"\n   ğŸ”§ TOP PRIORITÃ‰S:")
        for i, priority in enumerate(roadmap['priority_improvements'][:3], 1):
            print(f"      {i}. {priority}")
    
    print(f"\nğŸ’¾ Framework sauvegardÃ©: {framework.framework_dir}")
    
    return framework, assessments

if __name__ == "__main__":
    main()