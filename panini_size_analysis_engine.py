#!/usr/bin/env python3
"""
ğŸ“ PANINI-FS SIZE ANALYSIS ENGINE
=================================

Mission: Analyser en dÃ©tail les tailles des reprÃ©sentations Panini
et de l'encyclopÃ©die de connaissances pour optimiser l'efficacitÃ©
de stockage et comprendre l'overhead du systÃ¨me.

Analyses prÃ©vues:
1. Taille fichiers compressÃ©s vs originaux
2. Overhead mÃ©tadonnÃ©es par fichier
3. Taille encyclopÃ©dies de connaissances
4. Ratio efficacitÃ© stockage global
5. Analyse distribution tailles par format
6. Optimisations possibles identified

Objectifs:
- Mesurer overhead rÃ©el du systÃ¨me PaniniFS
- Identifier goulots d'Ã©tranglement de taille
- Proposer optimisations pour rÃ©duire overhead
- Valider viabilitÃ© Ã©conomique du systÃ¨me
"""

import os
import sys
import json
import glob
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import numpy as np

class PaniniSizeAnalysisEngine:
    def __init__(self):
        self.timestamp = datetime.now().isoformat()
        self.analysis_results = {}
        self.size_statistics = defaultdict(dict)
        self.overhead_analysis = defaultdict(dict)
        
        print(f"""
ğŸ“ PANINI-FS SIZE ANALYSIS ENGINE
============================================================
ğŸ¯ Mission: Analyser efficacitÃ© stockage systÃ¨me PaniniFS
ğŸ“Š Analyse: Fichiers compressÃ©s + MÃ©tadonnÃ©es + EncyclopÃ©dies
â° Session: {self.timestamp}

ğŸ” Initialisation analyse des tailles...
""")

    def analyze_compressed_files_sizes(self):
        """Analyser les tailles des fichiers compressÃ©s"""
        print("\nğŸ“Š ANALYSE TAILLES FICHIERS COMPRESSÃ‰S")
        print("=" * 60)
        
        # Trouver le dossier de batch le plus rÃ©cent
        batch_dirs = [d for d in os.listdir('.') if d.startswith('panini_universal_batch_')]
        if not batch_dirs:
            print("âŒ Aucun dossier de compression trouvÃ©")
            return {}
        
        latest_batch = sorted(batch_dirs)[-1]
        batch_path = Path(latest_batch)
        
        print(f"ğŸ“ Analyse du batch: {latest_batch}")
        
        # Analyser chaque fichier compressÃ©
        compressed_files = list(batch_path.glob("*.panini"))
        metadata_files = list(batch_path.glob("*.panini.meta"))
        
        print(f"ğŸ—œï¸  Fichiers compressÃ©s trouvÃ©s: {len(compressed_files)}")
        print(f"ğŸ“‹ Fichiers mÃ©tadonnÃ©es trouvÃ©s: {len(metadata_files)}")
        
        file_analysis = []
        total_original = 0
        total_compressed = 0
        total_metadata = 0
        
        for compressed_file in compressed_files:
            metadata_file = Path(str(compressed_file) + ".meta")
            
            if metadata_file.exists():
                # Tailles fichiers
                compressed_size = compressed_file.stat().st_size
                metadata_size = metadata_file.stat().st_size
                
                # Charger mÃ©tadonnÃ©es pour taille originale
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                original_size = metadata['original_size']
                extension = metadata['extension']
                category = metadata['category']
                
                # Calculer ratios
                compression_ratio = compressed_size / original_size if original_size > 0 else 1
                metadata_overhead = metadata_size / original_size if original_size > 0 else 0
                total_panini_size = compressed_size + metadata_size
                total_ratio = total_panini_size / original_size if original_size > 0 else 1
                
                analysis_point = {
                    'filename': compressed_file.name,
                    'extension': extension,
                    'category': category,
                    'original_size': original_size,
                    'compressed_size': compressed_size,
                    'metadata_size': metadata_size,
                    'total_panini_size': total_panini_size,
                    'compression_ratio': compression_ratio,
                    'metadata_overhead': metadata_overhead,
                    'total_ratio': total_ratio,
                    'space_saved': original_size - total_panini_size,
                    'space_saved_pct': (1 - total_ratio) * 100
                }
                
                file_analysis.append(analysis_point)
                
                total_original += original_size
                total_compressed += compressed_size
                total_metadata += metadata_size
        
        # Statistiques globales
        total_panini = total_compressed + total_metadata
        global_compression_ratio = total_compressed / total_original if total_original > 0 else 1
        global_metadata_overhead = total_metadata / total_original if total_original > 0 else 0
        global_total_ratio = total_panini / total_original if total_original > 0 else 1
        
        print(f"""
ğŸ“Š STATISTIQUES GLOBALES:
   Taille originale totale  : {self._format_size(total_original)}
   Taille compressÃ©e totale : {self._format_size(total_compressed)}
   Taille mÃ©tadonnÃ©es totale: {self._format_size(total_metadata)}
   Taille PaniniFS totale   : {self._format_size(total_panini)}
   
   Ratio compression seule  : {global_compression_ratio:.3f}
   Overhead mÃ©tadonnÃ©es     : {global_metadata_overhead:.3f} ({(global_metadata_overhead*100):.1f}%)
   Ratio PaniniFS total     : {global_total_ratio:.3f}
   Ã‰conomie globale         : {self._format_size(total_original - total_panini)} ({((1-global_total_ratio)*100):.1f}%)
""")
        
        # Top/Worst performers
        file_analysis.sort(key=lambda x: x['total_ratio'])
        
        print("\nğŸ† TOP 5 MEILLEURS RATIOS PANINI:")
        for i, analysis in enumerate(file_analysis[:5]):
            print(f"   {i+1}. {analysis['extension']:6} : {analysis['total_ratio']:.3f} "
                  f"({analysis['space_saved_pct']:+5.1f}%) - {Path(analysis['filename']).stem[:30]}")
        
        print("\nâš ï¸  TOP 5 PIRES RATIOS PANINI:")
        for i, analysis in enumerate(file_analysis[-5:]):
            print(f"   {i+1}. {analysis['extension']:6} : {analysis['total_ratio']:.3f} "
                  f"({analysis['space_saved_pct']:+5.1f}%) - {Path(analysis['filename']).stem[:30]}")
        
        self.size_statistics['files'] = {
            'total_files': len(file_analysis),
            'total_original': total_original,
            'total_compressed': total_compressed,
            'total_metadata': total_metadata,
            'total_panini': total_panini,
            'global_compression_ratio': global_compression_ratio,
            'global_metadata_overhead': global_metadata_overhead,
            'global_total_ratio': global_total_ratio,
            'file_details': file_analysis
        }
        
        return file_analysis

    def analyze_metadata_overhead(self, file_analysis):
        """Analyser l'overhead des mÃ©tadonnÃ©es en dÃ©tail"""
        print("\nğŸ“‹ ANALYSE OVERHEAD MÃ‰TADONNÃ‰ES")
        print("=" * 60)
        
        # Analyser overhead par extension
        overhead_by_ext = defaultdict(list)
        for analysis in file_analysis:
            overhead_by_ext[analysis['extension']].append(analysis['metadata_overhead'])
        
        print("ğŸ“Š OVERHEAD MÃ‰TADONNÃ‰ES PAR EXTENSION:")
        ext_overheads = []
        for ext, overheads in overhead_by_ext.items():
            avg_overhead = np.mean(overheads)
            max_overhead = max(overheads)
            min_overhead = min(overheads)
            ext_overheads.append((ext, avg_overhead, min_overhead, max_overhead, len(overheads)))
        
        ext_overheads.sort(key=lambda x: x[1], reverse=True)
        
        for ext, avg, min_oh, max_oh, count in ext_overheads:
            print(f"   {ext:8} : {avg:.3f} avg ({min_oh:.3f}-{max_oh:.3f}) sur {count} fichiers")
        
        # Analyser overhead par taille de fichier
        size_ranges = [
            (0, 1024, "< 1KB"),
            (1024, 10*1024, "1KB-10KB"), 
            (10*1024, 100*1024, "10KB-100KB"),
            (100*1024, 1024*1024, "100KB-1MB"),
            (1024*1024, 10*1024*1024, "1MB-10MB"),
            (10*1024*1024, float('inf'), "> 10MB")
        ]
        
        print("\nğŸ“Š OVERHEAD PAR TAILLE FICHIER:")
        for min_size, max_size, label in size_ranges:
            range_files = [a for a in file_analysis 
                          if min_size <= a['original_size'] < max_size]
            if range_files:
                avg_overhead = np.mean([a['metadata_overhead'] for a in range_files])
                print(f"   {label:12} : {avg_overhead:.3f} avg overhead ({len(range_files)} fichiers)")
        
        # Analyser contenu des mÃ©tadonnÃ©es
        sample_metadata_file = None
        for analysis in file_analysis[:3]:  # Prendre 3 exemples
            metadata_file = Path(f"panini_universal_batch_*/{analysis['filename']}.meta")
            metadata_files = glob.glob(str(metadata_file))
            if metadata_files:
                sample_metadata_file = metadata_files[0]
                break
        
        if sample_metadata_file:
            print(f"\nğŸ“‹ ANALYSE CONTENU MÃ‰TADONNÃ‰ES (exemple: {Path(sample_metadata_file).name}):")
            with open(sample_metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # Calculer taille de chaque section
            metadata_sections = {}
            for key, value in metadata.items():
                section_json = json.dumps({key: value}, ensure_ascii=False)
                metadata_sections[key] = len(section_json.encode('utf-8'))
            
            total_metadata_size = sum(metadata_sections.values())
            
            print("   RÃ©partition taille mÃ©tadonnÃ©es:")
            sorted_sections = sorted(metadata_sections.items(), key=lambda x: x[1], reverse=True)
            for section, size in sorted_sections:
                pct = (size / total_metadata_size) * 100
                print(f"     {section:20} : {size:5} bytes ({pct:4.1f}%)")
        
        self.overhead_analysis['metadata'] = {
            'by_extension': dict(overhead_by_ext),
            'by_size_range': size_ranges,
            'average_overhead': np.mean([a['metadata_overhead'] for a in file_analysis]),
            'max_overhead': max([a['metadata_overhead'] for a in file_analysis]),
            'min_overhead': min([a['metadata_overhead'] for a in file_analysis])
        }

    def analyze_encyclopedia_sizes(self):
        """Analyser les tailles des encyclopÃ©dies de connaissances"""
        print("\nğŸ“š ANALYSE TAILLES ENCYCLOPÃ‰DIES")
        print("=" * 60)
        
        encyclopedia_files = {
            'format_encyclopedia': glob.glob("PANINI_FORMAT_ENCYCLOPEDIA_*.json"),
            'optimization_encyclopedia': glob.glob("PANINI_OPTIMIZATION_ENCYCLOPEDIA_*.json"),
            'reports': glob.glob("PANINI_OPTIMIZATION_REPORT_*.md"),
            'batch_reports': glob.glob("panini_universal_batch_*/panini_universal_report.json")
        }
        
        encyclopedia_analysis = {}
        total_encyclopedia_size = 0
        
        for category, files in encyclopedia_files.items():
            if files:
                latest_file = sorted(files)[-1]  # Plus rÃ©cent
                file_size = os.path.getsize(latest_file)
                total_encyclopedia_size += file_size
                
                # Analyser contenu si JSON
                content_analysis = {}
                if latest_file.endswith('.json'):
                    try:
                        with open(latest_file, 'r', encoding='utf-8') as f:
                            content = json.load(f)
                        
                        # Analyser sections principales
                        for key, value in content.items():
                            if isinstance(value, (dict, list)):
                                section_json = json.dumps({key: value}, ensure_ascii=False)
                                content_analysis[key] = len(section_json.encode('utf-8'))
                    except Exception as e:
                        content_analysis['error'] = str(e)
                
                encyclopedia_analysis[category] = {
                    'file': latest_file,
                    'size': file_size,
                    'content_breakdown': content_analysis
                }
                
                print(f"ğŸ“– {category:25} : {self._format_size(file_size):>10} - {Path(latest_file).name}")
                
                # Afficher breakdown du contenu
                if content_analysis and 'error' not in content_analysis:
                    total_content = sum(content_analysis.values())
                    print("   RÃ©partition contenu:")
                    sorted_content = sorted(content_analysis.items(), key=lambda x: x[1], reverse=True)
                    for section, size in sorted_content[:5]:  # Top 5
                        pct = (size / total_content) * 100 if total_content > 0 else 0
                        print(f"     {section:20} : {self._format_size(size):>8} ({pct:4.1f}%)")
        
        print(f"\nğŸ“Š TAILLE TOTALE ENCYCLOPÃ‰DIES: {self._format_size(total_encyclopedia_size)}")
        
        self.size_statistics['encyclopedias'] = {
            'total_size': total_encyclopedia_size,
            'by_category': encyclopedia_analysis
        }
        
        return encyclopedia_analysis

    def calculate_storage_efficiency(self, file_analysis, encyclopedia_analysis):
        """Calculer l'efficacitÃ© globale de stockage"""
        print("\nâš¡ CALCUL EFFICACITÃ‰ STOCKAGE GLOBAL")
        print("=" * 60)
        
        # DonnÃ©es de base
        total_original = self.size_statistics['files']['total_original']
        total_panini_files = self.size_statistics['files']['total_panini']
        total_encyclopedias = self.size_statistics['encyclopedias']['total_size']
        
        # Calcul overhead encyclopÃ©dies par fichier
        num_files = len(file_analysis)
        encyclopedia_overhead_per_file = total_encyclopedias / num_files if num_files > 0 else 0
        
        # EfficacitÃ© avec et sans encyclopÃ©dies
        efficiency_without_encyclopedias = total_panini_files / total_original if total_original > 0 else 1
        efficiency_with_encyclopedias = (total_panini_files + total_encyclopedias) / total_original if total_original > 0 else 1
        
        # Analyse par seuil de taille
        size_thresholds = [1024, 10*1024, 100*1024, 1024*1024, 10*1024*1024]  # 1KB, 10KB, 100KB, 1MB, 10MB
        
        print("ğŸ“Š EFFICACITÃ‰ PAR TAILLE DE FICHIER:")
        print("   (inclut overhead encyclopÃ©dies rÃ©parti)")
        
        efficiency_by_size = []
        for threshold in size_thresholds:
            files_above = [a for a in file_analysis if a['original_size'] >= threshold]
            if files_above:
                total_orig = sum(a['original_size'] for a in files_above)
                total_panini = sum(a['total_panini_size'] for a in files_above)
                
                # Ajouter overhead encyclopÃ©dies proportionnel
                encyclopedia_overhead = encyclopedia_overhead_per_file * len(files_above)
                total_with_encyc = total_panini + encyclopedia_overhead
                
                efficiency = total_with_encyc / total_orig if total_orig > 0 else 1
                
                efficiency_by_size.append({
                    'threshold': threshold,
                    'files_count': len(files_above),
                    'efficiency': efficiency,
                    'viable': efficiency < 1.0
                })
                
                viability = "âœ… VIABLE" if efficiency < 1.0 else "âŒ NON-VIABLE"
                print(f"   Fichiers â‰¥ {self._format_size(threshold):>8} : {efficiency:.3f} ratio ({len(files_above):2} fichiers) {viability}")
        
        # Point de viabilitÃ© Ã©conomique
        viable_threshold = None
        for analysis in efficiency_by_size:
            if analysis['viable']:
                viable_threshold = analysis['threshold']
                break
        
        if viable_threshold:
            print(f"\nğŸ¯ SEUIL VIABILITÃ‰ Ã‰CONOMIQUE: â‰¥ {self._format_size(viable_threshold)}")
        else:
            print(f"\nâš ï¸  AUCUN SEUIL VIABLE TROUVÃ‰ - Optimisation nÃ©cessaire")
        
        # Recommandations d'optimisation
        print(f"\nğŸ’¡ OPTIMISATIONS RECOMMANDÃ‰ES:")
        
        # 1. Overhead mÃ©tadonnÃ©es
        avg_metadata_overhead = np.mean([a['metadata_overhead'] for a in file_analysis])
        if avg_metadata_overhead > 0.05:  # Plus de 5%
            print(f"   ğŸ”§ RÃ©duire mÃ©tadonnÃ©es: {avg_metadata_overhead:.1%} overhead actuel")
        
        # 2. EncyclopÃ©dies partagÃ©es
        if encyclopedia_overhead_per_file > 1024:  # Plus de 1KB par fichier
            print(f"   ğŸ“š EncyclopÃ©dies partagÃ©es: {self._format_size(encyclopedia_overhead_per_file)} overhead/fichier")
        
        # 3. Compression diffÃ©rentielle
        small_files = [a for a in file_analysis if a['original_size'] < 10*1024]
        if small_files:
            avg_small_ratio = np.mean([a['total_ratio'] for a in small_files])
            if avg_small_ratio > 1.0:
                print(f"   ğŸ“¦ Compression groupÃ©e petits fichiers: {len(small_files)} fichiers < 10KB")
        
        self.size_statistics['efficiency'] = {
            'without_encyclopedias': efficiency_without_encyclopedias,
            'with_encyclopedias': efficiency_with_encyclopedias,
            'encyclopedia_overhead_per_file': encyclopedia_overhead_per_file,
            'viable_threshold': viable_threshold,
            'by_size': efficiency_by_size
        }

    def generate_size_optimization_report(self):
        """GÃ©nÃ©rer rapport d'optimisation des tailles"""
        print("\nğŸ“„ GÃ‰NÃ‰RATION RAPPORT OPTIMISATION TAILLES")
        print("=" * 60)
        
        report_filename = f"PANINI_SIZE_ANALYSIS_REPORT_{self.timestamp.replace(':', '-')}.md"
        
        # Statistiques clÃ©s
        files_stats = self.size_statistics['files']
        encyc_stats = self.size_statistics['encyclopedias']
        efficiency_stats = self.size_statistics['efficiency']
        
        report_content = f"""# ğŸ“ PANINI-FS SIZE ANALYSIS REPORT

## ğŸ“Š Vue d'ensemble

**Session d'analyse:** {self.timestamp}  
**Fichiers analysÃ©s:** {files_stats['total_files']}  
**DonnÃ©es totales:** {self._format_size(files_stats['total_original'])}

## ğŸ—œï¸ Analyse fichiers compressÃ©s

**Tailles globales:**
- Original: {self._format_size(files_stats['total_original'])}
- CompressÃ©: {self._format_size(files_stats['total_compressed'])}
- MÃ©tadonnÃ©es: {self._format_size(files_stats['total_metadata'])}
- **Total PaniniFS: {self._format_size(files_stats['total_panini'])}**

**Ratios:**
- Compression seule: {files_stats['global_compression_ratio']:.3f}
- **Overhead mÃ©tadonnÃ©es: {files_stats['global_metadata_overhead']:.3f} ({(files_stats['global_metadata_overhead']*100):.1f}%)**
- **Ratio PaniniFS total: {files_stats['global_total_ratio']:.3f}**

## ğŸ“š Analyse encyclopÃ©dies

**Taille totale encyclopÃ©dies:** {self._format_size(encyc_stats['total_size'])}  
**Overhead par fichier:** {self._format_size(efficiency_stats['encyclopedia_overhead_per_file'])}

"""

        # Ajouter dÃ©tails par catÃ©gorie
        for category, data in encyc_stats['by_category'].items():
            report_content += f"- **{category}**: {self._format_size(data['size'])}\n"
        
        report_content += f"""

## âš¡ EfficacitÃ© stockage

**Sans encyclopÃ©dies:** {efficiency_stats['without_encyclopedias']:.3f}  
**Avec encyclopÃ©dies:** {efficiency_stats['with_encyclopedias']:.3f}

"""
        
        if efficiency_stats['viable_threshold']:
            report_content += f"**ğŸ¯ Seuil viabilitÃ© Ã©conomique:** â‰¥ {self._format_size(efficiency_stats['viable_threshold'])}\n\n"
        else:
            report_content += "**âš ï¸ Aucun seuil viable identifiÃ©**\n\n"
        
        # EfficacitÃ© par taille
        report_content += "**EfficacitÃ© par taille de fichier:**\n"
        for analysis in efficiency_stats['by_size']:
            viability = "âœ…" if analysis['viable'] else "âŒ"
            report_content += f"- Fichiers â‰¥ {self._format_size(analysis['threshold'])}: {analysis['efficiency']:.3f} ({analysis['files_count']} fichiers) {viability}\n"
        
        report_content += f"""

## ğŸ’¡ Recommandations d'optimisation

### RÃ©duction overhead mÃ©tadonnÃ©es
- Overhead actuel: {files_stats['global_metadata_overhead']:.1%}
- Optimisation possible: Compression mÃ©tadonnÃ©es, format binaire

### Optimisation encyclopÃ©dies  
- Taille actuelle: {self._format_size(encyc_stats['total_size'])}
- StratÃ©gie: EncyclopÃ©dies partagÃ©es, compression diffÃ©rentielle

### Seuils de viabilitÃ©
- Recommandation: Utiliser PaniniFS pour fichiers â‰¥ {self._format_size(efficiency_stats['viable_threshold']) if efficiency_stats['viable_threshold'] else '1MB'}
- Alternative: Compression groupÃ©e pour petits fichiers

---

*Rapport gÃ©nÃ©rÃ© par PaniniFS Size Analysis Engine*  
*BasÃ© sur analyse de {files_stats['total_files']} fichiers rÃ©els*
"""
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“„ Rapport sauvegardÃ©: {report_filename}")
        
        # Sauvegarder donnÃ©es JSON
        data_filename = f"PANINI_SIZE_ANALYSIS_DATA_{self.timestamp.replace(':', '-')}.json"
        with open(data_filename, 'w', encoding='utf-8') as f:
            json.dump(self.size_statistics, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ’¾ DonnÃ©es sauvegardÃ©es: {data_filename}")
        
        return report_filename, data_filename

    def _format_size(self, size_bytes):
        """Formater taille en unitÃ©s lisibles"""
        if size_bytes == 0:
            return "0 B"
        
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"

    def run_complete_size_analysis(self):
        """ExÃ©cuter l'analyse complÃ¨te des tailles"""
        print(f"""
ğŸš€ DÃ‰MARRAGE ANALYSE COMPLÃˆTE TAILLES PANINI-FS
============================================================
ğŸ¯ Mission: Analyser efficacitÃ© stockage systÃ¨me complet
ğŸ“Š Analyses: Fichiers + MÃ©tadonnÃ©es + EncyclopÃ©dies + EfficacitÃ©
âš¡ Objectif: Optimiser overhead et valider viabilitÃ© Ã©conomique

ğŸ” Phase 1: Analyse fichiers compressÃ©s...
""")
        
        try:
            # Phase 1: Analyser fichiers compressÃ©s
            file_analysis = self.analyze_compressed_files_sizes()
            
            if not file_analysis:
                print("âŒ Aucun fichier compressÃ© trouvÃ© pour analyse")
                return False
            
            # Phase 2: Analyser overhead mÃ©tadonnÃ©es
            self.analyze_metadata_overhead(file_analysis)
            
            # Phase 3: Analyser encyclopÃ©dies
            encyclopedia_analysis = self.analyze_encyclopedia_sizes()
            
            # Phase 4: Calculer efficacitÃ© globale
            self.calculate_storage_efficiency(file_analysis, encyclopedia_analysis)
            
            # Phase 5: GÃ©nÃ©rer rapport
            report_file, data_file = self.generate_size_optimization_report()
            
            # RÃ©sumÃ© final
            total_original = self.size_statistics['files']['total_original']
            total_panini = self.size_statistics['files']['total_panini']
            total_with_encyc = total_panini + self.size_statistics['encyclopedias']['total_size']
            
            final_efficiency = total_with_encyc / total_original if total_original > 0 else 1
            
            print(f"""
ğŸ‰ ANALYSE TAILLES TERMINÃ‰E AVEC SUCCÃˆS !
============================================================
ğŸ“Š RÃ©sultats clÃ©s:
   DonnÃ©es originales      : {self._format_size(total_original)}
   PaniniFS (fichiers)     : {self._format_size(total_panini)}
   EncyclopÃ©dies           : {self._format_size(self.size_statistics['encyclopedias']['total_size'])}
   **TOTAL SYSTÃˆME**       : {self._format_size(total_with_encyc)}
   
ğŸ¯ EfficacitÃ© finale       : {final_efficiency:.3f}
   Ã‰conomie/Overhead       : {((1-final_efficiency)*100):+.1f}%
   
ğŸ“„ Rapport dÃ©taillÃ©       : {report_file}
ğŸ’¾ DonnÃ©es complÃ¨tes      : {data_file}

ğŸš€ ANALYSE PRÃŠTE POUR OPTIMISATIONS !
""")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ Erreur dans analyse tailles: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """Point d'entrÃ©e principal"""
    print(f"""
ğŸ“ PANINI-FS SIZE ANALYSIS ENGINE
============================================================
ğŸ¯ Mission: Analyser tailles reprÃ©sentations et encyclopÃ©dies
ğŸ“Š Objectif: Mesurer overhead et optimiser efficacitÃ© stockage

Analyses complÃ¨tes:
- Tailles fichiers compressÃ©s vs originaux
- Overhead mÃ©tadonnÃ©es dÃ©taillÃ©
- Tailles encyclopÃ©dies de connaissances  
- EfficacitÃ© stockage globale
- Seuils viabilitÃ© Ã©conomique
- Recommandations optimisation

ğŸš€ Initialisation moteur d'analyse...
""")
    
    try:
        engine = PaniniSizeAnalysisEngine()
        success = engine.run_complete_size_analysis()
        
        if success:
            print(f"""
âœ… MISSION ANALYSE TAILLES ACCOMPLIE
==================================
ğŸ“Š Analyse complÃ¨te rÃ©alisÃ©e avec succÃ¨s
ğŸ¯ Overhead systÃ¨me quantifiÃ© prÃ©cisÃ©ment  
ğŸ’¡ Optimisations identifiÃ©es et documentÃ©es
ğŸ“„ Rapport dÃ©taillÃ© gÃ©nÃ©rÃ©

ğŸš€ PrÃªt pour phase d'optimisation !
""")
            return True
        else:
            print("âŒ Ã‰chec analyse tailles")
            return False
    
    except Exception as e:
        print(f"""
âŒ ERREUR ANALYSE TAILLES
========================
ğŸ”§ Erreur: {str(e)}
""")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)