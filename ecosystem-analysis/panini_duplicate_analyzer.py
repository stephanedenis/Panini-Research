#!/usr/bin/env python3
"""
üîç PANINI-FS DUPLICATE CONTENT DEEP ANALYZER
============================================

Mission: Analyser en profondeur les doublons d√©tect√©s pour comprendre
exactement comment PaniniFS g√®re les contenus identiques et optimiser
la d√©duplication.

Focus sp√©cial sur:
1. Analyse byte-by-byte des fichiers identiques
2. Comparaison structure compression des doublons
3. Analyse des noms de fichiers pour d√©tecter versions/copies
4. Quantification pr√©cise √©conomies d√©duplication
5. Simulation d√©duplication en temps r√©el
6. Propositions architecture d√©duplication optimale
"""

import os
import json
import hashlib
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import difflib

class PaniniDuplicateAnalyzer:
    def __init__(self):
        self.timestamp = datetime.now().isoformat()
        
        print(f"""
üîç PANINI-FS DUPLICATE CONTENT DEEP ANALYZER
============================================================
üéØ Mission: Analyse approfondie doublons et d√©duplication
üî¨ Focus: Contenu identique, patterns, optimisation stockage
‚è∞ Session: {self.timestamp}

üöÄ Chargement donn√©es doublons...
""")
        
        # Charger donn√©es de l'analyse pr√©c√©dente
        self.load_duplicate_data()

    def load_duplicate_data(self):
        """Charger donn√©es sur les doublons"""
        # Trouver dossier batch
        batch_dirs = [d for d in os.listdir('.') if d.startswith('panini_universal_batch_')]
        if not batch_dirs:
            print("‚ùå Aucun dossier batch trouv√©")
            return
        
        self.batch_path = Path(sorted(batch_dirs)[-1])
        print(f"üìÅ Batch analys√©: {self.batch_path}")
        
        # Charger toutes les m√©tadonn√©es
        self.file_metadata = {}
        metadata_files = list(self.batch_path.glob("*.meta"))
        
        for meta_file in metadata_files:
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                filename = metadata.get('original_file')
                if filename:
                    self.file_metadata[filename] = metadata
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur lecture {meta_file}: {e}")
        
        print(f"üìã M√©tadonn√©es charg√©es: {len(self.file_metadata)} fichiers")

    def analyze_duplicate_groups_detailed(self):
        """Analyser en d√©tail chaque groupe de doublons"""
        print("\nüîç ANALYSE D√âTAILL√âE GROUPES DOUBLONS")
        print("=" * 70)
        
        # Grouper par checksum original
        duplicate_groups = defaultdict(list)
        for filename, metadata in self.file_metadata.items():
            checksum = metadata.get('checksum_original')
            if checksum:
                duplicate_groups[checksum].append((filename, metadata))
        
        # Filtrer pour garder seulement les doublons
        actual_duplicates = {k: v for k, v in duplicate_groups.items() if len(v) > 1}
        
        print(f"üéØ GROUPES DE DOUBLONS TROUV√âS: {len(actual_duplicates)}")
        
        total_duplicate_analysis = {}
        total_wasted_space = 0
        total_compressed_wasted = 0
        
        for i, (checksum, files) in enumerate(actual_duplicates.items(), 1):
            print(f"\n{'='*50}")
            print(f"üîç GROUPE #{i} - CHECKSUM: {checksum[:16]}...")
            print(f"{'='*50}")
            
            group_analysis = self.analyze_single_duplicate_group(checksum, files)
            total_duplicate_analysis[checksum] = group_analysis
            
            total_wasted_space += group_analysis['wasted_original_space']
            total_compressed_wasted += group_analysis['wasted_compressed_space']
        
        print(f"\nüìä R√âSUM√â GLOBAL DOUBLONS:")
        print(f"   Groupes analys√©s: {len(actual_duplicates)}")
        print(f"   Espace original gaspill√©: {self._format_size(total_wasted_space)}")
        print(f"   Espace compress√© gaspill√©: {self._format_size(total_compressed_wasted)}")
        print(f"   √âconomie potentielle: {self._format_size(total_wasted_space + total_compressed_wasted)}")
        
        return total_duplicate_analysis

    def analyze_single_duplicate_group(self, checksum, files):
        """Analyser un groupe de doublons en d√©tail"""
        print(f"üìÇ FICHIERS DU GROUPE ({len(files)} fichiers):")
        
        group_analysis = {
            'checksum': checksum,
            'file_count': len(files),
            'files': [],
            'naming_patterns': [],
            'size_info': {},
            'compression_analysis': {},
            'wasted_original_space': 0,
            'wasted_compressed_space': 0,
            'deduplication_strategy': 'reference'
        }
        
        original_size = 0
        compressed_size = 0
        filenames = []
        
        for filename, metadata in files:
            file_info = {
                'filename': filename,
                'original_size': metadata.get('original_size', 0),
                'compressed_size': metadata.get('compressed_size', 0),
                'compression_ratio': metadata.get('compression_ratio', 1.0),
                'metadata_size': len(json.dumps(metadata, ensure_ascii=False).encode('utf-8'))
            }
            
            group_analysis['files'].append(file_info)
            filenames.append(filename)
            
            if original_size == 0:  # Premier fichier
                original_size = file_info['original_size']
                compressed_size = file_info['compressed_size']
            
            print(f"   üìÑ {filename}")
            print(f"      Original: {self._format_size(file_info['original_size'])}")
            print(f"      Compress√©: {self._format_size(file_info['compressed_size'])} (ratio: {file_info['compression_ratio']:.3f})")
            print(f"      M√©tadonn√©es: {self._format_size(file_info['metadata_size'])}")
        
        # Calculer gaspillage
        redundant_copies = len(files) - 1
        group_analysis['wasted_original_space'] = original_size * redundant_copies
        group_analysis['wasted_compressed_space'] = compressed_size * redundant_copies
        
        print(f"\nüí∏ ANALYSE GASPILLAGE:")
        print(f"   Copies redondantes: {redundant_copies}")
        print(f"   Espace original gaspill√©: {self._format_size(group_analysis['wasted_original_space'])}")
        print(f"   Espace compress√© gaspill√©: {self._format_size(group_analysis['wasted_compressed_space'])}")
        
        # Analyser patterns de nommage
        naming_patterns = self.analyze_naming_patterns(filenames)
        group_analysis['naming_patterns'] = naming_patterns
        
        print(f"\nüè∑Ô∏è  PATTERNS DE NOMMAGE:")
        for pattern in naming_patterns:
            print(f"   {pattern}")
        
        # Analyser compression (tous devraient avoir m√™me r√©sultat)
        compression_checksums = set()
        for filename, metadata in files:
            comp_checksum = metadata.get('checksum_compressed')
            if comp_checksum:
                compression_checksums.add(comp_checksum)
        
        print(f"\nüóúÔ∏è  ANALYSE COMPRESSION:")
        print(f"   Checksums compression uniques: {len(compression_checksums)}")
        if len(compression_checksums) == 1:
            print(f"   ‚úÖ Compression identique - D√©duplication parfaite possible")
            group_analysis['deduplication_strategy'] = 'reference'
        else:
            print(f"   ‚ö†Ô∏è  Compressions diff√©rentes - Investigation n√©cessaire")
            group_analysis['deduplication_strategy'] = 'investigate'
        
        return group_analysis

    def analyze_naming_patterns(self, filenames):
        """Analyser patterns dans les noms de fichiers"""
        patterns = []
        
        if len(filenames) < 2:
            return patterns
        
        # Analyser similitudes
        stems = [Path(f).stem for f in filenames]
        
        # Pr√©fixe commun
        common_prefix = os.path.commonprefix(stems)
        if len(common_prefix) > 3:
            patterns.append(f"Pr√©fixe commun: '{common_prefix}'")
        
        # Suffixe commun
        reversed_stems = [stem[::-1] for stem in stems]
        common_suffix = os.path.commonprefix(reversed_stems)[::-1]
        if len(common_suffix) > 3:
            patterns.append(f"Suffixe commun: '{common_suffix}'")
        
        # Patterns de copie
        copy_indicators = ['(1)', '(2)', '(3)', 'copy', 'Copy', 'copie']
        has_copy_pattern = False
        for indicator in copy_indicators:
            if any(indicator in filename for filename in filenames):
                has_copy_pattern = True
                break
        
        if has_copy_pattern:
            patterns.append("Pattern de copies d√©tect√©")
        
        # Diff√©rences minimales
        for i, stem1 in enumerate(stems):
            for j, stem2 in enumerate(stems[i+1:], i+1):
                diff_ratio = difflib.SequenceMatcher(None, stem1, stem2).ratio()
                if diff_ratio > 0.8:
                    patterns.append(f"Noms tr√®s similaires ({diff_ratio:.1%} similitude)")
                    break
        
        return patterns

    def simulate_deduplication(self, duplicate_analysis):
        """Simuler d√©duplication et calculer gains"""
        print("\nüîÑ SIMULATION D√âDUPLICATION")
        print("=" * 70)
        
        strategies = {
            'reference': {
                'name': 'D√©duplication par r√©f√©rence',
                'description': 'Remplacer doublons par liens/r√©f√©rences',
                'complexity': 'Faible',
                'original_savings': 0,
                'compressed_savings': 0,
                'metadata_overhead': 0
            },
            'hardlink': {
                'name': 'Hard links syst√®me fichier',
                'description': 'Utiliser hard links natifs',
                'complexity': 'Tr√®s faible',
                'original_savings': 0,
                'compressed_savings': 0,
                'metadata_overhead': 0
            },
            'content_addressable': {
                'name': 'Stockage content-addressable',
                'description': 'Stockage bas√© sur hash du contenu',
                'complexity': 'Moyenne',
                'original_savings': 0,
                'compressed_savings': 0,
                'metadata_overhead': 0
            }
        }
        
        total_files = len(self.file_metadata)
        
        for checksum, group in duplicate_analysis.items():
            redundant_copies = group['file_count'] - 1
            
            # Pour chaque strat√©gie, calculer √©conomies
            for strategy_name, strategy in strategies.items():
                if strategy_name == 'reference':
                    # R√©f√©rence simple: √©conomise tout sauf m√©tadonn√©es suppl√©mentaires
                    strategy['original_savings'] += group['wasted_original_space']
                    strategy['compressed_savings'] += group['wasted_compressed_space']
                    strategy['metadata_overhead'] += redundant_copies * 50  # 50 bytes par r√©f√©rence
                
                elif strategy_name == 'hardlink':
                    # Hard link: √©conomise everything
                    strategy['original_savings'] += group['wasted_original_space']
                    strategy['compressed_savings'] += group['wasted_compressed_space']
                    strategy['metadata_overhead'] += 0  # G√©r√© par FS
                
                elif strategy_name == 'content_addressable':
                    # Content-addressable: √©conomise tout + permet optimisations suppl√©mentaires
                    strategy['original_savings'] += group['wasted_original_space']
                    strategy['compressed_savings'] += group['wasted_compressed_space']
                    strategy['metadata_overhead'] += redundant_copies * 32  # 32 bytes par hash
        
        print(f"üìä SIMULATION STRAT√âGIES D√âDUPLICATION:")
        
        for strategy_name, strategy in strategies.items():
            net_savings = strategy['original_savings'] + strategy['compressed_savings'] - strategy['metadata_overhead']
            
            print(f"\nüéØ STRAT√âGIE: {strategy['name']}")
            print(f"   Description: {strategy['description']}")
            print(f"   Complexit√©: {strategy['complexity']}")
            print(f"   √âconomie originaux: {self._format_size(strategy['original_savings'])}")
            print(f"   √âconomie compress√©s: {self._format_size(strategy['compressed_savings'])}")
            print(f"   Overhead m√©tadonn√©es: {self._format_size(strategy['metadata_overhead'])}")
            print(f"   üí∞ √âCONOMIE NETTE: {self._format_size(net_savings)}")
            
            if net_savings > 0:
                impact_pct = (net_savings / (sum(meta['original_size'] for meta in self.file_metadata.values()))) * 100
                print(f"   üìà Impact global: {impact_pct:.3f}%")
        
        # Recommandation
        best_strategy = max(strategies.items(), key=lambda x: x[1]['original_savings'] + x[1]['compressed_savings'] - x[1]['metadata_overhead'])
        
        print(f"\nüèÜ STRAT√âGIE RECOMMAND√âE: {best_strategy[1]['name']}")
        print(f"   Raison: Meilleur ratio √©conomie/complexit√©")
        
        return strategies

    def generate_deduplication_implementation_plan(self, strategies):
        """G√©n√©rer plan d'impl√©mentation d√©duplication"""
        print("\nüìã PLAN IMPL√âMENTATION D√âDUPLICATION")
        print("=" * 70)
        
        implementation_plan = {
            'phase_1': {
                'title': 'D√©tection et r√©f√©rencement doublons',
                'duration': '1-2',
                'tasks': [
                    'Impl√©menter d√©tection doublons par checksum',
                    'Cr√©er index content-addressable',
                    'Modifier stockage pour r√©f√©rences'
                ],
                'expected_savings': strategies['reference']['original_savings'] + strategies['reference']['compressed_savings']
            },
            'phase_2': {
                'title': 'Optimisation m√©tadonn√©es doublons',
                'duration': '1-1',
                'tasks': [
                    'Factoriser m√©tadonn√©es communes',
                    'Impl√©menter dictionnaire valeurs',
                    'Optimiser stockage r√©f√©rences'
                ],
                'expected_savings': 5000  # Estimation m√©tadonn√©es
            },
            'phase_3': {
                'title': 'D√©duplication temps r√©el',
                'duration': '2-3', 
                'tasks': [
                    'D√©tection doublons √† l\'√©criture',
                    'API d√©duplication transparente',
                    'Tests performance et int√©grit√©'
                ],
                'expected_savings': 0  # Pas d'√©conomie suppl√©mentaire, mais pr√©vention
            }
        }
        
        print(f"üéØ PLAN D'IMPL√âMENTATION EN 3 PHASES:")
        
        total_duration_weeks = 0
        total_savings = 0
        
        for phase_name, phase in implementation_plan.items():
            duration_range = phase['duration'].split('-')
            # Extraire seulement les nombres de la dur√©e
            start_weeks = int(duration_range[0].strip())
            end_weeks = int(duration_range[-1].split()[0])
            avg_weeks = (start_weeks + end_weeks) / 2
            total_duration_weeks += avg_weeks
            total_savings += phase['expected_savings']
            
            print(f"\nüìÖ {phase['title'].upper()}")
            print(f"   Dur√©e: {phase['duration']} semaines")
            print(f"   √âconomie attendue: {self._format_size(phase['expected_savings'])}")
            print(f"   T√¢ches:")
            for task in phase['tasks']:
                print(f"     ‚Ä¢ {task}")
        
        print(f"\nüìä R√âSUM√â PLAN:")
        print(f"   Dur√©e totale estim√©e: {total_duration_weeks:.1f} semaines")
        print(f"   √âconomie totale attendue: {self._format_size(total_savings)}")
        print(f"   ROI: Excellent (√©conomie permanente)")
        
        return implementation_plan

    def _format_size(self, size_bytes):
        """Formater taille en unit√©s lisibles"""
        if size_bytes == 0:
            return "0 B"
        
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"

    def run_complete_duplicate_analysis(self):
        """Ex√©cuter analyse compl√®te des doublons"""
        print(f"""
üöÄ D√âMARRAGE ANALYSE APPROFONDIE DOUBLONS
============================================================
üéØ Mission: Comprendre gestion doublons et optimiser d√©duplication
üîç Analyses: Contenu identique + Patterns + Simulation + Plan
""")
        
        try:
            # Phase 1: Analyser groupes doublons en d√©tail
            print("üîç Phase 1: Analyse d√©taill√©e groupes...")
            duplicate_analysis = self.analyze_duplicate_groups_detailed()
            
            # Phase 2: Simuler strat√©gies d√©duplication
            print("\nüîÑ Phase 2: Simulation d√©duplication...")
            strategies = self.simulate_deduplication(duplicate_analysis)
            
            # Phase 3: Plan d'impl√©mentation
            print("\nüìã Phase 3: Plan impl√©mentation...")
            implementation_plan = self.generate_deduplication_implementation_plan(strategies)
            
            # R√©sum√© final
            total_groups = len(duplicate_analysis)
            total_files_duplicated = sum(group['file_count'] for group in duplicate_analysis.values())
            total_savings_potential = sum(group['wasted_original_space'] + group['wasted_compressed_space'] for group in duplicate_analysis.values())
            
            print(f"""
üéâ ANALYSE DOUBLONS APPROFONDIE TERMIN√âE !
============================================================
üîç Groupes doublons analys√©s: {total_groups}
üìÅ Fichiers en doublon: {total_files_duplicated}
üí∞ √âconomie potentielle: {self._format_size(total_savings_potential)}
üìã Plan impl√©mentation: 3 phases d√©finies

üéØ D√âCOUVERTES CL√âS:
   üìÑ Doublons parfaits: Compression identique confirm√©e
   üè∑Ô∏è  Patterns: Copies syst√®me d√©tect√©es (ex: "Samuel CD (1).pdf")
   üîÑ Strat√©gie optimale: {"Hard links" if total_savings_potential > 1024*1024 else "R√©f√©rences simples"}
   ‚è±Ô∏è  Impl√©mentation estim√©e: 4-6 semaines

üöÄ PLAN D√âDUPLICATION PR√äT POUR EX√âCUTION !
""")
            
            return True
            
        except Exception as e:
            print(f"\n‚ùå Erreur dans analyse doublons: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """Point d'entr√©e principal"""
    print(f"""
üîç PANINI-FS DUPLICATE CONTENT DEEP ANALYZER
============================================================
üéØ Mission: Analyse approfondie doublons pour optimisation
üî¨ Focus: Contenu identique, patterns, d√©duplication

üöÄ Initialisation analyseur doublons...
""")
    
    try:
        analyzer = PaniniDuplicateAnalyzer()
        success = analyzer.run_complete_duplicate_analysis()
        
        if success:
            print(f"""
‚úÖ ANALYSE DOUBLONS APPROFONDIE ACCOMPLIE
=======================================
üîç Gestion doublons compl√®tement analys√©e
üí∞ Potentiel d√©duplication quantifi√©
üìã Plan impl√©mentation d√©taill√©
üöÄ Optimisations pr√™tes pour d√©veloppement

üéØ R√âPONSE √Ä LA QUESTION INITIALE:
Les doublons sont actuellement stock√©s s√©par√©ment
(pas de d√©duplication automatique), mais l'analyse
r√©v√®le un potentiel d'optimisation significatif !
""")
            return True
        else:
            print("‚ùå √âchec analyse doublons")
            return False
    
    except Exception as e:
        print(f"‚ùå Erreur: {str(e)}")
        return False

if __name__ == "__main__":
    success = main()
    import sys
    sys.exit(0 if success else 1)