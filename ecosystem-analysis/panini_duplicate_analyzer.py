#!/usr/bin/env python3
"""
ğŸ” PANINI-FS DUPLICATE CONTENT DEEP ANALYZER
============================================

Mission: Analyser en profondeur les doublons dÃ©tectÃ©s pour comprendre
exactement comment PaniniFS gÃ¨re les contenus identiques et optimiser
la dÃ©duplication.

Focus spÃ©cial sur:
1. Analyse byte-by-byte des fichiers identiques
2. Comparaison structure compression des doublons
3. Analyse des noms de fichiers pour dÃ©tecter versions/copies
4. Quantification prÃ©cise Ã©conomies dÃ©duplication
5. Simulation dÃ©duplication en temps rÃ©el
6. Propositions architecture dÃ©duplication optimale
"""

import os
import json
import hashlib
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import difflib

class PaniniDuplicateAnalyzer:
    def __init__(self):
        self.timestamp = datetime.now().isoformat()
        
        print(f"""
ğŸ” PANINI-FS DUPLICATE CONTENT DEEP ANALYZER
============================================================
ğŸ¯ Mission: Analyse approfondie doublons et dÃ©duplication
ğŸ”¬ Focus: Contenu identique, patterns, optimisation stockage
â° Session: {self.timestamp}

ğŸš€ Chargement donnÃ©es doublons...
""")
        
        # Charger donnÃ©es de l'analyse prÃ©cÃ©dente
        self.load_duplicate_data()

    def load_duplicate_data(self):
        """Charger donnÃ©es sur les doublons"""
        # Trouver dossier batch
        batch_dirs = [d for d in os.listdir('.') if d.startswith('panini_universal_batch_')]
        if not batch_dirs:
            print("âŒ Aucun dossier batch trouvÃ©")
            return
        
        self.batch_path = Path(sorted(batch_dirs)[-1])
        print(f"ğŸ“ Batch analysÃ©: {self.batch_path}")
        
        # Charger toutes les mÃ©tadonnÃ©es
        self.file_metadata = {}
        metadata_files = list(self.batch_path.glob("*.meta"))
        
        for meta_file in metadata_files:
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                filename = metadata.get('original_file')
                if filename:
                    self.file_metadata[filename] = metadata
            except Exception as e:
                print(f"âš ï¸  Erreur lecture {meta_file}: {e}")
        
        print(f"ğŸ“‹ MÃ©tadonnÃ©es chargÃ©es: {len(self.file_metadata)} fichiers")

    def analyze_duplicate_groups_detailed(self):
        """Analyser en dÃ©tail chaque groupe de doublons"""
        print("\nğŸ” ANALYSE DÃ‰TAILLÃ‰E GROUPES DOUBLONS")
        print("=" * 70)
        
        # Grouper par checksum original
        duplicate_groups = defaultdict(list)
        for filename, metadata in self.file_metadata.items():
            checksum = metadata.get('checksum_original')
            if checksum:
                duplicate_groups[checksum].append((filename, metadata))
        
        # Filtrer pour garder seulement les doublons
        actual_duplicates = {k: v for k, v in duplicate_groups.items() if len(v) > 1}
        
        print(f"ğŸ¯ GROUPES DE DOUBLONS TROUVÃ‰S: {len(actual_duplicates)}")
        
        total_duplicate_analysis = {}
        total_wasted_space = 0
        total_compressed_wasted = 0
        
        for i, (checksum, files) in enumerate(actual_duplicates.items(), 1):
            print(f"\n{'='*50}")
            print(f"ğŸ” GROUPE #{i} - CHECKSUM: {checksum[:16]}...")
            print(f"{'='*50}")
            
            group_analysis = self.analyze_single_duplicate_group(checksum, files)
            total_duplicate_analysis[checksum] = group_analysis
            
            total_wasted_space += group_analysis['wasted_original_space']
            total_compressed_wasted += group_analysis['wasted_compressed_space']
        
        print(f"\nğŸ“Š RÃ‰SUMÃ‰ GLOBAL DOUBLONS:")
        print(f"   Groupes analysÃ©s: {len(actual_duplicates)}")
        print(f"   Espace original gaspillÃ©: {self._format_size(total_wasted_space)}")
        print(f"   Espace compressÃ© gaspillÃ©: {self._format_size(total_compressed_wasted)}")
        print(f"   Ã‰conomie potentielle: {self._format_size(total_wasted_space + total_compressed_wasted)}")
        
        return total_duplicate_analysis

    def analyze_single_duplicate_group(self, checksum, files):
        """Analyser un groupe de doublons en dÃ©tail"""
        print(f"ğŸ“‚ FICHIERS DU GROUPE ({len(files)} fichiers):")
        
        group_analysis = {
            'checksum': checksum,
            'file_count': len(files),
            'files': [],
            'naming_patterns': [],
            'size_info': {},
            'compression_analysis': {},
            'wasted_original_space': 0,
            'wasted_compressed_space': 0,
            'deduplication_strategy': 'reference'
        }
        
        original_size = 0
        compressed_size = 0
        filenames = []
        
        for filename, metadata in files:
            file_info = {
                'filename': filename,
                'original_size': metadata.get('original_size', 0),
                'compressed_size': metadata.get('compressed_size', 0),
                'compression_ratio': metadata.get('compression_ratio', 1.0),
                'metadata_size': len(json.dumps(metadata, ensure_ascii=False).encode('utf-8'))
            }
            
            group_analysis['files'].append(file_info)
            filenames.append(filename)
            
            if original_size == 0:  # Premier fichier
                original_size = file_info['original_size']
                compressed_size = file_info['compressed_size']
            
            print(f"   ğŸ“„ {filename}")
            print(f"      Original: {self._format_size(file_info['original_size'])}")
            print(f"      CompressÃ©: {self._format_size(file_info['compressed_size'])} (ratio: {file_info['compression_ratio']:.3f})")
            print(f"      MÃ©tadonnÃ©es: {self._format_size(file_info['metadata_size'])}")
        
        # Calculer gaspillage
        redundant_copies = len(files) - 1
        group_analysis['wasted_original_space'] = original_size * redundant_copies
        group_analysis['wasted_compressed_space'] = compressed_size * redundant_copies
        
        print(f"\nğŸ’¸ ANALYSE GASPILLAGE:")
        print(f"   Copies redondantes: {redundant_copies}")
        print(f"   Espace original gaspillÃ©: {self._format_size(group_analysis['wasted_original_space'])}")
        print(f"   Espace compressÃ© gaspillÃ©: {self._format_size(group_analysis['wasted_compressed_space'])}")
        
        # Analyser patterns de nommage
        naming_patterns = self.analyze_naming_patterns(filenames)
        group_analysis['naming_patterns'] = naming_patterns
        
        print(f"\nğŸ·ï¸  PATTERNS DE NOMMAGE:")
        for pattern in naming_patterns:
            print(f"   {pattern}")
        
        # Analyser compression (tous devraient avoir mÃªme rÃ©sultat)
        compression_checksums = set()
        for filename, metadata in files:
            comp_checksum = metadata.get('checksum_compressed')
            if comp_checksum:
                compression_checksums.add(comp_checksum)
        
        print(f"\nğŸ—œï¸  ANALYSE COMPRESSION:")
        print(f"   Checksums compression uniques: {len(compression_checksums)}")
        if len(compression_checksums) == 1:
            print(f"   âœ… Compression identique - DÃ©duplication parfaite possible")
            group_analysis['deduplication_strategy'] = 'reference'
        else:
            print(f"   âš ï¸  Compressions diffÃ©rentes - Investigation nÃ©cessaire")
            group_analysis['deduplication_strategy'] = 'investigate'
        
        return group_analysis

    def analyze_naming_patterns(self, filenames):
        """Analyser patterns dans les noms de fichiers"""
        patterns = []
        
        if len(filenames) < 2:
            return patterns
        
        # Analyser similitudes
        stems = [Path(f).stem for f in filenames]
        
        # PrÃ©fixe commun
        common_prefix = os.path.commonprefix(stems)
        if len(common_prefix) > 3:
            patterns.append(f"PrÃ©fixe commun: '{common_prefix}'")
        
        # Suffixe commun
        reversed_stems = [stem[::-1] for stem in stems]
        common_suffix = os.path.commonprefix(reversed_stems)[::-1]
        if len(common_suffix) > 3:
            patterns.append(f"Suffixe commun: '{common_suffix}'")
        
        # Patterns de copie
        copy_indicators = ['(1)', '(2)', '(3)', 'copy', 'Copy', 'copie']
        has_copy_pattern = False
        for indicator in copy_indicators:
            if any(indicator in filename for filename in filenames):
                has_copy_pattern = True
                break
        
        if has_copy_pattern:
            patterns.append("Pattern de copies dÃ©tectÃ©")
        
        # DiffÃ©rences minimales
        for i, stem1 in enumerate(stems):
            for j, stem2 in enumerate(stems[i+1:], i+1):
                diff_ratio = difflib.SequenceMatcher(None, stem1, stem2).ratio()
                if diff_ratio > 0.8:
                    patterns.append(f"Noms trÃ¨s similaires ({diff_ratio:.1%} similitude)")
                    break
        
        return patterns

    def simulate_deduplication(self, duplicate_analysis):
        """Simuler dÃ©duplication et calculer gains"""
        print("\nğŸ”„ SIMULATION DÃ‰DUPLICATION")
        print("=" * 70)
        
        strategies = {
            'reference': {
                'name': 'DÃ©duplication par rÃ©fÃ©rence',
                'description': 'Remplacer doublons par liens/rÃ©fÃ©rences',
                'complexity': 'Faible',
                'original_savings': 0,
                'compressed_savings': 0,
                'metadata_overhead': 0
            },
            'hardlink': {
                'name': 'Hard links systÃ¨me fichier',
                'description': 'Utiliser hard links natifs',
                'complexity': 'TrÃ¨s faible',
                'original_savings': 0,
                'compressed_savings': 0,
                'metadata_overhead': 0
            },
            'content_addressable': {
                'name': 'Stockage content-addressable',
                'description': 'Stockage basÃ© sur hash du contenu',
                'complexity': 'Moyenne',
                'original_savings': 0,
                'compressed_savings': 0,
                'metadata_overhead': 0
            }
        }
        
        total_files = len(self.file_metadata)
        
        for checksum, group in duplicate_analysis.items():
            redundant_copies = group['file_count'] - 1
            
            # Pour chaque stratÃ©gie, calculer Ã©conomies
            for strategy_name, strategy in strategies.items():
                if strategy_name == 'reference':
                    # RÃ©fÃ©rence simple: Ã©conomise tout sauf mÃ©tadonnÃ©es supplÃ©mentaires
                    strategy['original_savings'] += group['wasted_original_space']
                    strategy['compressed_savings'] += group['wasted_compressed_space']
                    strategy['metadata_overhead'] += redundant_copies * 50  # 50 bytes par rÃ©fÃ©rence
                
                elif strategy_name == 'hardlink':
                    # Hard link: Ã©conomise everything
                    strategy['original_savings'] += group['wasted_original_space']
                    strategy['compressed_savings'] += group['wasted_compressed_space']
                    strategy['metadata_overhead'] += 0  # GÃ©rÃ© par FS
                
                elif strategy_name == 'content_addressable':
                    # Content-addressable: Ã©conomise tout + permet optimisations supplÃ©mentaires
                    strategy['original_savings'] += group['wasted_original_space']
                    strategy['compressed_savings'] += group['wasted_compressed_space']
                    strategy['metadata_overhead'] += redundant_copies * 32  # 32 bytes par hash
        
        print(f"ğŸ“Š SIMULATION STRATÃ‰GIES DÃ‰DUPLICATION:")
        
        for strategy_name, strategy in strategies.items():
            net_savings = strategy['original_savings'] + strategy['compressed_savings'] - strategy['metadata_overhead']
            
            print(f"\nğŸ¯ STRATÃ‰GIE: {strategy['name']}")
            print(f"   Description: {strategy['description']}")
            print(f"   ComplexitÃ©: {strategy['complexity']}")
            print(f"   Ã‰conomie originaux: {self._format_size(strategy['original_savings'])}")
            print(f"   Ã‰conomie compressÃ©s: {self._format_size(strategy['compressed_savings'])}")
            print(f"   Overhead mÃ©tadonnÃ©es: {self._format_size(strategy['metadata_overhead'])}")
            print(f"   ğŸ’° Ã‰CONOMIE NETTE: {self._format_size(net_savings)}")
            
            if net_savings > 0:
                impact_pct = (net_savings / (sum(meta['original_size'] for meta in self.file_metadata.values()))) * 100
                print(f"   ğŸ“ˆ Impact global: {impact_pct:.3f}%")
        
        # Recommandation
        best_strategy = max(strategies.items(), key=lambda x: x[1]['original_savings'] + x[1]['compressed_savings'] - x[1]['metadata_overhead'])
        
        print(f"\nğŸ† STRATÃ‰GIE RECOMMANDÃ‰E: {best_strategy[1]['name']}")
        print(f"   Raison: Meilleur ratio Ã©conomie/complexitÃ©")
        
        return strategies

    def generate_deduplication_implementation_plan(self, strategies):
        """GÃ©nÃ©rer plan d'implÃ©mentation dÃ©duplication"""
        print("\nğŸ“‹ PLAN IMPLÃ‰MENTATION DÃ‰DUPLICATION")
        print("=" * 70)
        
        implementation_plan = {
            'phase_1': {
                'title': 'DÃ©tection et rÃ©fÃ©rencement doublons',
                'duration': '1-2',
                'tasks': [
                    'ImplÃ©menter dÃ©tection doublons par checksum',
                    'CrÃ©er index content-addressable',
                    'Modifier stockage pour rÃ©fÃ©rences'
                ],
                'expected_savings': strategies['reference']['original_savings'] + strategies['reference']['compressed_savings']
            },
            'phase_2': {
                'title': 'Optimisation mÃ©tadonnÃ©es doublons',
                'duration': '1-1',
                'tasks': [
                    'Factoriser mÃ©tadonnÃ©es communes',
                    'ImplÃ©menter dictionnaire valeurs',
                    'Optimiser stockage rÃ©fÃ©rences'
                ],
                'expected_savings': 5000  # Estimation mÃ©tadonnÃ©es
            },
            'phase_3': {
                'title': 'DÃ©duplication temps rÃ©el',
                'duration': '2-3', 
                'tasks': [
                    'DÃ©tection doublons Ã  l\'Ã©criture',
                    'API dÃ©duplication transparente',
                    'Tests performance et intÃ©gritÃ©'
                ],
                'expected_savings': 0  # Pas d'Ã©conomie supplÃ©mentaire, mais prÃ©vention
            }
        }
        
        print(f"ğŸ¯ PLAN D'IMPLÃ‰MENTATION EN 3 PHASES:")
        
        total_duration_weeks = 0
        total_savings = 0
        
        for phase_name, phase in implementation_plan.items():
            duration_range = phase['duration'].split('-')
            # Extraire seulement les nombres de la durÃ©e
            start_weeks = int(duration_range[0].strip())
            end_weeks = int(duration_range[-1].split()[0])
            avg_weeks = (start_weeks + end_weeks) / 2
            total_duration_weeks += avg_weeks
            total_savings += phase['expected_savings']
            
            print(f"\nğŸ“… {phase['title'].upper()}")
            print(f"   DurÃ©e: {phase['duration']} semaines")
            print(f"   Ã‰conomie attendue: {self._format_size(phase['expected_savings'])}")
            print(f"   TÃ¢ches:")
            for task in phase['tasks']:
                print(f"     â€¢ {task}")
        
        print(f"\nğŸ“Š RÃ‰SUMÃ‰ PLAN:")
        print(f"   DurÃ©e totale estimÃ©e: {total_duration_weeks:.1f} semaines")
        print(f"   Ã‰conomie totale attendue: {self._format_size(total_savings)}")
        print(f"   ROI: Excellent (Ã©conomie permanente)")
        
        return implementation_plan

    def _format_size(self, size_bytes):
        """Formater taille en unitÃ©s lisibles"""
        if size_bytes == 0:
            return "0 B"
        
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"

    def run_complete_duplicate_analysis(self):
        """ExÃ©cuter analyse complÃ¨te des doublons"""
        print(f"""
ğŸš€ DÃ‰MARRAGE ANALYSE APPROFONDIE DOUBLONS
============================================================
ğŸ¯ Mission: Comprendre gestion doublons et optimiser dÃ©duplication
ğŸ” Analyses: Contenu identique + Patterns + Simulation + Plan
""")
        
        try:
            # Phase 1: Analyser groupes doublons en dÃ©tail
            print("ğŸ” Phase 1: Analyse dÃ©taillÃ©e groupes...")
            duplicate_analysis = self.analyze_duplicate_groups_detailed()
            
            # Phase 2: Simuler stratÃ©gies dÃ©duplication
            print("\nğŸ”„ Phase 2: Simulation dÃ©duplication...")
            strategies = self.simulate_deduplication(duplicate_analysis)
            
            # Phase 3: Plan d'implÃ©mentation
            print("\nğŸ“‹ Phase 3: Plan implÃ©mentation...")
            implementation_plan = self.generate_deduplication_implementation_plan(strategies)
            
            # RÃ©sumÃ© final
            total_groups = len(duplicate_analysis)
            total_files_duplicated = sum(group['file_count'] for group in duplicate_analysis.values())
            total_savings_potential = sum(group['wasted_original_space'] + group['wasted_compressed_space'] for group in duplicate_analysis.values())
            
            print(f"""
ğŸ‰ ANALYSE DOUBLONS APPROFONDIE TERMINÃ‰E !
============================================================
ğŸ” Groupes doublons analysÃ©s: {total_groups}
ğŸ“ Fichiers en doublon: {total_files_duplicated}
ğŸ’° Ã‰conomie potentielle: {self._format_size(total_savings_potential)}
ğŸ“‹ Plan implÃ©mentation: 3 phases dÃ©finies

ğŸ¯ DÃ‰COUVERTES CLÃ‰S:
   ğŸ“„ Doublons parfaits: Compression identique confirmÃ©e
   ğŸ·ï¸  Patterns: Copies systÃ¨me dÃ©tectÃ©es (ex: "Samuel CD (1).pdf")
   ğŸ”„ StratÃ©gie optimale: {"Hard links" if total_savings_potential > 1024*1024 else "RÃ©fÃ©rences simples"}
   â±ï¸  ImplÃ©mentation estimÃ©e: 4-6 semaines

ğŸš€ PLAN DÃ‰DUPLICATION PRÃŠT POUR EXÃ‰CUTION !
""")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ Erreur dans analyse doublons: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """Point d'entrÃ©e principal"""
    print(f"""
ğŸ” PANINI-FS DUPLICATE CONTENT DEEP ANALYZER
============================================================
ğŸ¯ Mission: Analyse approfondie doublons pour optimisation
ğŸ”¬ Focus: Contenu identique, patterns, dÃ©duplication

ğŸš€ Initialisation analyseur doublons...
""")
    
    try:
        analyzer = PaniniDuplicateAnalyzer()
        success = analyzer.run_complete_duplicate_analysis()
        
        if success:
            print(f"""
âœ… ANALYSE DOUBLONS APPROFONDIE ACCOMPLIE
=======================================
ğŸ” Gestion doublons complÃ¨tement analysÃ©e
ğŸ’° Potentiel dÃ©duplication quantifiÃ©
ğŸ“‹ Plan implÃ©mentation dÃ©taillÃ©
ğŸš€ Optimisations prÃªtes pour dÃ©veloppement

ğŸ¯ RÃ‰PONSE Ã€ LA QUESTION INITIALE:
Les doublons sont actuellement stockÃ©s sÃ©parÃ©ment
(pas de dÃ©duplication automatique), mais l'analyse
rÃ©vÃ¨le un potentiel d'optimisation significatif !
""")
            return True
        else:
            print("âŒ Ã‰chec analyse doublons")
            return False
    
    except Exception as e:
        print(f"âŒ Erreur: {str(e)}")
        return False

if __name__ == "__main__":
    success = main()
    import sys
    sys.exit(0 if success else 1)