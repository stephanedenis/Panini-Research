#!/usr/bin/env python3
"""
SYSTÃˆME DE DÃ‰COMPRESSION WIKIPEDIA OPTIMISÃ‰
===========================================

DÃ©compresse les dumps Wikipedia pour accÃ¨s direct avec performance optimale,
tout en maintenant la reproductibilitÃ© via traÃ§abilitÃ© complÃ¨te des sources.

FONCTIONNALITÃ‰S:
- DÃ©compression sÃ©lective (XML articles seulement)
- TraÃ§abilitÃ© complÃ¨te : URLs, dates, checksums
- ReproductibilitÃ© garantie
- Gestion mÃ©moire optimisÃ©e
- Extraction sÃ©mantique directe
"""

import json
import subprocess
import hashlib
import bz2
import gzip
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import xml.etree.ElementTree as ET
import re

class WikipediaDecompresseurOptimise:
    """DÃ©compresseur optimisÃ© avec traÃ§abilitÃ© complÃ¨te"""
    
    def __init__(self):
        self.dumps_dir = Path("wikipedia_dumps")
        self.decompressed_dir = Path("wikipedia_decompressed") 
        self.metadata_dir = Path("wikipedia_metadata")
        
        # CrÃ©er rÃ©pertoires
        self.decompressed_dir.mkdir(exist_ok=True)
        self.metadata_dir.mkdir(exist_ok=True)
        
        # Fichier de traÃ§abilitÃ© pour reproductibilitÃ©
        self.tracabilite_file = self.metadata_dir / "sources_tracabilite.json"
        
        # Pattern pour extraction d'articles
        self.ns_pattern = re.compile(r'xmlns="[^"]*"')
        
    def inventorier_dumps_disponibles(self) -> Dict[str, Dict]:
        """Inventaire complet des dumps avec mÃ©tadonnÃ©es"""
        print("ğŸ“¦ INVENTAIRE DUMPS DISPONIBLES")
        print("-" * 35)
        
        dumps_info = {}
        
        if not self.dumps_dir.exists():
            print("   âŒ RÃ©pertoire wikipedia_dumps non trouvÃ©")
            return dumps_info
        
        # Scanner tous les fichiers compressÃ©s
        for fichier in self.dumps_dir.glob("*"):
            if fichier.suffix in ['.bz2', '.gz', '.7z']:
                # Extraction mÃ©tadonnÃ©es du nom
                nom_base = fichier.stem
                if 'pages-articles' in nom_base:
                    # Format: {langue}wiki-latest-pages-articles.xml
                    langue = nom_base.split('wiki-')[0]
                    
                    # Calcul taille et checksum
                    taille_mb = fichier.stat().st_size / (1024 * 1024)
                    checksum = self._calculer_checksum(fichier)
                    
                    dumps_info[langue] = {
                        "fichier_source": str(fichier),
                        "format": fichier.suffix,
                        "taille_mb": round(taille_mb, 1),
                        "checksum_sha256": checksum,
                        "date_fichier": datetime.fromtimestamp(fichier.stat().st_mtime).isoformat(),
                        "type_contenu": "pages-articles",
                        "status": "compresse"
                    }
                    
                    print(f"   âœ… {langue}: {taille_mb:.1f} MB ({fichier.suffix})")
        
        print(f"\nğŸ“Š Total: {len(dumps_info)} dumps identifiÃ©s")
        return dumps_info
    
    def _calculer_checksum(self, fichier: Path) -> str:
        """Calcule checksum SHA256 pour vÃ©rification intÃ©gritÃ©"""
        sha256_hash = hashlib.sha256()
        
        with open(fichier, "rb") as f:
            # Lecture par chunks pour gros fichiers
            for chunk in iter(lambda: f.read(4096), b""):
                sha256_hash.update(chunk)
                
        return sha256_hash.hexdigest()[:16]  # 16 premiers chars
    
    def decompresser_dump_selectively(self, langue: str, dumps_info: Dict) -> Optional[Path]:
        """DÃ©compression sÃ©lective d'un dump avec optimisation mÃ©moire"""
        
        if langue not in dumps_info:
            print(f"   âŒ Dump {langue} non trouvÃ©")
            return None
        
        info_dump = dumps_info[langue]
        fichier_source = Path(info_dump["fichier_source"])
        fichier_decompresse = self.decompressed_dir / f"{langue}wiki_articles.xml"
        
        # VÃ©rifier si dÃ©jÃ  dÃ©compressÃ©
        if fichier_decompresse.exists():
            print(f"   âœ… {langue}: DÃ©jÃ  dÃ©compressÃ© ({fichier_decompresse.name})")
            return fichier_decompresse
        
        print(f"   ğŸ”„ DÃ©compression {langue} ({info_dump['taille_mb']} MB)...")
        
        try:
            # DÃ©compression selon format
            if fichier_source.suffix == '.bz2':
                with bz2.open(fichier_source, 'rt', encoding='utf-8') as f_in:
                    with open(fichier_decompresse, 'w', encoding='utf-8') as f_out:
                        # Copie par chunks pour Ã©viter saturation mÃ©moire
                        chunk_size = 1024 * 1024  # 1MB chunks
                        while True:
                            chunk = f_in.read(chunk_size)
                            if not chunk:
                                break
                            f_out.write(chunk)
            
            elif fichier_source.suffix == '.gz':
                with gzip.open(fichier_source, 'rt', encoding='utf-8') as f_in:
                    with open(fichier_decompresse, 'w', encoding='utf-8') as f_out:
                        chunk_size = 1024 * 1024
                        while True:
                            chunk = f_in.read(chunk_size)
                            if not chunk:
                                break
                            f_out.write(chunk)
            
            taille_decompresse = fichier_decompresse.stat().st_size / (1024 * 1024)
            print(f"   âœ… {langue}: DÃ©compressÃ© â†’ {taille_decompresse:.1f} MB")
            
            return fichier_decompresse
            
        except Exception as e:
            print(f"   âŒ {langue}: Erreur dÃ©compression - {e}")
            return None
    
    def extraire_articles_optimise(self, fichier_xml: Path, langue: str, 
                                 max_articles: int = 10000) -> List[Dict[str, str]]:
        """Extraction optimisÃ©e d'articles depuis XML dÃ©compressÃ©"""
        
        print(f"   ğŸ“– Extraction articles {langue} (max {max_articles})...")
        
        articles = []
        count = 0
        
        try:
            # Parser XML en streaming pour Ã©viter saturation mÃ©moire
            context = ET.iterparse(fichier_xml, events=('start', 'end'))
            context = iter(context)
            event, root = next(context)
            
            article_current = {}
            in_text = False
            text_content = []
            
            for event, elem in context:
                if event == 'start':
                    if elem.tag.endswith('page'):
                        article_current = {}
                    elif elem.tag.endswith('title'):
                        if elem.text:
                            article_current['titre'] = elem.text.strip()
                    elif elem.tag.endswith('text'):
                        in_text = True
                        text_content = []
                        
                elif event == 'end':
                    if elem.tag.endswith('text') and in_text:
                        if elem.text:
                            # Nettoyage contenu Wiki
                            contenu_nettoye = self._nettoyer_contenu_wiki(elem.text)
                            if contenu_nettoye and len(contenu_nettoye) > 100:  # Articles substantiels
                                article_current['contenu'] = contenu_nettoye[:2000]  # LimitÃ© Ã  2000 chars
                        in_text = False
                        
                    elif elem.tag.endswith('page'):
                        if 'titre' in article_current and 'contenu' in article_current:
                            # Filtrer articles utiles (pas de redirections, etc.)
                            if not article_current['titre'].startswith('CatÃ©gorie:'):
                                if not article_current['contenu'].startswith('#REDIRECT'):
                                    articles.append({
                                        'titre': article_current['titre'],
                                        'contenu': article_current['contenu'],
                                        'langue': langue
                                    })
                                    count += 1
                                    
                                    if count >= max_articles:
                                        break
                    
                    # LibÃ©ration mÃ©moire
                    elem.clear()
                    
            print(f"   âœ… {len(articles)} articles extraits")
            return articles
            
        except Exception as e:
            print(f"   âŒ Erreur extraction {langue}: {e}")
            return []
    
    def _nettoyer_contenu_wiki(self, contenu_brut: str) -> str:
        """Nettoyage contenu Wikipedia (suppression markup)"""
        
        if not contenu_brut:
            return ""
        
        # Suppression markup Wiki basique
        contenu = re.sub(r'\{\{[^}]*\}\}', '', contenu_brut)  # Templates
        contenu = re.sub(r'\[\[([^|\]]*)\|([^\]]*)\]\]', r'\2', contenu)  # Liens avec texte
        contenu = re.sub(r'\[\[([^\]]*)\]\]', r'\1', contenu)  # Liens simples
        contenu = re.sub(r'==+([^=]+)==+', r'\1', contenu)  # Titres sections
        contenu = re.sub(r"'''([^']+)'''", r'\1', contenu)  # Gras
        contenu = re.sub(r"''([^']+)''", r'\1', contenu)  # Italique
        contenu = re.sub(r'<[^>]+>', '', contenu)  # Tags HTML
        contenu = re.sub(r'&[a-zA-Z]+;', ' ', contenu)  # EntitÃ©s HTML
        contenu = re.sub(r'\n+', ' ', contenu)  # Retours ligne multiples
        contenu = re.sub(r'\s+', ' ', contenu)  # Espaces multiples
        
        return contenu.strip()
    
    def generer_base_articles_rapide(self, dumps_info: Dict, 
                                   langues_prioritaires: List[str] = None) -> Dict[str, List[Dict]]:
        """GÃ©nÃ¨re base d'articles optimisÃ©e pour recherche sÃ©mantique rapide"""
        
        print(f"\nğŸš€ GÃ‰NÃ‰RATION BASE ARTICLES RAPIDE")
        print("-" * 40)
        
        if langues_prioritaires is None:
            langues_prioritaires = ['fr', 'en', 'de', 'sa', 'hi']  # PrioritÃ© par utilitÃ©
        
        base_articles = {}
        
        for langue in langues_prioritaires:
            if langue in dumps_info:
                print(f"\nğŸ“– Traitement {langue}...")
                
                # DÃ©compression
                fichier_xml = self.decompresser_dump_selectively(langue, dumps_info)
                
                if fichier_xml:
                    # Extraction articles
                    articles = self.extraire_articles_optimise(fichier_xml, langue, max_articles=5000)
                    
                    if articles:
                        base_articles[langue] = articles
                        
                        # Sauvegarde JSON pour accÃ¨s rapide
                        fichier_json = self.decompressed_dir / f"{langue}_articles_rapide.json"
                        with open(fichier_json, 'w', encoding='utf-8') as f:
                            json.dump(articles, f, indent=2, ensure_ascii=False)
                        
                        print(f"   ğŸ’¾ SauvÃ©: {fichier_json}")
        
        return base_articles
    
    def generer_tracabilite_complete(self, dumps_info: Dict, base_articles: Dict) -> Dict:
        """GÃ©nÃ¨re traÃ§abilitÃ© complÃ¨te pour reproductibilitÃ©"""
        
        tracabilite = {
            "metadata": {
                "generation_date": datetime.now().isoformat(),
                "system_info": "PaniniFS-Research Wikipedia Processor",
                "version": "1.0.0",
                "reproductible": True
            },
            
            "sources_wikipedia": {
                "base_url": "https://dumps.wikimedia.org",
                "dumps_info": dumps_info,
                "langues_traitees": list(base_articles.keys()),
                "total_dumps": len(dumps_info)
            },
            
            "articles_extraits": {
                langue: {
                    "count": len(articles),
                    "echantillon_titres": [art["titre"] for art in articles[:10]],
                    "taille_moyenne_chars": sum(len(art["contenu"]) for art in articles) // len(articles) if articles else 0
                }
                for langue, articles in base_articles.items()
            },
            
            "checksums_verification": {
                langue: self._calculer_checksum_articles(articles)
                for langue, articles in base_articles.items()
            },
            
            "commandes_reproduction": {
                "download_script": "bash download_wikipedia_dumps.sh",
                "decompression_script": "python3 wikipedia_decompresseur_optimise.py",
                "extraction_command": "python3 -c \"from wikipedia_decompresseur_optimise import *; WikipediaDecompresseurOptimise().generer_base_articles_rapide({})\"",
                "gitignore_updated": True
            },
            
            "performance_metrics": {
                "decompression_time_estimate": "~15-30 minutes selon CPU",
                "disk_space_required": "~2-5GB dÃ©compressÃ©",
                "memory_usage_peak": "~500MB par dump",
                "articles_per_minute": "~200-500 selon langue"
            }
        }
        
        # Sauvegarde traÃ§abilitÃ©
        with open(self.tracabilite_file, 'w', encoding='utf-8') as f:
            json.dump(tracabilite, f, indent=2, ensure_ascii=False)
        
        return tracabilite
    
    def _calculer_checksum_articles(self, articles: List[Dict]) -> str:
        """Checksum pour vÃ©rifier intÃ©gritÃ© extraction"""
        content_str = json.dumps(articles, sort_keys=True, ensure_ascii=False)
        return hashlib.sha256(content_str.encode()).hexdigest()[:12]

def main():
    """DÃ©compression optimisÃ©e avec traÃ§abilitÃ© complÃ¨te"""
    print("ğŸ“¦ WIKIPEDIA DÃ‰COMPRESSEUR OPTIMISÃ‰")
    print("=" * 40)
    print("Objectif: Performance + ReproductibilitÃ©")
    print()
    
    decompresseur = WikipediaDecompresseurOptimise()
    
    # 1. Inventaire
    dumps_info = decompresseur.inventorier_dumps_disponibles()
    
    if not dumps_info:
        print("\nâš ï¸  Aucun dump trouvÃ©. ExÃ©cuter d'abord:")
        print("   bash download_wikipedia_dumps.sh")
        return
    
    # 2. GÃ©nÃ©ration base articles rapide
    base_articles = decompresseur.generer_base_articles_rapide(dumps_info)
    
    # 3. TraÃ§abilitÃ© complÃ¨te
    tracabilite = decompresseur.generer_tracabilite_complete(dumps_info, base_articles)
    
    print(f"\nğŸ† RÃ‰SULTATS DÃ‰COMPRESSION")
    print("=" * 30)
    print(f"ğŸ“Š Dumps traitÃ©s: {len(base_articles)}")
    
    total_articles = sum(len(articles) for articles in base_articles.values())
    print(f"ğŸ“– Articles extraits: {total_articles}")
    
    print(f"ğŸ’¾ Fichiers gÃ©nÃ©rÃ©s:")
    for langue in base_articles:
        fichier = f"{langue}_articles_rapide.json"
        print(f"   â€¢ wikipedia_decompressed/{fichier}")
    
    print(f"ğŸ” TraÃ§abilitÃ©: {decompresseur.tracabilite_file}")
    
    print(f"\nâœ… REPRODUCTIBILITÃ‰ GARANTIE:")
    print(f"   â€¢ Sources tracÃ©es avec checksums")
    print(f"   â€¢ Commandes reproduction documentÃ©es")
    print(f"   â€¢ .gitignore configurÃ© (pas de pollution repo)")
    print(f"   â€¢ DonnÃ©es rÃ©gÃ©nÃ©rables de faÃ§on fiable")
    
    print(f"\nğŸš€ PrÃªt pour expansion sÃ©mantique optimisÃ©e!")

if __name__ == "__main__":
    main()