#!/usr/bin/env python3
"""
ANALYSEUR DE DUMPS WIKIPEDIA POUR PRIMITIVES UNIVERSELLES
=========================================================

Analyse les dumps Wikipedia multilingues pour extraire les syst√®mes de 
classification et construire une base de primitives universelles.

Int√©gration avec encyclopedie_compositionnelle_universelle.py
"""

import requests
import json
import xml.etree.ElementTree as ET
from pathlib import Path
import gzip
import bz2
from typing import Dict, List, Set, Tuple
import re
from dataclasses import dataclass
from datetime import datetime
import urllib.parse

@dataclass
class WikipediaSource:
    """Source Wikipedia avec m√©tadonn√©es"""
    language: str
    language_name: str
    dump_url: str
    categories_file: str
    pages_file: str
    size_mb: float
    last_updated: str
    priority: int  # 1=√©lev√©e, 3=faible

class WikipediaDumpAnalyzer:
    """Analyseur des dumps Wikipedia pour extraction de primitives"""
    
    def __init__(self):
        self.base_url = "https://dumps.wikimedia.org"
        self.sources = self._initialize_sources()
        self.classification_patterns = self._initialize_patterns()
        self.output_dir = Path("wikipedia_classifications")
        self.output_dir.mkdir(exist_ok=True)
        
    def _initialize_sources(self) -> List[WikipediaSource]:
        """Initialise les sources Wikipedia pour PanLang √©rudite universelle"""
        return [
            # LANGUES FONDATRICES - Racines anciennes
            WikipediaSource(
                language="sa", language_name="Sanskrit", dump_url="sawiki",
                categories_file="sawiki-latest-category.sql.gz",
                pages_file="sawiki-latest-pages-articles.xml.bz2", 
                size_mb=45, last_updated="2024", priority=1  # DhƒÅtu originels
            ),
            WikipediaSource(
                language="la", language_name="Latin", dump_url="lawiki",
                categories_file="lawiki-latest-category.sql.gz",
                pages_file="lawiki-latest-pages-articles.xml.bz2",
                size_mb=180, last_updated="2024", priority=1  # Racines europ√©ennes
            ),
            WikipediaSource(
                language="el", language_name="Grec", dump_url="elwiki",
                categories_file="elwiki-latest-category.sql.gz", 
                pages_file="elwiki-latest-pages-articles.xml.bz2",
                size_mb=420, last_updated="2024", priority=1  # Science antique
            ),
            WikipediaSource(
                language="ar", language_name="Arabe", dump_url="arwiki",
                categories_file="arwiki-latest-category.sql.gz",
                pages_file="arwiki-latest-pages-articles.xml.bz2",
                size_mb=1200, last_updated="2024", priority=1  # Sciences m√©di√©vales
            ),
            
            # LANGUES MODERNES EXHAUSTIVES
            WikipediaSource(
                language="en", language_name="English", dump_url="enwiki",
                categories_file="enwiki-latest-category.sql.gz", 
                pages_file="enwiki-latest-pages-articles.xml.bz2",
                size_mb=19000, last_updated="2024", priority=1  # Plus exhaustif
            ),
            WikipediaSource(
                language="fr", language_name="Fran√ßais", dump_url="frwiki",
                categories_file="frwiki-latest-category.sql.gz",
                pages_file="frwiki-latest-pages-articles.xml.bz2",
                size_mb=2800, last_updated="2024", priority=1  # Classifications fran√ßaise
            ),
            WikipediaSource(
                language="de", language_name="Deutsch", dump_url="dewiki", 
                categories_file="dewiki-latest-category.sql.gz",
                pages_file="dewiki-latest-pages-articles.xml.bz2",
                size_mb=5200, last_updated="2024", priority=1  # Pr√©cision germanique
            ),
            WikipediaSource(
                language="ru", language_name="–†—É—Å—Å–∫–∏–π", dump_url="ruwiki",
                categories_file="ruwiki-latest-category.sql.gz",
                pages_file="ruwiki-latest-pages-articles.xml.bz2", 
                size_mb=3100, last_updated="2024", priority=1  # Sciences slaves
            ),
            WikipediaSource(
                language="es", language_name="Espa√±ol", dump_url="eswiki",
                categories_file="eswiki-latest-category.sql.gz",
                pages_file="eswiki-latest-pages-articles.xml.bz2",
                size_mb=4200, last_updated="2024", priority=1  # Monde ib√©rique
            ),
            WikipediaSource(
                language="it", language_name="Italiano", dump_url="itwiki",
                categories_file="itwiki-latest-category.sql.gz", 
                pages_file="itwiki-latest-pages-articles.xml.bz2",
                size_mb=1800, last_updated="2024", priority=1  # Renaissance italienne
            ),
            
            # LANGUES ASIATIQUES √âRUDITES
            WikipediaSource(
                language="zh", language_name="‰∏≠Êñá", dump_url="zhwiki",
                categories_file="zhwiki-latest-category.sql.gz",
                pages_file="zhwiki-latest-pages-articles.xml.bz2",
                size_mb=2400, last_updated="2024", priority=1  # Sagesse chinoise
            ),
            WikipediaSource(
                language="ja", language_name="Êó•Êú¨Ë™û", dump_url="jawiki",
                categories_file="jawiki-latest-category.sql.gz",
                pages_file="jawiki-latest-pages-articles.xml.bz2", 
                size_mb=3200, last_updated="2024", priority=1  # Pr√©cision japonaise
            ),
            WikipediaSource(
                language="hi", language_name="‡§π‡§ø‡§®‡•ç‡§¶‡•Ä", dump_url="hiwiki",
                categories_file="hiwiki-latest-category.sql.gz",
                pages_file="hiwiki-latest-pages-articles.xml.bz2",
                size_mb=520, last_updated="2024", priority=1  # Continuit√© sanskrite
            )
        ]
        
    def _initialize_patterns(self) -> Dict[str, List[str]]:
        """Patterns de classification √† rechercher dans Wikipedia"""
        return {
            # Biologie et taxonomie
            "taxonomie": [
                r"\{\{Taxobox",
                r"\{\{Infobox.*biolog",
                r"Cat√©gorie:.*taxonomie",
                r"Category:.*taxonomy",
                r"regne\s*=",
                r"kingdom\s*=", 
                r"classe\s*=",
                r"class\s*=",
                r"ordre\s*=", 
                r"order\s*="
            ],
            
            # Chimie 
            "chimie": [
                r"\{\{Infobox.*chimi",
                r"\{\{Chembox",
                r"Cat√©gorie:.*chimique", 
                r"Category:.*chemical",
                r"formule.*=",
                r"formula.*=",
                r"masse.*molaire", 
                r"molecular.*weight"
            ],
            
            # G√©ographie
            "geographie": [
                r"\{\{Infobox.*pays",
                r"\{\{Infobox.*ville", 
                r"\{\{Infobox.*country",
                r"\{\{Infobox.*city",
                r"Cat√©gorie:.*g√©ograph",
                r"Category:.*geography",
                r"continent\s*=",
                r"coordonn√©es\s*=",
                r"coordinates\s*="
            ],
            
            # Langues et linguistique
            "linguistique": [
                r"\{\{Infobox.*langue",
                r"\{\{Infobox.*language", 
                r"Cat√©gorie:.*langue",
                r"Category:.*language",
                r"famille\s*=",
                r"family\s*=",
                r"ISO.*639",
                r"racine\s*=",
                r"root\s*="
            ],
            
            # Math√©matiques
            "mathematiques": [
                r"\{\{Infobox.*math",
                r"Cat√©gorie:.*math√©m",
                r"Category:.*mathemat", 
                r"th√©or√®me",
                r"theorem",
                r"√©quation",
                r"equation"
            ],
            
            # Couleurs (extension du syst√®me HSL)
            "couleurs": [
                r"Cat√©gorie:.*couleur",
                r"Category:.*color",
                r"RGB\s*=",
                r"HSL\s*=", 
                r"teinte\s*=",
                r"hue\s*=",
                r"saturation\s*=",
                r"luminosit√©\s*=",
                r"brightness\s*="
            ]
        }
        
    def analyze_available_dumps(self) -> Dict[str, any]:
        """Analyse les dumps Wikipedia disponibles"""
        print("üîç ANALYSE DES DUMPS WIKIPEDIA DISPONIBLES")
        print("=" * 60)
        
        analysis = {
            "sources_prioritaires": [],
            "taille_totale_gb": 0,
            "langues_disponibles": [],
            "recommandations": []
        }
        
        for source in self.sources:
            print(f"\nüìä {source.language_name} ({source.language})")
            print(f"   üìÇ URL: {self.base_url}/{source.dump_url}/")
            print(f"   üìè Taille: {source.size_mb} MB")
            print(f"   ‚≠ê Priorit√©: {source.priority}/3")
            print(f"   üìã Cat√©gories: {source.categories_file}")
            print(f"   üìÑ Articles: {source.pages_file}")
            
            if source.priority <= 2:
                analysis["sources_prioritaires"].append({
                    "langue": source.language,
                    "nom": source.language_name,
                    "taille_mb": source.size_mb,
                    "priorite": source.priority
                })
                
            analysis["taille_totale_gb"] += source.size_mb / 1000
            analysis["langues_disponibles"].append(source.language)
            
        # Recommandations
        analysis["recommandations"] = [
            "Commencer par Sanskrit (45 MB) - essentiel pour dhƒÅtu",
            "Fran√ßais (2.8 GB) - classifications compl√®tes", 
            "Anglais (19 GB) - plus exhaustif mais volumineux",
            "T√©l√©charger les cat√©gories d'abord (.sql.gz)",
            "Parser les infoboxes pour primitives structur√©es"
        ]
        
        print(f"\n‚úÖ R√âSUM√â:")
        print(f"   üìä {len(analysis['sources_prioritaires'])} sources prioritaires")
        print(f"   üíæ {analysis['taille_totale_gb']:.1f} GB total")
        print(f"   üåê {len(analysis['langues_disponibles'])} langues")
        
        return analysis
        
    def get_dump_urls(self, language: str) -> Dict[str, str]:
        """Obtient les URLs de t√©l√©chargement pour une langue"""
        source = next((s for s in self.sources if s.language == language), None)
        if not source:
            return {}
            
        base = f"{self.base_url}/{source.dump_url}/latest/"
        return {
            "categories": base + source.categories_file,
            "pages": base + source.pages_file,
            "base_url": base
        }
        
    def estimate_classification_density(self, language: str) -> Dict[str, int]:
        """Estime la densit√© de classifications par domaine"""
        estimates = {
            "fr": {
                "taxonomie": 45000,
                "chimie": 8000, 
                "geographie": 35000,
                "linguistique": 12000,
                "mathematiques": 15000,
                "couleurs": 500
            },
            "en": {
                "taxonomie": 180000,
                "chimie": 25000,
                "geographie": 120000, 
                "linguistique": 35000,
                "mathematiques": 45000,
                "couleurs": 1500
            },
            "sa": {
                "taxonomie": 200,
                "chimie": 50,
                "geographie": 800,
                "linguistique": 2000,  # Racines dhƒÅtu
                "mathematiques": 300,
                "couleurs": 100
            },
            "de": {
                "taxonomie": 65000,
                "chimie": 12000,
                "geographie": 45000,
                "linguistique": 18000, 
                "mathematiques": 22000,
                "couleurs": 800
            }
        }
        
        return estimates.get(language, {})
        
    def create_download_script(self, languages: List[str] = None) -> str:
        """G√©n√®re un script de t√©l√©chargement"""
        if not languages:
            languages = ["sa", "fr"]  # Commencer petit
            
        script_lines = [
            "#!/bin/bash",
            "# Script de t√©l√©chargement Wikipedia dumps", 
            "# G√©n√©r√© automatiquement",
            "",
            "mkdir -p wikipedia_dumps",
            "cd wikipedia_dumps", 
            ""
        ]
        
        total_size = 0
        for lang in languages:
            urls = self.get_dump_urls(lang)
            if urls:
                source = next(s for s in self.sources if s.language == lang)
                script_lines.extend([
                    f"# {source.language_name} ({source.size_mb} MB)",
                    f"echo 'T√©l√©chargement {source.language_name}...'",
                    f"wget -c '{urls['categories']}'",
                    f"wget -c '{urls['pages']}'", 
                    ""
                ])
                total_size += source.size_mb
                
        script_lines.extend([
            f"echo 'T√©l√©chargement termin√© - {total_size} MB'",
            "ls -lah"
        ])
        
        script_content = "\n".join(script_lines)
        
        script_path = "download_wikipedia_dumps.sh"
        with open(script_path, "w") as f:
            f.write(script_content)
            
        import os
        os.chmod(script_path, 0o755)
        
        return script_path
        
    def demonstrate_classification_extraction(self):
        """D√©montre l'extraction de classifications (simulation)"""
        print("\nüî¨ D√âMONSTRATION EXTRACTION DE CLASSIFICATIONS")
        print("=" * 60)
        
        # Simulation avec des exemples r√©els de Wikipedia
        examples = {
            "taxonomie": {
                "Chat domestique": {
                    "regne": "Animalia",
                    "embranchement": "Chordata", 
                    "classe": "Mammalia",
                    "ordre": "Carnivora",
                    "famille": "Felidae",
                    "genre": "Felis",
                    "espece": "Felis catus"
                }
            },
            "chimie": {
                "Eau": {
                    "formule": "H2O",
                    "masse_molaire": "18.015 g/mol",
                    "point_fusion": "0¬∞C", 
                    "point_ebullition": "100¬∞C",
                    "densite": "1 g/cm¬≥"
                }
            },
            "geographie": {
                "Paris": {
                    "pays": "France",
                    "region": "√éle-de-France", 
                    "continent": "Europe",
                    "coordonnees": "48.8566¬∞N, 2.3522¬∞E",
                    "population": "2161000"
                }
            }
        }
        
        for domaine, concepts in examples.items():
            print(f"\nüìã {domaine.upper()}")
            print("-" * 40)
            for concept, proprietes in concepts.items():
                print(f"üîç {concept}:")
                for prop, valeur in proprietes.items():
                    print(f"   {prop}: {valeur}")
                    
        print("\n‚úÖ Ces structures serviront de primitives universelles")
        print("   pour enrichir l'encyclop√©die compositionnelle")

def main():
    """Fonction principale"""
    print("üåü ANALYSEUR DUMPS WIKIPEDIA - PRIMITIVES UNIVERSELLES")
    print("=" * 70)
    
    analyzer = WikipediaDumpAnalyzer()
    
    # Analyse des dumps disponibles
    analysis = analyzer.analyze_available_dumps()
    
    # Sauvegarde de l'analyse
    with open("wikipedia_dumps_analysis.json", "w", encoding="utf-8") as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)
        
    print(f"\nüìÑ Analyse sauvegard√©e: wikipedia_dumps_analysis.json")
    
    # G√©n√©ration du script de t√©l√©chargement
    script_path = analyzer.create_download_script(["sa", "fr"])
    print(f"üìú Script g√©n√©r√©: {script_path}")
    
    # D√©monstration
    analyzer.demonstrate_classification_extraction()
    
    print("\nüéØ PROCHAINES √âTAPES:")
    print("1. Ex√©cuter ./download_wikipedia_dumps.sh")
    print("2. Parser les cat√©gories et infoboxes") 
    print("3. Extraire les primitives taxonomiques")
    print("4. Int√©grer √† encyclopedie_compositionnelle_universelle.py")
    print("5. Tester reconstruction cross-linguistique")

if __name__ == "__main__":
    main()