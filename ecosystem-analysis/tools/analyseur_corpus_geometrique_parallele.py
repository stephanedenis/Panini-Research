#!/usr/bin/env python3
"""
ANALYSEUR CORPUS GÃ‰OMÃ‰TRIQUE PARALLÃˆLE v4.0
==========================================

SystÃ¨me d'analyse concurrent basÃ© sur gÃ©omÃ©trie dhÄtu avancÃ©e
avec dimensions supÃ©rieures (nombres premiers) et reproductibilitÃ© complÃ¨te.

INNOVATIONS:
- Espaces 5D/7D/11D pour dhÄtu antagonistes  
- Analyse concurrente avec monitoring ressources
- Logging expÃ©rimental dÃ©taillÃ© pour reproductibilitÃ©
- IntÃ©gration dictionnaire raffinÃ© contextuel
- MÃ©triques critiques pour amÃ©lioration continue
"""

import json
import numpy as np
import multiprocessing as mp
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import time
import psutil
import os
import logging
from pathlib import Path
import hashlib
import pickle

# Configuration logging expÃ©rimental
def setup_experimental_logging(experiment_id: str) -> logging.Logger:
    """Configuration logging dÃ©taillÃ© pour reproductibilitÃ©"""
    logger = logging.getLogger(f'corpus_geometric_{experiment_id}')
    logger.setLevel(logging.DEBUG)
    
    # Handler fichier avec timestamp
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    log_file = f"corpus_geometric_experiment_{experiment_id}_{timestamp}.log"
    
    fh = logging.FileHandler(log_file, encoding='utf-8')
    fh.setLevel(logging.DEBUG)
    
    # Format dÃ©taillÃ© avec mÃ©tadonnÃ©es systÃ¨me
    formatter = logging.Formatter(
        '%(asctime)s | PID:%(process)d | %(levelname)s | %(funcName)s:%(lineno)d | %(message)s'
    )
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    
    return logger, log_file

@dataclass 
class ExperimentalConfig:
    """Configuration expÃ©rimentale complÃ¨te pour reproductibilitÃ©"""
    experiment_id: str
    prime_dimensions: List[int]  # [5, 7, 11] pour antagonistes
    dhatu_count: int
    corpus_chunk_size: int
    concurrent_workers: int
    geometric_thresholds: Dict[str, float]
    random_seed: int
    system_specs: Dict[str, Any]
    input_files: List[str]
    algorithm_version: str
    
    def save_config(self, path: str):
        """Sauvegarde configuration pour reproductibilitÃ©"""
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(asdict(self), f, ensure_ascii=False, indent=2)
    
    def hash_config(self) -> str:
        """Hash configuration pour vÃ©rification reproductibilitÃ©"""
        config_str = json.dumps(asdict(self), sort_keys=True)
        return hashlib.sha256(config_str.encode()).hexdigest()[:16]

@dataclass
class GeometricDhatuVector:
    """Vecteur dhÄtu multi-dimensionnel avec bases nombres premiers"""
    name: str
    base_2d: np.ndarray    # ReprÃ©sentation binaire simple
    base_5d: np.ndarray    # Dimension quintaire pour antagonistes  
    base_7d: np.ndarray    # Dimension septenary pour nuances
    base_11d: np.ndarray   # Dimension undenary pour hypercomplexitÃ©
    activation_strength: float
    context_weights: Dict[str, float]
    
@dataclass
class ParallelAnalysisResult:
    """RÃ©sultat analyse parallÃ¨le avec mÃ©triques critiques"""
    experiment_id: str
    processing_time: float
    memory_usage: Dict[str, float]
    cpu_usage: Dict[str, float] 
    dhatu_activations: Dict[str, GeometricDhatuVector]
    geometric_relations: List[Dict[str, Any]]
    quality_metrics: Dict[str, float]
    critical_insights: List[str]
    improvement_suggestions: List[str]

class ResourceMonitor:
    """Monitoring ressources systÃ¨me pour analyses concurrentes"""
    
    def __init__(self, experiment_id: str):
        self.experiment_id = experiment_id
        self.start_time = time.time()
        self.measurements = []
    
    def measure(self, label: str = "checkpoint"):
        """Mesure instantanÃ©e ressources"""
        measurement = {
            'timestamp': time.time() - self.start_time,
            'label': label,
            'memory': {
                'rss': psutil.Process().memory_info().rss / (1024**2),  # MB
                'vms': psutil.Process().memory_info().vms / (1024**2),  # MB
                'available': psutil.virtual_memory().available / (1024**2),  # MB
                'percent': psutil.virtual_memory().percent
            },
            'cpu': {
                'percent': psutil.cpu_percent(interval=0.1),
                'cores_used': psutil.cpu_count(logical=False),
                'load_avg': os.getloadavg() if hasattr(os, 'getloadavg') else [0,0,0]
            }
        }
        self.measurements.append(measurement)
        return measurement

class GeometricDhatuAnalyzer:
    """Analyseur gÃ©omÃ©trique avancÃ© avec dimensions supÃ©rieures"""
    
    def __init__(self, config: ExperimentalConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.monitor = ResourceMonitor(config.experiment_id)
        
        # Initialisation espaces gÃ©omÃ©triques multi-dimensionnels
        self._init_prime_dimensional_spaces()
        
        # Chargement dictionnaire raffinÃ©
        self._load_refined_dictionary()
        
        # DhÄtu universels avec reprÃ©sentations multi-dimensionnelles
        self._init_geometric_dhatus()
        
        self.logger.info(f"Analyseur gÃ©omÃ©trique initialisÃ© - Experiment: {config.experiment_id}")
        self.logger.info(f"Config hash: {config.hash_config()}")
        
    def _init_prime_dimensional_spaces(self):
        """Initialise espaces gÃ©omÃ©triques basÃ©s nombres premiers"""
        self.prime_spaces = {}
        
        # Set random seed pour reproductibilitÃ©
        np.random.seed(self.config.random_seed)
        
        for prime in self.config.prime_dimensions:
            # Espaces orthogonaux pour chaque dimension premier
            # GÃ©nÃ©ration matrice orthogonale via QR decomposition
            random_matrix = np.random.randn(prime, prime)
            q, r = np.linalg.qr(random_matrix)
            
            self.prime_spaces[prime] = {
                'dimension': prime,
                'basis_vectors': q,  # Matrice orthogonale
                'antagonist_projections': self._compute_antagonist_projections(prime),
                'discrete_lattice': self._build_discrete_lattice(prime)
            }
            
            self.logger.debug(f"Espace {prime}D initialisÃ© avec base orthonormÃ©e")
        
        self.monitor.measure("prime_spaces_init")
    
    def _compute_antagonist_projections(self, dimension: int) -> np.ndarray:
        """Calcule projections pour dhÄtu antagonistes"""
        # Projections maximisant distance pour concepts opposÃ©s
        projections = np.zeros((9, dimension))  # 9 dhÄtu
        
        # Exemples: AMOUR vs DESTRUCTION, CREATION vs ITERATION
        antagonist_pairs = [
            (0, 4),  # RELATE vs COMM (spatial vs communicatif)
            (1, 7),  # MODAL vs DECIDE (possibilitÃ© vs action)  
            (2, 5),  # EXIST vs CAUSE (Ãªtre vs faire)
        ]
        
        for i, (dhatu_a, dhatu_b) in enumerate(antagonist_pairs):
            if i < dimension // 2:
                projections[dhatu_a, i] = 1.0
                projections[dhatu_b, i] = -1.0  # Opposition gÃ©omÃ©trique
        
        return projections
    
    def _build_discrete_lattice(self, dimension: int) -> Dict[str, np.ndarray]:
        """Construit lattice discret pour dhÄtu non-continus"""
        lattice_points = {}
        
        # Points lattice pour concepts discrets (nombres, categories, etc.)
        for i in range(min(dimension, 9)):  # Un point par dhÄtu 
            point = np.zeros(dimension)
            point[i] = 1.0
            lattice_points[f'dhatu_{i}'] = point
            
        return lattice_points
    
    def _load_refined_dictionary(self):
        """Charge dictionnaire raffinÃ© pour enrichissement contextuel"""
        try:
            refined_files = list(Path('.').glob('dictionnaire_raffine_*.json'))
            if refined_files:
                latest_refined = max(refined_files, key=os.path.getctime)
                
                with open(latest_refined, 'r', encoding='utf-8') as f:
                    self.refined_dict = json.load(f)
                
                self.logger.info(f"Dictionnaire raffinÃ© chargÃ©: {latest_refined}")
                self.logger.info(f"Concepts raffinÃ©s: {len(self.refined_dict.get('concepts_raffines', {}))}")
            else:
                self.refined_dict = {}
                self.logger.warning("Aucun dictionnaire raffinÃ© trouvÃ©")
                
        except Exception as e:
            self.logger.error(f"Erreur chargement dictionnaire: {e}")
            self.refined_dict = {}
    
    def _init_geometric_dhatus(self):
        """Initialise dhÄtu avec reprÃ©sentations gÃ©omÃ©triques multi-dimensionnelles"""
        dhatu_names = ['RELATE', 'MODAL', 'EXIST', 'EVAL', 'COMM', 'CAUSE', 'ITER', 'DECIDE', 'FEEL']
        
        self.geometric_dhatus = {}
        
        for i, name in enumerate(dhatu_names):
            # Vecteurs base pour chaque dimension
            base_2d = np.random.randn(2)
            base_5d = np.random.randn(5)  
            base_7d = np.random.randn(7)
            base_11d = np.random.randn(11)
            
            # Normalisation sur sphÃ¨re unitaire
            base_2d = base_2d / np.linalg.norm(base_2d)
            base_5d = base_5d / np.linalg.norm(base_5d)
            base_7d = base_7d / np.linalg.norm(base_7d) 
            base_11d = base_11d / np.linalg.norm(base_11d)
            
            # Enrichissement contextuel depuis dictionnaire raffinÃ©
            context_weights = self._extract_contextual_weights(name)
            
            dhatu = GeometricDhatuVector(
                name=name,
                base_2d=base_2d,
                base_5d=base_5d, 
                base_7d=base_7d,
                base_11d=base_11d,
                activation_strength=0.0,
                context_weights=context_weights
            )
            
            self.geometric_dhatus[name] = dhatu
            
        self.logger.info(f"DhÄtu gÃ©omÃ©triques initialisÃ©s: {len(self.geometric_dhatus)}")
        self.monitor.measure("dhatus_init")
    
    def _extract_contextual_weights(self, dhatu_name: str) -> Dict[str, float]:
        """Extrait poids contextuels depuis dictionnaire raffinÃ©"""
        if dhatu_name in self.refined_dict.get('concepts_raffines', {}):
            refined_concept = self.refined_dict['concepts_raffines'][dhatu_name]
            
            # Convertir dimensions contextuelles en poids
            context_dims = refined_concept.get('contextual_dimensions', {})
            weights = {}
            
            for context, data in context_dims.items():
                if isinstance(data, dict) and 'weight' in data:
                    weights[context] = data['weight']
                    
            return weights
        
        # Poids par dÃ©faut uniformes
        return {'default': 1.0}
    
    def analyze_corpus_chunk_parallel(self, text_chunk: str, chunk_id: int) -> Dict[str, Any]:
        """Analyse chunk corpus avec gÃ©omÃ©trie multi-dimensionnelle"""
        self.logger.debug(f"DÃ©but analyse chunk {chunk_id} - Taille: {len(text_chunk)} chars")
        
        start_time = time.time()
        chunk_result = {
            'chunk_id': chunk_id,
            'text_size': len(text_chunk),
            'dhatu_activations': {},
            'geometric_features': {},
            'quality_score': 0.0
        }
        
        # Analyse dans chaque espace dimensionnel
        for prime_dim in self.config.prime_dimensions:
            dim_analysis = self._analyze_in_prime_dimension(text_chunk, prime_dim)
            chunk_result['geometric_features'][f'dim_{prime_dim}'] = dim_analysis
        
        # Fusion multi-dimensionnelle
        fused_activations = self._fuse_dimensional_activations(chunk_result['geometric_features'])
        chunk_result['dhatu_activations'] = fused_activations
        
        # Score qualitÃ© contextuel
        chunk_result['quality_score'] = self._compute_contextual_quality_score(text_chunk, fused_activations)
        
        processing_time = time.time() - start_time
        chunk_result['processing_time'] = processing_time
        
        self.logger.debug(f"Chunk {chunk_id} terminÃ© en {processing_time:.2f}s - QualitÃ©: {chunk_result['quality_score']:.3f}")
        
        return chunk_result
    
    def _analyze_in_prime_dimension(self, text: str, prime_dim: int) -> Dict[str, Any]:
        """Analyse texte dans espace prime_dim dimensionnel"""
        space = self.prime_spaces[prime_dim]
        
        # DÃ©tection activations dhÄtu dans texte
        activations = {}
        
        for dhatu_name, dhatu in self.geometric_dhatus.items():
            # Projection dans espace prime_dim
            if prime_dim == 5:
                vector = dhatu.base_5d
            elif prime_dim == 7:
                vector = dhatu.base_7d 
            elif prime_dim == 11:
                vector = dhatu.base_11d
            else:
                vector = dhatu.base_2d
                
            # DÃ©tection patterns dans texte (simulation)
            pattern_strength = self._detect_dhatu_patterns(text, dhatu_name)
            
            # Projection gÃ©omÃ©trique
            projected_activation = np.dot(vector, space['basis_vectors'][:len(vector), :])
            
            activations[dhatu_name] = {
                'pattern_strength': pattern_strength,
                'geometric_projection': projected_activation.tolist(),
                'dimension': prime_dim
            }
        
        return {
            'dimension': prime_dim,
            'activations': activations,
            'space_coverage': self._compute_space_coverage(activations, prime_dim)
        }
    
    def _detect_dhatu_patterns(self, text: str, dhatu_name: str) -> float:
        """DÃ©tection patterns dhÄtu dans texte (version simplifiÃ©e pour demo)"""
        # Patterns de base par dhÄtu  
        patterns = {
            'RELATE': ['with', 'between', 'among', 'relationship'],
            'MODAL': ['can', 'could', 'should', 'must', 'may'],
            'EXIST': ['is', 'are', 'be', 'being', 'exist'],
            'EVAL': ['good', 'bad', 'better', 'compare', 'quality'],
            'COMM': ['say', 'tell', 'speak', 'write', 'message'],
            'CAUSE': ['because', 'make', 'cause', 'reason'],
            'ITER': ['again', 'repeat', 'continue', 'keep'],
            'DECIDE': ['choose', 'decide', 'select', 'pick'],
            'FEEL': ['feel', 'emotion', 'mood', 'sense']
        }
        
        dhatu_patterns = patterns.get(dhatu_name, [])
        
        # Comptage occurrences (normalisÃ©)
        text_lower = text.lower()
        total_matches = sum(text_lower.count(pattern) for pattern in dhatu_patterns)
        
        # Normalisation par longueur texte
        strength = min(total_matches / max(len(text_lower.split()), 1), 1.0)
        
        return strength
    
    def _compute_space_coverage(self, activations: Dict[str, Any], dimension: int) -> float:
        """Calcule couverture espace gÃ©omÃ©trique"""
        active_vectors = []
        
        for dhatu_name, activation in activations.items():
            if activation['pattern_strength'] > 0.1:
                active_vectors.append(np.array(activation['geometric_projection']))
        
        if len(active_vectors) < 2:
            return 0.0
        
        # Volume parallÃ©lÃ©pipÃ¨de formÃ© par vecteurs actifs (approximation)
        active_matrix = np.array(active_vectors)
        try:
            coverage = abs(np.linalg.det(active_matrix[:dimension, :dimension]))
            return min(coverage, 1.0)
        except:
            return 0.0
    
    def _fuse_dimensional_activations(self, geometric_features: Dict[str, Any]) -> Dict[str, Any]:
        """Fusionne activations multi-dimensionnelles"""
        fused_activations = {}
        
        for dhatu_name in self.geometric_dhatus.keys():
            activations_by_dim = {}
            total_strength = 0.0
            
            for dim_key, dim_analysis in geometric_features.items():
                dim_num = int(dim_key.split('_')[1])
                dhatu_activation = dim_analysis['activations'].get(dhatu_name, {})
                
                pattern_strength = dhatu_activation.get('pattern_strength', 0.0)
                activations_by_dim[dim_num] = pattern_strength
                total_strength += pattern_strength
            
            # Moyenne pondÃ©rÃ©e par dimension
            weights = {2: 1.0, 5: 2.0, 7: 1.5, 11: 1.2}  # Poids selon complexitÃ©
            weighted_strength = sum(
                activations_by_dim.get(dim, 0.0) * weights.get(dim, 1.0) 
                for dim in self.config.prime_dimensions
            ) / sum(weights.get(dim, 1.0) for dim in self.config.prime_dimensions)
            
            fused_activations[dhatu_name] = {
                'total_strength': total_strength,
                'weighted_strength': weighted_strength,
                'dimensional_breakdown': activations_by_dim
            }
        
        return fused_activations
    
    def _compute_contextual_quality_score(self, text: str, activations: Dict[str, Any]) -> float:
        """Score qualitÃ© contextuel utilisant dictionnaire raffinÃ©"""
        quality_factors = []
        
        # DiversitÃ© dhÄtu activÃ©s
        active_dhatus = sum(1 for act in activations.values() if act['weighted_strength'] > 0.1)
        diversity_score = min(active_dhatus / len(self.geometric_dhatus), 1.0)
        quality_factors.append(('diversity', diversity_score, 0.3))
        
        # CohÃ©rence contextuelle (basÃ©e sur dictionnaire raffinÃ©)
        coherence_score = self._compute_contextual_coherence(activations)
        quality_factors.append(('coherence', coherence_score, 0.4))
        
        # ExpressivitÃ© gÃ©omÃ©trique
        expressivity_score = self._compute_geometric_expressivity(activations)
        quality_factors.append(('expressivity', expressivity_score, 0.3))
        
        # Score pondÃ©rÃ©
        total_score = sum(score * weight for _, score, weight in quality_factors)
        
        return total_score
    
    def _compute_contextual_coherence(self, activations: Dict[str, Any]) -> float:
        """CohÃ©rence contextuelle basÃ©e dictionnaire raffinÃ©"""
        coherence_sum = 0.0
        active_count = 0
        
        for dhatu_name, activation in activations.items():
            if activation['weighted_strength'] > 0.1:
                # Cherche cohÃ©rence contextuelle dans dictionnaire raffinÃ©
                if dhatu_name in self.refined_dict.get('concepts_raffines', {}):
                    concept_data = self.refined_dict['concepts_raffines'][dhatu_name]
                    confidence_score = concept_data.get('confidence_score', 0.5)
                    coherence_sum += confidence_score
                else:
                    coherence_sum += 0.5  # Score neutre
                
                active_count += 1
        
        return coherence_sum / max(active_count, 1)
    
    def _compute_geometric_expressivity(self, activations: Dict[str, Any]) -> float:
        """ExpressivitÃ© gÃ©omÃ©trique multi-dimensionnelle"""
        expressivity_scores = []
        
        for dhatu_name, activation in activations.items():
            dimensional_variance = np.var(list(activation['dimensional_breakdown'].values()))
            expressivity_scores.append(dimensional_variance)
        
        # Variance moyenne = expressivitÃ©
        return np.mean(expressivity_scores) if expressivity_scores else 0.0

    def run_parallel_corpus_analysis(self, corpus_files: List[str]) -> ParallelAnalysisResult:
        """Lance analyse corpus parallÃ¨le complÃ¨te"""
        self.logger.info(f"ğŸš€ DÃ‰BUT ANALYSE PARALLÃˆLE - Experiment {self.config.experiment_id}")
        self.logger.info(f"ğŸ“ Fichiers corpus: {len(corpus_files)}")
        self.logger.info(f"âš™ï¸ Workers concurrents: {self.config.concurrent_workers}")
        
        start_time = time.time()
        
        # Chargement et chunking corpus
        all_chunks = self._load_and_chunk_corpus(corpus_files)
        self.logger.info(f"ğŸ“Š Chunks crÃ©Ã©s: {len(all_chunks)}")
        
        # Analyse parallÃ¨le
        chunk_results = []
        
        with ProcessPoolExecutor(max_workers=self.config.concurrent_workers) as executor:
            # Soumission tÃ¢ches
            future_to_chunk = {
                executor.submit(self.analyze_corpus_chunk_parallel, chunk_text, chunk_id): chunk_id
                for chunk_id, chunk_text in enumerate(all_chunks)
            }
            
            # Collecte rÃ©sultats avec monitoring
            for future in as_completed(future_to_chunk):
                chunk_id = future_to_chunk[future]
                
                try:
                    chunk_result = future.result()
                    chunk_results.append(chunk_result)
                    
                    # Monitoring pÃ©riodique
                    if len(chunk_results) % 10 == 0:
                        self.monitor.measure(f"chunks_processed_{len(chunk_results)}")
                        self.logger.info(f"âœ… Chunks traitÃ©s: {len(chunk_results)}/{len(all_chunks)}")
                        
                except Exception as e:
                    self.logger.error(f"Erreur chunk {chunk_id}: {e}")
        
        # Fusion rÃ©sultats et analyse finale
        final_result = self._synthesize_parallel_results(chunk_results)
        final_result.processing_time = time.time() - start_time
        final_result.experiment_id = self.config.experiment_id
        
        # MÃ©triques finales
        final_measurement = self.monitor.measure("analysis_complete")
        final_result.memory_usage = final_measurement['memory']
        final_result.cpu_usage = final_measurement['cpu']
        
        self.logger.info(f"ğŸ¯ ANALYSE TERMINÃ‰E en {final_result.processing_time:.1f}s")
        self.logger.info(f"ğŸ’¾ MÃ©moire max: {max(m['memory']['rss'] for m in self.monitor.measurements):.1f}MB")
        self.logger.info(f"âš¡ CPU moyen: {np.mean([m['cpu']['percent'] for m in self.monitor.measurements]):.1f}%")
        
        return final_result
    
    def _load_and_chunk_corpus(self, corpus_files: List[str]) -> List[str]:
        """Charge et dÃ©coupe corpus en chunks pour parallÃ©lisation"""
        all_chunks = []
        
        for file_path in corpus_files:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # Chunking par taille
                    chunk_size = self.config.corpus_chunk_size
                    for i in range(0, len(content), chunk_size):
                        chunk = content[i:i + chunk_size]
                        if len(chunk.strip()) > 50:  # Ã‰viter chunks trop petits
                            all_chunks.append(chunk)
                    
                    self.logger.debug(f"Fichier {file_path} chargÃ©: {len(content)} chars â†’ {len(all_chunks)} chunks")
                    
                except Exception as e:
                    self.logger.error(f"Erreur lecture {file_path}: {e}")
            else:
                self.logger.warning(f"Fichier introuvable: {file_path}")
        
        return all_chunks
    
    def _synthesize_parallel_results(self, chunk_results: List[Dict[str, Any]]) -> ParallelAnalysisResult:
        """SynthÃ¨se rÃ©sultats parallÃ¨les avec mÃ©triques critiques"""
        
        # Fusion activations dhÄtu
        global_dhatu_activations = {}
        total_quality_score = 0.0
        
        for chunk_result in chunk_results:
            total_quality_score += chunk_result.get('quality_score', 0.0)
            
            for dhatu_name, activation in chunk_result.get('dhatu_activations', {}).items():
                if dhatu_name not in global_dhatu_activations:
                    global_dhatu_activations[dhatu_name] = {
                        'total_strength': 0.0,
                        'weighted_strength': 0.0,
                        'activation_count': 0
                    }
                
                global_dhatu_activations[dhatu_name]['total_strength'] += activation.get('total_strength', 0.0)
                global_dhatu_activations[dhatu_name]['weighted_strength'] += activation.get('weighted_strength', 0.0)
                global_dhatu_activations[dhatu_name]['activation_count'] += 1
        
        # Moyennes globales
        for dhatu_name in global_dhatu_activations:
            count = global_dhatu_activations[dhatu_name]['activation_count']
            if count > 0:
                global_dhatu_activations[dhatu_name]['avg_strength'] = \
                    global_dhatu_activations[dhatu_name]['weighted_strength'] / count
        
        # MÃ©triques qualitÃ©
        avg_quality = total_quality_score / max(len(chunk_results), 1)
        
        quality_metrics = {
            'average_quality': avg_quality,
            'dhatu_coverage': len([d for d in global_dhatu_activations.values() if d.get('avg_strength', 0) > 0.1]),
            'processing_efficiency': len(chunk_results) / self.config.concurrent_workers,
            'geometric_expressivity': self._compute_global_geometric_expressivity(global_dhatu_activations)
        }
        
        # Insights critiques
        critical_insights = self._generate_critical_insights(global_dhatu_activations, quality_metrics)
        improvement_suggestions = self._generate_improvement_suggestions(global_dhatu_activations, quality_metrics)
        
        return ParallelAnalysisResult(
            experiment_id=self.config.experiment_id,
            processing_time=0.0,  # Sera rempli par run_parallel_corpus_analysis
            memory_usage={},      # Sera rempli par run_parallel_corpus_analysis  
            cpu_usage={},         # Sera rempli par run_parallel_corpus_analysis
            dhatu_activations=global_dhatu_activations,
            geometric_relations=[],  # TODO: implÃ©menter si nÃ©cessaire
            quality_metrics=quality_metrics,
            critical_insights=critical_insights,
            improvement_suggestions=improvement_suggestions
        )
    
    def _compute_global_geometric_expressivity(self, activations: Dict[str, Any]) -> float:
        """ExpressivitÃ© gÃ©omÃ©trique globale"""
        expressivity_values = []
        
        for dhatu_name, activation_data in activations.items():
            avg_strength = activation_data.get('avg_strength', 0.0)
            if avg_strength > 0.1:
                expressivity_values.append(avg_strength)
        
        # Variance des forces d'activation = expressivitÃ©
        return np.var(expressivity_values) if len(expressivity_values) > 1 else 0.0
    
    def _generate_critical_insights(self, activations: Dict[str, Any], quality_metrics: Dict[str, float]) -> List[str]:
        """GÃ©nÃ¨re insights critiques pour amÃ©lioration"""
        insights = []
        
        # Top dhÄtu activÃ©s
        top_dhatus = sorted(activations.items(), key=lambda x: x[1].get('avg_strength', 0), reverse=True)[:3]
        insights.append(f"ğŸ”¥ DhÄtu dominants: {', '.join([d[0] for d in top_dhatus])}")
        
        # Couverture gÃ©omÃ©trique  
        coverage = quality_metrics['dhatu_coverage']
        if coverage < 5:
            insights.append(f"âš ï¸ Couverture dhÄtu faible: {coverage}/9 - Texte possiblement spÃ©cialisÃ©")
        elif coverage >= 7:
            insights.append(f"âœ… Excellente couverture dhÄtu: {coverage}/9 - Texte riche et variÃ©")
        
        # ExpressivitÃ© gÃ©omÃ©trique
        expressivity = quality_metrics['geometric_expressivity']
        if expressivity > 0.1:
            insights.append(f"ğŸ¯ Haute expressivitÃ© gÃ©omÃ©trique: {expressivity:.3f} - Nuances complexes dÃ©tectÃ©es")
        
        return insights
    
    def _generate_improvement_suggestions(self, activations: Dict[str, Any], quality_metrics: Dict[str, float]) -> List[str]:
        """Suggestions d'amÃ©lioration basÃ©es sur analyse critique"""
        suggestions = []
        
        # EfficacitÃ© traitement
        efficiency = quality_metrics['processing_efficiency']
        if efficiency < 0.8:
            suggestions.append(f"ğŸš€ Augmenter workers concurrents (efficacitÃ© actuelle: {efficiency:.2f})")
        
        # QualitÃ© moyenne
        avg_quality = quality_metrics['average_quality']
        if avg_quality < 0.6:
            suggestions.append(f"ğŸ“Š AmÃ©liorer qualitÃ© corpus (score actuel: {avg_quality:.2f}) - Filtrer ou enrichir donnÃ©es")
        
        # DhÄtu sous-utilisÃ©s
        underused_dhatus = [name for name, data in activations.items() if data.get('avg_strength', 0) < 0.05]
        if len(underused_dhatus) > 3:
            suggestions.append(f"ğŸ” AmÃ©liorer dÃ©tection patterns pour: {', '.join(underused_dhatus[:3])}")
        
        # Dimensions gÃ©omÃ©triques
        if quality_metrics['geometric_expressivity'] < 0.05:
            suggestions.append("ğŸ“ Explorer dimensions supÃ©rieures (13D, 17D) pour expressivitÃ© accrue")
        
        return suggestions

def create_experimental_config() -> ExperimentalConfig:
    """CrÃ©e configuration expÃ©rimentale pour reproductibilitÃ©"""
    
    # DÃ©tection corpus disponibles
    corpus_files = []
    potential_files = [
        'data/corpus_complet_unifie.json',
        'data/corpus_multilingue_dev.json', 
        'data/corpus_prescolaire.json',
        'data/corpus_scientifique.json',
        'corpus_complet_unifie.json',
        'corpus_multilingue_dev.json', 
        'corpus_prescolaire.json',
        'corpus_scientifique.json'
    ]
    
    for file in potential_files:
        if os.path.exists(file):
            corpus_files.append(file)
    
    # Configuration systÃ¨me
    system_specs = {
        'cpu_count': psutil.cpu_count(logical=False),
        'cpu_count_logical': psutil.cpu_count(logical=True),
        'memory_total_gb': psutil.virtual_memory().total / (1024**3),
        'python_version': os.sys.version,
        'platform': os.sys.platform
    }
    
    # Workers optimaux (75% des cÅ“urs logiques)
    optimal_workers = max(1, int(psutil.cpu_count(logical=True) * 0.75))
    
    experiment_id = f"geometric_parallel_{int(time.time())}"
    
    config = ExperimentalConfig(
        experiment_id=experiment_id,
        prime_dimensions=[5, 7, 11],  # Dimensions supÃ©rieures
        dhatu_count=9,
        corpus_chunk_size=8192,      # 8KB par chunk
        concurrent_workers=optimal_workers,
        geometric_thresholds={
            'activation_threshold': 0.1,
            'quality_threshold': 0.6,
            'expressivity_threshold': 0.05
        },
        random_seed=42,              # ReproductibilitÃ©
        system_specs=system_specs,
        input_files=corpus_files,
        algorithm_version="geometric_parallel_v4.0"
    )
    
    return config

def main():
    """Point d'entrÃ©e analyse corpus gÃ©omÃ©trique parallÃ¨le"""
    
    print("ğŸ”º ANALYSEUR CORPUS GÃ‰OMÃ‰TRIQUE PARALLÃˆLE v4.0")
    print("=" * 60)
    
    # Configuration expÃ©rimentale
    config = create_experimental_config()
    print(f"ğŸ§ª Experiment ID: {config.experiment_id}")
    print(f"ğŸ“ Dimensions: {config.prime_dimensions}")
    print(f"âš™ï¸ Workers: {config.concurrent_workers}")
    print(f"ğŸ“ Corpus files: {len(config.input_files)}")
    
    # Logging expÃ©rimental
    logger, log_file = setup_experimental_logging(config.experiment_id)
    logger.info("ğŸš€ DÃ‰BUT EXPÃ‰RIENCE GÃ‰OMÃ‰TRIQUE PARALLÃˆLE")
    
    # Sauvegarde configuration
    config_file = f"config_{config.experiment_id}.json"
    config.save_config(config_file)
    logger.info(f"Configuration sauvegardÃ©e: {config_file}")
    print(f"ğŸ“‹ Config hash: {config.hash_config()}")
    
    if not config.input_files:
        print("âŒ Aucun fichier corpus trouvÃ©")
        logger.error("Aucun fichier corpus disponible")
        return
    
    try:
        # Initialisation analyseur
        analyzer = GeometricDhatuAnalyzer(config, logger)
        
        # Lancement analyse parallÃ¨le
        result = analyzer.run_parallel_corpus_analysis(config.input_files)
        
        # Sauvegarde rÃ©sultats
        result_file = f"parallel_analysis_result_{config.experiment_id}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(result), f, ensure_ascii=False, indent=2)
        
        # Rapport final
        print(f"\nğŸ¯ ANALYSE TERMINÃ‰E")
        print(f"â±ï¸  DurÃ©e: {result.processing_time:.1f}s")
        print(f"ğŸ’¾ MÃ©moire max: {result.memory_usage.get('rss', 0):.1f}MB")
        print(f"ğŸ“Š QualitÃ© moyenne: {result.quality_metrics.get('average_quality', 0):.3f}")
        print(f"ğŸ”¥ DhÄtu coverage: {result.quality_metrics.get('dhatu_coverage', 0)}/9")
        
        print(f"\nğŸ’¡ INSIGHTS CRITIQUES:")
        for insight in result.critical_insights:
            print(f"   {insight}")
        
        print(f"\nğŸš€ SUGGESTIONS D'AMÃ‰LIORATION:")
        for suggestion in result.improvement_suggestions:
            print(f"   {suggestion}")
        
        print(f"\nğŸ“„ RÃ©sultats: {result_file}")
        print(f"ğŸ“„ Log dÃ©taillÃ©: {log_file}")
        print(f"ğŸ“„ Configuration: {config_file}")
        
        logger.info("âœ… EXPÃ‰RIENCE TERMINÃ‰E AVEC SUCCÃˆS")
        
    except Exception as e:
        logger.error(f"Erreur critique: {e}")
        print(f"âŒ ERREUR: {e}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    main()