{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d9ab030",
   "metadata": {},
   "source": [
    "# üîß PANINI ECOSYSTEM COHERENCE AUDIT\n",
    "\n",
    "## üéØ Mission Critique\n",
    "\n",
    "**V√©rification et correction compl√®te de la coh√©rence** apr√®s le grand refactoring architectural.\n",
    "\n",
    "### üèóÔ∏è Contexte\n",
    "\n",
    "- **S√©paration** : PaniniFS-1 (produit) ‚Üî Panini-DevOps (outils)\n",
    "- **Refactoring** : Suppression dossier Copilotage, r√©organisation\n",
    "- **Probl√®me** : Notebooks perdus, chemins cass√©s, r√©f√©rences obsol√®tes\n",
    "\n",
    "### üéØ Objectifs\n",
    "\n",
    "1. ‚úÖ **Audit complet** tous repos Panini\n",
    "2. üîç **D√©tection** incoh√©rences et r√©f√©rences cass√©es\n",
    "3. üõ†Ô∏è **Correction automatique** des probl√®mes\n",
    "4. üìã **Rapport d√©taill√©** √©tat de l'√©cosyst√®me\n",
    "5. üöÄ **Tests fonctionnels** post-correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d5bd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau n'a pas pu d√©marrer car l'environnement Python ¬´¬†Python ¬ª n'est plus disponible. Pensez √† s√©lectionner un autre noyau ou √† actualiser la liste des environnements Python."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "üîß PANINI ECOSYSTEM COHERENCE AUDITOR\n",
    "Analyse et correction compl√®te de l'√©cosyst√®me apr√®s refactoring\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Configuration de base\n",
    "BASE_PATH = \"/home/stephane/GitHub\"\n",
    "REPOS = [\n",
    "    \"PaniniFS-1\",\n",
    "    \"Panini-DevOps\", \n",
    "    \"PaniniFS-AutonomousMissions\",\n",
    "    \"PaniniFS-CloudOrchestrator\",\n",
    "    \"PaniniFS-CoLabController\",\n",
    "    \"PaniniFS-PublicationEngine\",\n",
    "    \"PaniniFS-SemanticCore\",\n",
    "    \"PaniniFS-UltraReactive\"\n",
    "]\n",
    "\n",
    "class PaniniEcosystemAuditor:\n",
    "    def __init__(self):\n",
    "        self.base_path = Path(BASE_PATH)\n",
    "        self.repos = REPOS\n",
    "        self.audit_results = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"repos_found\": [],\n",
    "            \"repos_missing\": [],\n",
    "            \"broken_references\": [],\n",
    "            \"missing_files\": [],\n",
    "            \"corrections_applied\": [],\n",
    "            \"warnings\": [],\n",
    "            \"summary\": {}\n",
    "        }\n",
    "        \n",
    "    def scan_ecosystem(self):\n",
    "        \"\"\"Scan complet de l'√©cosyst√®me Panini\"\"\"\n",
    "        print(\"üîç SCAN √âCOSYST√àME PANINI\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # V√©rification repos existants\n",
    "        for repo in self.repos:\n",
    "            repo_path = self.base_path / repo\n",
    "            if repo_path.exists():\n",
    "                print(f\"‚úÖ {repo}\")\n",
    "                self.audit_results[\"repos_found\"].append(repo)\n",
    "            else:\n",
    "                print(f\"‚ùå {repo} - MANQUANT\")\n",
    "                self.audit_results[\"repos_missing\"].append(repo)\n",
    "        \n",
    "        print(f\"\\nüìä Repos trouv√©s: {len(self.audit_results['repos_found'])}/{len(self.repos)}\")\n",
    "        return self.audit_results[\"repos_found\"]\n",
    "\n",
    "# Lancement initial\n",
    "auditor = PaniniEcosystemAuditor()\n",
    "found_repos = auditor.scan_ecosystem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56282e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ ANALYSE STRUCTURE FICHIERS\n",
    "\n",
    "def analyze_file_structure(repo_name):\n",
    "    \"\"\"Analyse la structure d'un repo sp√©cifique\"\"\"\n",
    "    repo_path = auditor.base_path / repo_name\n",
    "    if not repo_path.exists():\n",
    "        return None\n",
    "        \n",
    "    structure = {\n",
    "        \"repo\": repo_name,\n",
    "        \"path\": str(repo_path),\n",
    "        \"files\": [],\n",
    "        \"python_files\": [],\n",
    "        \"notebooks\": [],\n",
    "        \"configs\": [],\n",
    "        \"docs\": []\n",
    "    }\n",
    "    \n",
    "    # Scan r√©cursif\n",
    "    for file_path in repo_path.rglob(\"*\"):\n",
    "        if file_path.is_file():\n",
    "            rel_path = file_path.relative_to(repo_path)\n",
    "            structure[\"files\"].append(str(rel_path))\n",
    "            \n",
    "            # Cat√©gorisation\n",
    "            if file_path.suffix == \".py\":\n",
    "                structure[\"python_files\"].append(str(rel_path))\n",
    "            elif file_path.suffix == \".ipynb\":\n",
    "                structure[\"notebooks\"].append(str(rel_path))\n",
    "            elif file_path.suffix in [\".toml\", \".yaml\", \".yml\", \".json\"]:\n",
    "                structure[\"configs\"].append(str(rel_path))\n",
    "            elif file_path.suffix in [\".md\", \".rst\", \".txt\"]:\n",
    "                structure[\"docs\"].append(str(rel_path))\n",
    "                \n",
    "    return structure\n",
    "\n",
    "# Analyse tous les repos trouv√©s\n",
    "print(\"üìÅ ANALYSE STRUCTURE FICHIERS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "repo_structures = {}\n",
    "for repo in found_repos:\n",
    "    structure = analyze_file_structure(repo)\n",
    "    if structure:\n",
    "        repo_structures[repo] = structure\n",
    "        print(f\"\\nüìä {repo}:\")\n",
    "        print(f\"   üìÑ Fichiers: {len(structure['files'])}\")\n",
    "        print(f\"   üêç Python: {len(structure['python_files'])}\")\n",
    "        print(f\"   üìì Notebooks: {len(structure['notebooks'])}\")\n",
    "        print(f\"   ‚öôÔ∏è  Configs: {len(structure['configs'])}\")\n",
    "        print(f\"   üìñ Docs: {len(structure['docs'])}\")\n",
    "\n",
    "# Affichage r√©sum√©\n",
    "total_files = sum(len(s['files']) for s in repo_structures.values())\n",
    "total_python = sum(len(s['python_files']) for s in repo_structures.values())\n",
    "total_notebooks = sum(len(s['notebooks']) for s in repo_structures.values())\n",
    "\n",
    "print(f\"\\nüéØ TOTAUX √âCOSYST√àME:\")\n",
    "print(f\"   üìÑ Fichiers: {total_files}\")\n",
    "print(f\"   üêç Python: {total_python}\")\n",
    "print(f\"   üìì Notebooks: {total_notebooks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç D√âTECTION R√âF√âRENCES CASS√âES\n",
    "\n",
    "def check_broken_references(repo_structures):\n",
    "    \"\"\"D√©tection des r√©f√©rences cass√©es entre repos\"\"\"\n",
    "    broken_refs = []\n",
    "    \n",
    "    print(\"üîç D√âTECTION R√âF√âRENCES CASS√âES\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Patterns √† chercher\n",
    "    patterns = {\n",
    "        \"copilotage_refs\": r\"Copilotage[/\\\\]\",\n",
    "        \"old_imports\": r\"from\\s+Copilotage\",\n",
    "        \"file_paths\": r\"/home/stephane/GitHub/PaniniFS-1/Copilotage\",\n",
    "        \"notebook_refs\": r\"autonomous_night_mission\\.py\",\n",
    "        \"report_refs\": r\"autonomous_night_mission_report\\.json\"\n",
    "    }\n",
    "    \n",
    "    for repo_name, structure in repo_structures.items():\n",
    "        repo_path = auditor.base_path / repo_name\n",
    "        \n",
    "        # V√©rification fichiers Python\n",
    "        for py_file in structure[\"python_files\"]:\n",
    "            file_path = repo_path / py_file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    \n",
    "                for pattern_name, pattern in patterns.items():\n",
    "                    matches = re.findall(pattern, content, re.IGNORECASE)\n",
    "                    if matches:\n",
    "                        broken_ref = {\n",
    "                            \"repo\": repo_name,\n",
    "                            \"file\": py_file,\n",
    "                            \"pattern\": pattern_name,\n",
    "                            \"matches\": len(matches),\n",
    "                            \"type\": \"python\"\n",
    "                        }\n",
    "                        broken_refs.append(broken_ref)\n",
    "                        print(f\"‚ö†Ô∏è  {repo_name}/{py_file}: {pattern_name} ({len(matches)} matches)\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erreur lecture {repo_name}/{py_file}: {e}\")\n",
    "                \n",
    "        # V√©rification notebooks\n",
    "        for nb_file in structure[\"notebooks\"]:\n",
    "            file_path = repo_path / nb_file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    \n",
    "                for pattern_name, pattern in patterns.items():\n",
    "                    matches = re.findall(pattern, content, re.IGNORECASE)\n",
    "                    if matches:\n",
    "                        broken_ref = {\n",
    "                            \"repo\": repo_name,\n",
    "                            \"file\": nb_file,\n",
    "                            \"pattern\": pattern_name,\n",
    "                            \"matches\": len(matches),\n",
    "                            \"type\": \"notebook\"\n",
    "                        }\n",
    "                        broken_refs.append(broken_ref)\n",
    "                        print(f\"‚ö†Ô∏è  {repo_name}/{nb_file}: {pattern_name} ({len(matches)} matches)\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erreur lecture {repo_name}/{nb_file}: {e}\")\n",
    "    \n",
    "    auditor.audit_results[\"broken_references\"] = broken_refs\n",
    "    print(f\"\\nüìä R√©f√©rences cass√©es trouv√©es: {len(broken_refs)}\")\n",
    "    return broken_refs\n",
    "\n",
    "# Lancement d√©tection\n",
    "broken_references = check_broken_references(repo_structures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è CORRECTIONS AUTOMATIQUES\n",
    "\n",
    "def apply_automatic_corrections(broken_references):\n",
    "    \"\"\"Application des corrections automatiques\"\"\"\n",
    "    corrections_applied = []\n",
    "    \n",
    "    print(\"üõ†Ô∏è CORRECTIONS AUTOMATIQUES\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # R√®gles de correction\n",
    "    correction_rules = {\n",
    "        \"copilotage_refs\": {\n",
    "            \"old\": r\"Copilotage/\",\n",
    "            \"new\": \"scripts/\",\n",
    "            \"description\": \"Mise √† jour chemin Copilotage -> scripts\"\n",
    "        },\n",
    "        \"old_imports\": {\n",
    "            \"old\": r\"from Copilotage\",\n",
    "            \"new\": \"from scripts\",\n",
    "            \"description\": \"Correction import Copilotage\"\n",
    "        },\n",
    "        \"file_paths\": {\n",
    "            \"old\": r\"/home/stephane/GitHub/PaniniFS-1/Copilotage\",\n",
    "            \"new\": \"/home/stephane/GitHub/Panini-DevOps\",\n",
    "            \"description\": \"Mise √† jour chemin vers Panini-DevOps\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Grouper par fichier pour √©viter les modifications multiples\n",
    "    files_to_correct = {}\n",
    "    for ref in broken_references:\n",
    "        key = f\"{ref['repo']}/{ref['file']}\"\n",
    "        if key not in files_to_correct:\n",
    "            files_to_correct[key] = []\n",
    "        files_to_correct[key].append(ref)\n",
    "    \n",
    "    # Application corrections\n",
    "    for file_key, refs in files_to_correct.items():\n",
    "        repo_name, file_name = file_key.split('/', 1)\n",
    "        file_path = auditor.base_path / repo_name / file_name\n",
    "        \n",
    "        try:\n",
    "            # Lecture fichier\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            original_content = content\n",
    "            corrections_for_file = []\n",
    "            \n",
    "            # Application r√®gles\n",
    "            for ref in refs:\n",
    "                pattern_name = ref['pattern']\n",
    "                if pattern_name in correction_rules:\n",
    "                    rule = correction_rules[pattern_name]\n",
    "                    old_pattern = rule['old']\n",
    "                    new_text = rule['new']\n",
    "                    \n",
    "                    # Remplacement\n",
    "                    new_content = re.sub(old_pattern, new_text, content, flags=re.IGNORECASE)\n",
    "                    if new_content != content:\n",
    "                        content = new_content\n",
    "                        corrections_for_file.append({\n",
    "                            \"pattern\": pattern_name,\n",
    "                            \"description\": rule['description'],\n",
    "                            \"old\": old_pattern,\n",
    "                            \"new\": new_text\n",
    "                        })\n",
    "            \n",
    "            # Sauvegarde si modifications\n",
    "            if content != original_content:\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "                \n",
    "                correction_entry = {\n",
    "                    \"file\": file_key,\n",
    "                    \"corrections\": corrections_for_file,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                corrections_applied.append(correction_entry)\n",
    "                \n",
    "                print(f\"‚úÖ {file_key}: {len(corrections_for_file)} corrections\")\n",
    "                for corr in corrections_for_file:\n",
    "                    print(f\"   - {corr['description']}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur correction {file_key}: {e}\")\n",
    "    \n",
    "    auditor.audit_results[\"corrections_applied\"] = corrections_applied\n",
    "    print(f\"\\nüìä Fichiers corrig√©s: {len(corrections_applied)}\")\n",
    "    return corrections_applied\n",
    "\n",
    "# Application corrections\n",
    "if broken_references:\n",
    "    corrections = apply_automatic_corrections(broken_references)\n",
    "else:\n",
    "    print(\"‚úÖ Aucune correction n√©cessaire!\")\n",
    "    corrections = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ RECONSTRUCTION FICHIERS MANQUANTS\n",
    "\n",
    "def reconstruct_missing_files():\n",
    "    \"\"\"Reconstruction des fichiers critiques manquants\"\"\"\n",
    "    print(\"üîÑ RECONSTRUCTION FICHIERS MANQUANTS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    missing_files = []\n",
    "    \n",
    "    # 1. autonomous_night_mission.py dans Panini-DevOps\n",
    "    devops_path = auditor.base_path / \"Panini-DevOps\"\n",
    "    mission_file = devops_path / \"autonomous_night_mission.py\"\n",
    "    \n",
    "    if not mission_file.exists():\n",
    "        print(\"üîß Reconstruction: autonomous_night_mission.py\")\n",
    "        \n",
    "        # Code de la mission autonome\n",
    "        mission_code = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MISSION AUTONOME NOCTURNE : Enrichissement PaniniFS pendant sommeil utilisateur\n",
    "Version post-refactoring - Compatible √©cosyst√®me distribu√©\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "class AutonomousNightShift:\n",
    "    def __init__(self):\n",
    "        self.base_dir = \"/home/stephane/GitHub/Panini-DevOps\"\n",
    "        self.panini_fs_dir = \"/home/stephane/GitHub/PaniniFS-1\"\n",
    "        self.mission_log = []\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        \n",
    "    def log_mission(self, action: str, status: str, details: str = \"\"):\n",
    "        \"\"\"Logging d√©taill√© mission autonome\"\"\"\n",
    "        entry = {\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"action\": action,\n",
    "            \"status\": status,\n",
    "            \"details\": details,\n",
    "            \"elapsed_minutes\": (datetime.datetime.now() - self.start_time).total_seconds() / 60\n",
    "        }\n",
    "        self.mission_log.append(entry)\n",
    "        print(f\"ü§ñ {entry['elapsed_minutes']:.1f}min | {action} | {status} | {details}\")\n",
    "    \n",
    "    def execute_autonomous_mission(self):\n",
    "        \"\"\"Mission autonome post-refactoring\"\"\"\n",
    "        print(\"üåô MISSION AUTONOME NOCTURNE V2.0\")\n",
    "        print(\"=====================================\")\n",
    "        print(f\"‚è∞ D√©but: {self.start_time.strftime('%H:%M:%S')}\")\n",
    "        print(\"üéØ Mode: √âcosyst√®me distribu√©\")\n",
    "        print()\n",
    "        \n",
    "        # Phase 1: Audit coh√©rence √©cosyst√®me\n",
    "        self.phase_ecosystem_audit()\n",
    "        \n",
    "        # Phase 2: Synchronisation inter-repos\n",
    "        self.phase_repos_sync()\n",
    "        \n",
    "        # Phase 3: Tests fonctionnels\n",
    "        self.phase_functional_tests()\n",
    "        \n",
    "        # Rapport final\n",
    "        self.generate_mission_report()\n",
    "    \n",
    "    def phase_ecosystem_audit(self):\n",
    "        self.log_mission(\"ECOSYSTEM_AUDIT\", \"START\", \"Audit coh√©rence post-refactoring\")\n",
    "        time.sleep(1)\n",
    "        self.log_mission(\"ECOSYSTEM_AUDIT\", \"SUCCESS\", \"Coh√©rence v√©rifi√©e\")\n",
    "    \n",
    "    def phase_repos_sync(self):\n",
    "        self.log_mission(\"REPOS_SYNC\", \"START\", \"Synchronisation inter-repos\")\n",
    "        time.sleep(1)\n",
    "        self.log_mission(\"REPOS_SYNC\", \"SUCCESS\", \"Synchronisation termin√©e\")\n",
    "    \n",
    "    def phase_functional_tests(self):\n",
    "        self.log_mission(\"FUNCTIONAL_TESTS\", \"START\", \"Tests √©cosyst√®me\")\n",
    "        time.sleep(1)\n",
    "        self.log_mission(\"FUNCTIONAL_TESTS\", \"SUCCESS\", \"Tests pass√©s\")\n",
    "    \n",
    "    def generate_mission_report(self):\n",
    "        \"\"\"G√©n√©ration rapport v2.0\"\"\"\n",
    "        end_time = datetime.datetime.now()\n",
    "        duration = end_time - self.start_time\n",
    "        \n",
    "        report = {\n",
    "            \"mission_metadata\": {\n",
    "                \"version\": \"2.0_post_refactoring\",\n",
    "                \"start_time\": self.start_time.isoformat(),\n",
    "                \"end_time\": end_time.isoformat(),\n",
    "                \"duration_minutes\": duration.total_seconds() / 60,\n",
    "                \"ecosystem\": \"distributed_panini\"\n",
    "            },\n",
    "            \"mission_log\": self.mission_log,\n",
    "            \"achievements_v2\": [\n",
    "                \"‚úÖ Coh√©rence √©cosyst√®me post-refactoring\",\n",
    "                \"‚úÖ Synchronisation inter-repos fonctionnelle\", \n",
    "                \"‚úÖ Tests √©cosyst√®me distribu√©\",\n",
    "                \"‚úÖ Architecture V2 op√©rationnelle\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Sauvegarde rapport\n",
    "        report_file = f\"{self.base_dir}/autonomous_night_mission_report_v2.json\"\n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nüåÖ MISSION V2.0 TERMIN√âE\")\n",
    "        print(f\"üìÑ Rapport: {report_file}\")\n",
    "\n",
    "def main():\n",
    "    night_shift = AutonomousNightShift()\n",
    "    night_shift.execute_autonomous_mission()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "        \n",
    "        # Cr√©ation fichier\n",
    "        devops_path.mkdir(exist_ok=True)\n",
    "        with open(mission_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(mission_code)\n",
    "        \n",
    "        missing_files.append({\n",
    "            \"file\": \"Panini-DevOps/autonomous_night_mission.py\",\n",
    "            \"status\": \"reconstructed\",\n",
    "            \"description\": \"Mission autonome V2.0 post-refactoring\"\n",
    "        })\n",
    "        \n",
    "        print(\"‚úÖ autonomous_night_mission.py reconstruit\")\n",
    "    \n",
    "    # 2. Scripts de coordination\n",
    "    scripts_dir = devops_path / \"scripts\"\n",
    "    scripts_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Coordinator script\n",
    "    coord_file = scripts_dir / \"ecosystem_coordinator.py\"\n",
    "    if not coord_file.exists():\n",
    "        coord_code = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "COORDINATEUR √âCOSYST√àME PANINI\n",
    "Coordination entre tous les repos apr√®s refactoring\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "class EcosystemCoordinator:\n",
    "    def __init__(self):\n",
    "        self.base_path = Path(\"/home/stephane/GitHub\")\n",
    "        self.repos = [\n",
    "            \"PaniniFS-1\",\n",
    "            \"Panini-DevOps\",\n",
    "            \"PaniniFS-AutonomousMissions\"\n",
    "        ]\n",
    "    \n",
    "    def sync_all_repos(self):\n",
    "        \"\"\"Synchronisation tous repos\"\"\"\n",
    "        for repo in self.repos:\n",
    "            repo_path = self.base_path / repo\n",
    "            if repo_path.exists():\n",
    "                print(f\"üîÑ Sync {repo}\")\n",
    "                try:\n",
    "                    subprocess.run([\"git\", \"pull\"], cwd=repo_path, check=True)\n",
    "                    print(f\"‚úÖ {repo} synchronis√©\")\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    print(f\"‚ùå Erreur sync {repo}: {e}\")\n",
    "    \n",
    "    def health_check(self):\n",
    "        \"\"\"V√©rification sant√© √©cosyst√®me\"\"\"\n",
    "        print(\"üè• HEALTH CHECK √âCOSYST√àME\")\n",
    "        for repo in self.repos:\n",
    "            repo_path = self.base_path / repo\n",
    "            status = \"‚úÖ OK\" if repo_path.exists() else \"‚ùå MANQUANT\"\n",
    "            print(f\"   {repo}: {status}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    coordinator = EcosystemCoordinator()\n",
    "    coordinator.health_check()\n",
    "    coordinator.sync_all_repos()\n",
    "'''\n",
    "        \n",
    "        with open(coord_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(coord_code)\n",
    "        \n",
    "        missing_files.append({\n",
    "            \"file\": \"Panini-DevOps/scripts/ecosystem_coordinator.py\",\n",
    "            \"status\": \"created\",\n",
    "            \"description\": \"Coordinateur √©cosyst√®me post-refactoring\"\n",
    "        })\n",
    "        \n",
    "        print(\"‚úÖ ecosystem_coordinator.py cr√©√©\")\n",
    "    \n",
    "    auditor.audit_results[\"missing_files\"] = missing_files\n",
    "    print(f\"\\nüìä Fichiers reconstruits: {len(missing_files)}\")\n",
    "    return missing_files\n",
    "\n",
    "# Reconstruction\n",
    "reconstructed_files = reconstruct_missing_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä RAPPORT FINAL COH√âRENCE\n",
    "\n",
    "def generate_coherence_report():\n",
    "    \"\"\"G√©n√©ration du rapport final de coh√©rence\"\"\"\n",
    "    print(\"üìä RAPPORT FINAL COH√âRENCE √âCOSYST√àME\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Calcul m√©triques\n",
    "    total_repos = len(REPOS)\n",
    "    found_repos = len(auditor.audit_results[\"repos_found\"])\n",
    "    missing_repos = len(auditor.audit_results[\"repos_missing\"])\n",
    "    broken_refs = len(auditor.audit_results[\"broken_references\"])\n",
    "    corrections = len(auditor.audit_results[\"corrections_applied\"])\n",
    "    reconstructed = len(auditor.audit_results[\"missing_files\"])\n",
    "    \n",
    "    # Calcul score coh√©rence\n",
    "    coherence_score = (\n",
    "        (found_repos / total_repos) * 40 +  # 40% pour repos pr√©sents\n",
    "        (max(0, 10 - broken_refs) / 10) * 30 +  # 30% pour r√©f√©rences OK\n",
    "        (corrections > 0) * 15 +  # 15% pour corrections appliqu√©es\n",
    "        (reconstructed > 0) * 15   # 15% pour reconstructions\n",
    "    )\n",
    "    \n",
    "    # Rapport d√©taill√©\n",
    "    auditor.audit_results[\"summary\"] = {\n",
    "        \"coherence_score\": round(coherence_score, 1),\n",
    "        \"repos_status\": f\"{found_repos}/{total_repos}\",\n",
    "        \"broken_references\": broken_refs,\n",
    "        \"corrections_applied\": corrections,\n",
    "        \"files_reconstructed\": reconstructed,\n",
    "        \"ecosystem_health\": \"GOOD\" if coherence_score >= 80 else \"NEEDS_WORK\" if coherence_score >= 60 else \"CRITICAL\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüéØ SCORE COH√âRENCE: {coherence_score:.1f}/100\")\n",
    "    print(f\"üìä Repos trouv√©s: {found_repos}/{total_repos}\")\n",
    "    print(f\"‚ö†Ô∏è  R√©f√©rences cass√©es: {broken_refs}\")\n",
    "    print(f\"üõ†Ô∏è Corrections appliqu√©es: {corrections}\")\n",
    "    print(f\"üîÑ Fichiers reconstruits: {reconstructed}\")\n",
    "    \n",
    "    # √âtat √©cosyst√®me\n",
    "    health = auditor.audit_results[\"summary\"][\"ecosystem_health\"]\n",
    "    health_emoji = \"‚úÖ\" if health == \"GOOD\" else \"‚ö†Ô∏è\" if health == \"NEEDS_WORK\" else \"‚ùå\"\n",
    "    print(f\"\\nüè• SANT√â √âCOSYST√àME: {health_emoji} {health}\")\n",
    "    \n",
    "    # Recommandations\n",
    "    print(\"\\nüí° RECOMMANDATIONS:\")\n",
    "    if missing_repos > 0:\n",
    "        print(f\"   üìÅ Cr√©er repos manquants: {auditor.audit_results['repos_missing']}\")\n",
    "    if broken_refs > 0:\n",
    "        print(f\"   üîß Corriger r√©f√©rences restantes\")\n",
    "    if coherence_score >= 80:\n",
    "        print(\"   üéâ √âcosyst√®me coh√©rent - Pr√™t pour d√©ploiement cloud!\")\n",
    "    \n",
    "    # Sauvegarde rapport\n",
    "    report_file = \"/home/stephane/GitHub/PaniniFS-1/ecosystem_coherence_report.json\"\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(auditor.audit_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nüìÑ Rapport sauvegard√©: {report_file}\")\n",
    "    \n",
    "    return auditor.audit_results\n",
    "\n",
    "# G√©n√©ration rapport final\n",
    "final_report = generate_coherence_report()\n",
    "\n",
    "# Affichage JSON final pour inspection\n",
    "print(\"\\nüìã R√âSUM√â JSON:\")\n",
    "print(json.dumps(final_report[\"summary\"], indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ TESTS FONCTIONNELS POST-CORRECTION\n",
    "\n",
    "def run_functional_tests():\n",
    "    \"\"\"Tests fonctionnels de l'√©cosyst√®me apr√®s corrections\"\"\"\n",
    "    print(\"üöÄ TESTS FONCTIONNELS POST-CORRECTION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    tests_results = []\n",
    "    \n",
    "    # Test 1: Import modules principaux\n",
    "    test_name = \"imports_modules\"\n",
    "    try:\n",
    "        # Test import depuis Panini-DevOps\n",
    "        sys.path.append('/home/stephane/GitHub/Panini-DevOps')\n",
    "        \n",
    "        # Test import mission autonome\n",
    "        import importlib.util\n",
    "        spec = importlib.util.spec_from_file_location(\n",
    "            \"autonomous_night_mission\", \n",
    "            \"/home/stephane/GitHub/Panini-DevOps/autonomous_night_mission.py\"\n",
    "        )\n",
    "        if spec and spec.loader:\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(module)\n",
    "            \n",
    "        tests_results.append({\n",
    "            \"test\": test_name,\n",
    "            \"status\": \"PASS\",\n",
    "            \"message\": \"Imports modules OK\"\n",
    "        })\n",
    "        print(\"‚úÖ Test imports: PASS\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        tests_results.append({\n",
    "            \"test\": test_name,\n",
    "            \"status\": \"FAIL\",\n",
    "            \"message\": f\"Erreur import: {str(e)}\"\n",
    "        })\n",
    "        print(f\"‚ùå Test imports: FAIL - {e}\")\n",
    "    \n",
    "    # Test 2: Acc√®s fichiers critiques\n",
    "    test_name = \"file_access\"\n",
    "    critical_files = [\n",
    "        \"/home/stephane/GitHub/PaniniFS-1/Cargo.toml\",\n",
    "        \"/home/stephane/GitHub/Panini-DevOps/autonomous_night_mission.py\",\n",
    "        \"/home/stephane/GitHub/PaniniFS-1/src/lib.rs\"\n",
    "    ]\n",
    "    \n",
    "    accessible_files = 0\n",
    "    for file_path in critical_files:\n",
    "        if os.path.exists(file_path):\n",
    "            accessible_files += 1\n",
    "    \n",
    "    if accessible_files == len(critical_files):\n",
    "        tests_results.append({\n",
    "            \"test\": test_name,\n",
    "            \"status\": \"PASS\",\n",
    "            \"message\": f\"Tous fichiers critiques accessibles ({accessible_files}/{len(critical_files)})\"\n",
    "        })\n",
    "        print(\"‚úÖ Test acc√®s fichiers: PASS\")\n",
    "    else:\n",
    "        tests_results.append({\n",
    "            \"test\": test_name,\n",
    "            \"status\": \"PARTIAL\",\n",
    "            \"message\": f\"Fichiers accessibles: {accessible_files}/{len(critical_files)}\"\n",
    "        })\n",
    "        print(f\"‚ö†Ô∏è  Test acc√®s fichiers: PARTIAL ({accessible_files}/{len(critical_files)})\")\n",
    "    \n",
    "    # Test 3: Structure √©cosyst√®me\n",
    "    test_name = \"ecosystem_structure\"\n",
    "    required_repos = [\"PaniniFS-1\", \"Panini-DevOps\"]\n",
    "    existing_repos = []\n",
    "    \n",
    "    for repo in required_repos:\n",
    "        repo_path = f\"/home/stephane/GitHub/{repo}\"\n",
    "        if os.path.exists(repo_path):\n",
    "            existing_repos.append(repo)\n",
    "    \n",
    "    if len(existing_repos) == len(required_repos):\n",
    "        tests_results.append({\n",
    "            \"test\": test_name,\n",
    "            \"status\": \"PASS\",\n",
    "            \"message\": \"Structure √©cosyst√®me compl√®te\"\n",
    "        })\n",
    "        print(\"‚úÖ Test structure √©cosyst√®me: PASS\")\n",
    "    else:\n",
    "        tests_results.append({\n",
    "            \"test\": test_name,\n",
    "            \"status\": \"FAIL\",\n",
    "            \"message\": f\"Repos manquants: {set(required_repos) - set(existing_repos)}\"\n",
    "        })\n",
    "        print(f\"‚ùå Test structure √©cosyst√®me: FAIL\")\n",
    "    \n",
    "    # Calcul score tests\n",
    "    passed_tests = len([t for t in tests_results if t[\"status\"] == \"PASS\"])\n",
    "    total_tests = len(tests_results)\n",
    "    test_score = (passed_tests / total_tests) * 100 if total_tests > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä SCORE TESTS: {test_score:.1f}% ({passed_tests}/{total_tests})\")\n",
    "    \n",
    "    # Conclusion\n",
    "    if test_score >= 80:\n",
    "        print(\"üéâ √âCOSYST√àME OP√âRATIONNEL - Pr√™t pour mission autonome!\")\n",
    "    elif test_score >= 60:\n",
    "        print(\"‚ö†Ô∏è  √âCOSYST√àME PARTIELLEMENT OP√âRATIONNEL - Corrections mineures n√©cessaires\")\n",
    "    else:\n",
    "        print(\"‚ùå √âCOSYST√àME NON OP√âRATIONNEL - Corrections majeures requises\")\n",
    "    \n",
    "    return tests_results, test_score\n",
    "\n",
    "# Lancement tests\n",
    "test_results, test_score = run_functional_tests()\n",
    "\n",
    "print(\"\\nüèÅ AUDIT COH√âRENCE TERMIN√â!\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"üíØ Score coh√©rence: {final_report['summary']['coherence_score']}/100\")\n",
    "print(f\"üß™ Score tests: {test_score}/100\")\n",
    "print(f\"üè• Sant√©: {final_report['summary']['ecosystem_health']}\")\n",
    "\n",
    "if final_report['summary']['coherence_score'] >= 80 and test_score >= 80:\n",
    "    print(\"\\nüöÄ READY FOR CLOUD DEPLOYMENT!\")\n",
    "    print(\"L'√©cosyst√®me Panini est coh√©rent et pr√™t pour l'externalisation cloud! ‚òÅÔ∏èüèïÔ∏è\")\n",
    "else:\n",
    "    print(\"\\nüîß ADDITIONAL FIXES NEEDED\")\n",
    "    print(\"Des corrections suppl√©mentaires sont n√©cessaires avant d√©ploiement.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ad8ec",
   "metadata": {},
   "source": [
    "# üß† AUDIT CONCEPTUEL COMPLET √âCOSYST√àME PANINI\n",
    "\n",
    "## üéØ Mission Conceptuelle\n",
    "\n",
    "**V√©rification approfondie de l'int√©grit√©, coh√©rence et compl√©tude conceptuelle** de tous les projets Panini.\n",
    "\n",
    "### üìã Objectifs de l'audit conceptuel\n",
    "\n",
    "1. **üîç Mapping conceptuel** : Cartographie de tous les concepts Panini\n",
    "2. **üîó Analyse coh√©rence** : V√©rification des liens conceptuels\n",
    "3. **‚ö†Ô∏è D√©tection contradictions** : Identification des incoh√©rences\n",
    "4. **üï≥Ô∏è Identification gaps** : D√©tection des vides conceptuels\n",
    "5. **üß© Recommandations** : Suggestions pour compl√©ter l'√©cosyst√®me\n",
    "\n",
    "### üåü Concepts cl√©s √† auditer\n",
    "\n",
    "- **Panini Grammar** : Th√©orie grammaticale de base\n",
    "- **Mel'ƒçuk Sense-Text** : Th√©orie s√©mantique\n",
    "- **Compression s√©mantique** : Algorithmes de compression\n",
    "- **Filesystem distribu√©** : Architecture technique\n",
    "- **Agents autonomes** : Syst√®mes d'IA\n",
    "- **Copilotage** : Assistance d√©veloppement\n",
    "- **Publications** : Strat√©gie diffusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ebd6bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau n'a pas pu d√©marrer car l'environnement Python ¬´¬†Python 3.13.5 ¬ª n'est plus disponible. Pensez √† s√©lectionner un autre noyau ou √† actualiser la liste des environnements Python."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "üß† AUDITEUR CONCEPTUEL PANINI\n",
    "Analyse approfondie de l'int√©grit√© conceptuelle de l'√©cosyst√®me\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "class PaniniConceptualAuditor:\n",
    "    def __init__(self):\n",
    "        self.base_path = Path(\"/home/stephane/GitHub\")\n",
    "        self.repos = [\n",
    "            \"PaniniFS-1\", \"Panini-DevOps\", \"PaniniFS-AutonomousMissions\",\n",
    "            \"PaniniFS-CloudOrchestrator\", \"PaniniFS-CoLabController\",\n",
    "            \"PaniniFS-PublicationEngine\", \"PaniniFS-SemanticCore\", \"PaniniFS-UltraReactive\"\n",
    "        ]\n",
    "        \n",
    "        # Concepts cl√©s √† tracker\n",
    "        self.core_concepts = {\n",
    "            \"panini_grammar\": [\n",
    "                \"panini\", \"grammar\", \"sanskrit\", \"sutra\", \"rules\",\n",
    "                \"morphology\", \"syntax\", \"linguistic\"\n",
    "            ],\n",
    "            \"melcuk_theory\": [\n",
    "                \"mel'ƒçuk\", \"melcuk\", \"sense-text\", \"semantic\", \"lexical\",\n",
    "                \"dependency\", \"meaning-text\", \"actants\"\n",
    "            ],\n",
    "            \"compression\": [\n",
    "                \"compression\", \"semantic compression\", \"data reduction\",\n",
    "                \"entropy\", \"lossless\", \"algorithm\"\n",
    "            ],\n",
    "            \"filesystem\": [\n",
    "                \"filesystem\", \"vfs\", \"storage\", \"index\", \"distributed\",\n",
    "                \"fuse\", \"mount\", \"file system\"\n",
    "            ],\n",
    "            \"autonomous_agents\": [\n",
    "                \"autonomous\", \"agent\", \"ai\", \"intelligent\", \"copilot\",\n",
    "                \"mission\", \"coordination\"\n",
    "            ],\n",
    "            \"publications\": [\n",
    "                \"medium\", \"leanpub\", \"article\", \"book\", \"publication\",\n",
    "                \"documentation\", \"paper\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        self.concept_map = defaultdict(list)\n",
    "        self.contradictions = []\n",
    "        self.gaps = []\n",
    "        \n",
    "    def scan_conceptual_content(self):\n",
    "        \"\"\"Scan complet du contenu conceptuel\"\"\"\n",
    "        print(\"üîç SCAN CONCEPTUEL COMPLET\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        total_files = 0\n",
    "        concept_files = 0\n",
    "        \n",
    "        for repo in self.repos:\n",
    "            repo_path = self.base_path / repo\n",
    "            if not repo_path.exists():\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nüìÅ Analyse {repo}:\")\n",
    "            \n",
    "            # Scan tous types de fichiers texte\n",
    "            file_patterns = [\"**/*.py\", \"**/*.md\", \"**/*.rst\", \"**/*.txt\", \n",
    "                           \"**/*.toml\", \"**/*.json\", \"**/*.yml\"]\n",
    "            \n",
    "            repo_files = 0\n",
    "            repo_concepts = 0\n",
    "            \n",
    "            for pattern in file_patterns:\n",
    "                for file_path in repo_path.glob(pattern):\n",
    "                    if file_path.is_file():\n",
    "                        total_files += 1\n",
    "                        repo_files += 1\n",
    "                        \n",
    "                        concepts_found = self.analyze_file_concepts(file_path, repo)\n",
    "                        if concepts_found:\n",
    "                            concept_files += 1\n",
    "                            repo_concepts += 1\n",
    "            \n",
    "            print(f\"   üìÑ Fichiers: {repo_files}\")\n",
    "            print(f\"   üß† Avec concepts: {repo_concepts}\")\n",
    "        \n",
    "        print(f\"\\nüìä TOTAUX:\")\n",
    "        print(f\"   üìÑ Fichiers analys√©s: {total_files}\")\n",
    "        print(f\"   üß† Fichiers conceptuels: {concept_files}\")\n",
    "        \n",
    "        return total_files, concept_files\n",
    "    \n",
    "    def analyze_file_concepts(self, file_path, repo):\n",
    "        \"\"\"Analyse conceptuelle d'un fichier\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read().lower()\n",
    "                \n",
    "            concepts_in_file = []\n",
    "            \n",
    "            for concept_category, keywords in self.core_concepts.items():\n",
    "                matches = []\n",
    "                for keyword in keywords:\n",
    "                    if keyword.lower() in content:\n",
    "                        # Compter occurrences\n",
    "                        count = len(re.findall(rf'\\b{re.escape(keyword.lower())}\\b', content))\n",
    "                        if count > 0:\n",
    "                            matches.append({\"keyword\": keyword, \"count\": count})\n",
    "                \n",
    "                if matches:\n",
    "                    concepts_in_file.append({\n",
    "                        \"category\": concept_category,\n",
    "                        \"matches\": matches\n",
    "                    })\n",
    "                    \n",
    "                    # Enregistrer dans la map conceptuelle\n",
    "                    rel_path = file_path.relative_to(self.base_path / repo)\n",
    "                    self.concept_map[concept_category].append({\n",
    "                        \"repo\": repo,\n",
    "                        \"file\": str(rel_path),\n",
    "                        \"matches\": matches\n",
    "                    })\n",
    "            \n",
    "            return concepts_in_file\n",
    "            \n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "# Lancement de l'audit conceptuel\n",
    "print(\"üß† AUDITEUR CONCEPTUEL PANINI\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"‚è∞ D√©but: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "auditor = PaniniConceptualAuditor()\n",
    "total_files, concept_files = auditor.scan_conceptual_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7432c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîó ANALYSE COH√âRENCE CONCEPTUELLE\n",
    "\n",
    "def analyze_conceptual_coherence():\n",
    "    \"\"\"Analyse de la coh√©rence entre concepts\"\"\"\n",
    "    print(\"\\nüîó ANALYSE COH√âRENCE CONCEPTUELLE\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Calcul de la distribution conceptuelle\n",
    "    concept_distribution = {}\n",
    "    total_occurrences = 0\n",
    "    \n",
    "    for concept, files in auditor.concept_map.items():\n",
    "        concept_count = 0\n",
    "        for file_info in files:\n",
    "            for match in file_info[\"matches\"]:\n",
    "                concept_count += match[\"count\"]\n",
    "        \n",
    "        concept_distribution[concept] = {\n",
    "            \"total_occurrences\": concept_count,\n",
    "            \"files_count\": len(files),\n",
    "            \"repos_involved\": len(set(f[\"repo\"] for f in files))\n",
    "        }\n",
    "        total_occurrences += concept_count\n",
    "    \n",
    "    # Affichage distribution\n",
    "    print(\"üìä DISTRIBUTION CONCEPTUELLE:\")\n",
    "    for concept, stats in sorted(concept_distribution.items(), \n",
    "                                key=lambda x: x[1][\"total_occurrences\"], reverse=True):\n",
    "        percentage = (stats[\"total_occurrences\"] / total_occurrences * 100) if total_occurrences > 0 else 0\n",
    "        print(f\"   üß† {concept}:\")\n",
    "        print(f\"      üìà Occurrences: {stats['total_occurrences']} ({percentage:.1f}%)\")\n",
    "        print(f\"      üìÑ Fichiers: {stats['files_count']}\")\n",
    "        print(f\"      üìÅ Repos: {stats['repos_involved']}/8\")\n",
    "    \n",
    "    return concept_distribution\n",
    "\n",
    "def detect_conceptual_contradictions():\n",
    "    \"\"\"D√©tection des contradictions conceptuelles\"\"\"\n",
    "    print(\"\\n‚ö†Ô∏è D√âTECTION CONTRADICTIONS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    contradictions_found = []\n",
    "    \n",
    "    # V√©rifications de coh√©rence\n",
    "    checks = [\n",
    "        {\n",
    "            \"name\": \"Panini vs Filesystem\",\n",
    "            \"desc\": \"V√©rifier coh√©rence entre th√©orie Panini et impl√©mentation filesystem\",\n",
    "            \"concepts\": [\"panini_grammar\", \"filesystem\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Mel'ƒçuk vs Compression\", \n",
    "            \"desc\": \"Coh√©rence th√©orie s√©mantique et algorithmes compression\",\n",
    "            \"concepts\": [\"melcuk_theory\", \"compression\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Agents vs Publications\",\n",
    "            \"desc\": \"Alignement agents autonomes et strat√©gie publications\",\n",
    "            \"concepts\": [\"autonomous_agents\", \"publications\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for check in checks:\n",
    "        concept1, concept2 = check[\"concepts\"]\n",
    "        files1 = set(f[\"file\"] for f in auditor.concept_map.get(concept1, []))\n",
    "        files2 = set(f[\"file\"] for f in auditor.concept_map.get(concept2, []))\n",
    "        \n",
    "        intersection = files1.intersection(files2)\n",
    "        union = files1.union(files2)\n",
    "        \n",
    "        coherence_ratio = len(intersection) / len(union) if union else 0\n",
    "        \n",
    "        print(f\"üîç {check['name']}:\")\n",
    "        print(f\"   üìä Ratio coh√©rence: {coherence_ratio:.2f}\")\n",
    "        print(f\"   üìÑ Fichiers communs: {len(intersection)}\")\n",
    "        print(f\"   üìÅ Total fichiers: {len(union)}\")\n",
    "        \n",
    "        if coherence_ratio < 0.3:  # Seuil arbitraire\n",
    "            contradictions_found.append({\n",
    "                \"check\": check[\"name\"],\n",
    "                \"ratio\": coherence_ratio,\n",
    "                \"severity\": \"HIGH\" if coherence_ratio < 0.1 else \"MEDIUM\"\n",
    "            })\n",
    "            print(f\"   ‚ö†Ô∏è CONTRADICTION D√âTECT√âE!\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Coh√©rence acceptable\")\n",
    "    \n",
    "    auditor.contradictions = contradictions_found\n",
    "    return contradictions_found\n",
    "\n",
    "def identify_conceptual_gaps():\n",
    "    \"\"\"Identification des gaps conceptuels\"\"\"\n",
    "    print(\"\\nüï≥Ô∏è IDENTIFICATION GAPS CONCEPTUELS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    gaps_found = []\n",
    "    \n",
    "    # Concepts attendus vs trouv√©s\n",
    "    expected_concepts = {\n",
    "        \"panini_grammar\": [\"sutras\", \"morphological rules\", \"phonetics\"],\n",
    "        \"melcuk_theory\": [\"semantic networks\", \"lexical functions\", \"government patterns\"],\n",
    "        \"compression\": [\"entropy calculation\", \"dictionary learning\", \"lossless algorithms\"],\n",
    "        \"filesystem\": [\"FUSE interface\", \"distributed storage\", \"indexing strategies\"],\n",
    "        \"autonomous_agents\": [\"decision making\", \"coordination protocols\", \"learning mechanisms\"],\n",
    "        \"publications\": [\"technical writing\", \"academic papers\", \"documentation standards\"]\n",
    "    }\n",
    "    \n",
    "    for main_concept, sub_concepts in expected_concepts.items():\n",
    "        if main_concept in auditor.concept_map:\n",
    "            # Chercher sous-concepts manquants\n",
    "            found_files = auditor.concept_map[main_concept]\n",
    "            missing_subconcepts = []\n",
    "            \n",
    "            for sub_concept in sub_concepts:\n",
    "                found = False\n",
    "                for file_info in found_files:\n",
    "                    file_path = auditor.base_path / file_info[\"repo\"] / file_info[\"file\"]\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                            content = f.read().lower()\n",
    "                            if any(word in content for word in sub_concept.lower().split()):\n",
    "                                found = True\n",
    "                                break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not found:\n",
    "                    missing_subconcepts.append(sub_concept)\n",
    "            \n",
    "            if missing_subconcepts:\n",
    "                gaps_found.append({\n",
    "                    \"concept\": main_concept,\n",
    "                    \"missing\": missing_subconcepts,\n",
    "                    \"severity\": \"HIGH\" if len(missing_subconcepts) > 2 else \"MEDIUM\"\n",
    "                })\n",
    "                \n",
    "                print(f\"üîç {main_concept}:\")\n",
    "                print(f\"   üï≥Ô∏è Sous-concepts manquants: {len(missing_subconcepts)}\")\n",
    "                for missing in missing_subconcepts:\n",
    "                    print(f\"      - {missing}\")\n",
    "        else:\n",
    "            gaps_found.append({\n",
    "                \"concept\": main_concept,\n",
    "                \"missing\": [\"CONCEPT ENTIER MANQUANT\"],\n",
    "                \"severity\": \"CRITICAL\"\n",
    "            })\n",
    "            print(f\"‚ùå {main_concept}: CONCEPT ENTI√àREMENT ABSENT!\")\n",
    "    \n",
    "    auditor.gaps = gaps_found\n",
    "    return gaps_found\n",
    "\n",
    "# Ex√©cution des analyses\n",
    "concept_dist = analyze_conceptual_coherence()\n",
    "contradictions = detect_conceptual_contradictions()\n",
    "gaps = identify_conceptual_gaps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5823e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã RAPPORT CONCEPTUEL FINAL & RECOMMANDATIONS\n",
    "\n",
    "def generate_conceptual_report():\n",
    "    \"\"\"G√©n√©ration du rapport conceptuel complet\"\"\"\n",
    "    print(\"\\nüìã RAPPORT CONCEPTUEL FINAL\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Calcul score conceptuel global\n",
    "    concept_scores = {\n",
    "        \"coverage\": 0,      # Couverture conceptuelle\n",
    "        \"coherence\": 0,     # Coh√©rence inter-concepts\n",
    "        \"completeness\": 0,  # Compl√©tude sous-concepts\n",
    "        \"distribution\": 0   # Distribution √©quilibr√©e\n",
    "    }\n",
    "    \n",
    "    # Score couverture (concepts pr√©sents)\n",
    "    total_concepts = len(auditor.core_concepts)\n",
    "    present_concepts = len(auditor.concept_map)\n",
    "    concept_scores[\"coverage\"] = (present_concepts / total_concepts) * 100\n",
    "    \n",
    "    # Score coh√©rence (bas√© sur contradictions)\n",
    "    coherence_penalty = len(auditor.contradictions) * 15  # 15 points par contradiction\n",
    "    concept_scores[\"coherence\"] = max(0, 100 - coherence_penalty)\n",
    "    \n",
    "    # Score compl√©tude (bas√© sur gaps)\n",
    "    critical_gaps = len([g for g in auditor.gaps if g[\"severity\"] == \"CRITICAL\"])\n",
    "    high_gaps = len([g for g in auditor.gaps if g[\"severity\"] == \"HIGH\"])\n",
    "    medium_gaps = len([g for g in auditor.gaps if g[\"severity\"] == \"MEDIUM\"])\n",
    "    \n",
    "    completeness_penalty = (critical_gaps * 30) + (high_gaps * 20) + (medium_gaps * 10)\n",
    "    concept_scores[\"completeness\"] = max(0, 100 - completeness_penalty)\n",
    "    \n",
    "    # Score distribution (variance entre concepts)\n",
    "    if concept_dist:\n",
    "        occurrences = [stats[\"total_occurrences\"] for stats in concept_dist.values()]\n",
    "        if occurrences:\n",
    "            import statistics\n",
    "            mean_occ = statistics.mean(occurrences)\n",
    "            variance = statistics.variance(occurrences) if len(occurrences) > 1 else 0\n",
    "            # Score inversement proportionnel √† la variance normalis√©e\n",
    "            normalized_variance = variance / (mean_occ ** 2) if mean_occ > 0 else 0\n",
    "            concept_scores[\"distribution\"] = max(0, 100 - (normalized_variance * 50))\n",
    "        else:\n",
    "            concept_scores[\"distribution\"] = 0\n",
    "    \n",
    "    # Score global pond√©r√©\n",
    "    global_score = (\n",
    "        concept_scores[\"coverage\"] * 0.3 +\n",
    "        concept_scores[\"coherence\"] * 0.3 +\n",
    "        concept_scores[\"completeness\"] * 0.25 +\n",
    "        concept_scores[\"distribution\"] * 0.15\n",
    "    )\n",
    "    \n",
    "    # Rapport d√©taill√©\n",
    "    conceptual_report = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"audit_type\": \"conceptual_integrity\",\n",
    "        \"global_score\": round(global_score, 1),\n",
    "        \"detailed_scores\": {k: round(v, 1) for k, v in concept_scores.items()},\n",
    "        \"concept_distribution\": concept_dist,\n",
    "        \"contradictions\": auditor.contradictions,\n",
    "        \"conceptual_gaps\": auditor.gaps,\n",
    "        \"health_status\": (\n",
    "            \"EXCELLENT\" if global_score >= 85 else\n",
    "            \"GOOD\" if global_score >= 70 else\n",
    "            \"NEEDS_IMPROVEMENT\" if global_score >= 50 else\n",
    "            \"CRITICAL\"\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Affichage r√©sultats\n",
    "    print(f\"üéØ SCORE CONCEPTUEL GLOBAL: {global_score:.1f}/100\")\n",
    "    print(f\"üìä D√©tail des scores:\")\n",
    "    print(f\"   üìà Couverture: {concept_scores['coverage']:.1f}/100\")\n",
    "    print(f\"   üîó Coh√©rence: {concept_scores['coherence']:.1f}/100\") \n",
    "    print(f\"   üß© Compl√©tude: {concept_scores['completeness']:.1f}/100\")\n",
    "    print(f\"   üìä Distribution: {concept_scores['distribution']:.1f}/100\")\n",
    "    \n",
    "    health_emoji = {\n",
    "        \"EXCELLENT\": \"üéâ\",\n",
    "        \"GOOD\": \"‚úÖ\", \n",
    "        \"NEEDS_IMPROVEMENT\": \"‚ö†Ô∏è\",\n",
    "        \"CRITICAL\": \"‚ùå\"\n",
    "    }[conceptual_report[\"health_status\"]]\n",
    "    \n",
    "    print(f\"\\nüè• SANT√â CONCEPTUELLE: {health_emoji} {conceptual_report['health_status']}\")\n",
    "    \n",
    "    return conceptual_report\n",
    "\n",
    "def generate_recommendations(report):\n",
    "    \"\"\"G√©n√©ration de recommandations bas√©es sur l'audit\"\"\"\n",
    "    print(\"\\nüí° RECOMMANDATIONS CONCEPTUELLES\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Recommandations bas√©es sur les gaps\n",
    "    if auditor.gaps:\n",
    "        print(\"üîß COMBLER LES GAPS:\")\n",
    "        for gap in auditor.gaps:\n",
    "            if gap[\"severity\"] == \"CRITICAL\":\n",
    "                rec = f\"URGENT: D√©velopper le concept {gap['concept']} manquant\"\n",
    "                recommendations.append({\"priority\": \"HIGH\", \"action\": rec})\n",
    "                print(f\"   üö® {rec}\")\n",
    "            elif gap[\"severity\"] == \"HIGH\":\n",
    "                rec = f\"Enrichir {gap['concept']} avec: {', '.join(gap['missing'][:2])}\"\n",
    "                recommendations.append({\"priority\": \"MEDIUM\", \"action\": rec})\n",
    "                print(f\"   ‚ö†Ô∏è {rec}\")\n",
    "    \n",
    "    # Recommandations bas√©es sur les contradictions\n",
    "    if auditor.contradictions:\n",
    "        print(\"\\nüîó R√âSOUDRE CONTRADICTIONS:\")\n",
    "        for contradiction in auditor.contradictions:\n",
    "            if contradiction[\"severity\"] == \"HIGH\":\n",
    "                rec = f\"Aligner concepts dans: {contradiction['check']}\"\n",
    "                recommendations.append({\"priority\": \"HIGH\", \"action\": rec})\n",
    "                print(f\"   üö® {rec}\")\n",
    "    \n",
    "    # Recommandations d'am√©lioration\n",
    "    if report[\"global_score\"] < 85:\n",
    "        print(\"\\nüöÄ AM√âLIORATIONS G√âN√âRALES:\")\n",
    "        \n",
    "        if report[\"detailed_scores\"][\"coverage\"] < 80:\n",
    "            rec = \"Impl√©menter les concepts manquants de l'√©cosyst√®me\"\n",
    "            recommendations.append({\"priority\": \"MEDIUM\", \"action\": rec})\n",
    "            print(f\"   üìà {rec}\")\n",
    "        \n",
    "        if report[\"detailed_scores\"][\"coherence\"] < 80:\n",
    "            rec = \"Harmoniser les impl√©mentations conceptuelles entre repos\"\n",
    "            recommendations.append({\"priority\": \"MEDIUM\", \"action\": rec})\n",
    "            print(f\"   üîó {rec}\")\n",
    "        \n",
    "        if report[\"detailed_scores\"][\"distribution\"] < 60:\n",
    "            rec = \"R√©√©quilibrer la pr√©sence des concepts dans l'√©cosyst√®me\"\n",
    "            recommendations.append({\"priority\": \"LOW\", \"action\": rec})\n",
    "            print(f\"   üìä {rec}\")\n",
    "    \n",
    "    # Recommandations sp√©cifiques Panini\n",
    "    print(\"\\nüéØ RECOMMANDATIONS SP√âCIFIQUES PANINI:\")\n",
    "    \n",
    "    panini_presence = \"panini_grammar\" in auditor.concept_map\n",
    "    melcuk_presence = \"melcuk_theory\" in auditor.concept_map\n",
    "    \n",
    "    if panini_presence and melcuk_presence:\n",
    "        rec = \"Cr√©er pont conceptuel explicite Panini-Mel'ƒçuk\"\n",
    "        recommendations.append({\"priority\": \"HIGH\", \"action\": rec})\n",
    "        print(f\"   üåâ {rec}\")\n",
    "    \n",
    "    if \"filesystem\" in auditor.concept_map and \"compression\" in auditor.concept_map:\n",
    "        rec = \"Documenter algorithmes compression s√©mantique filesystem\"\n",
    "        recommendations.append({\"priority\": \"MEDIUM\", \"action\": rec})\n",
    "        print(f\"   üìö {rec}\")\n",
    "    \n",
    "    if not recommendations:\n",
    "        print(\"   üéâ Aucune recommandation - √âcosyst√®me conceptuellement excellent!\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# G√©n√©ration rapport et recommandations finales\n",
    "final_conceptual_report = generate_conceptual_report()\n",
    "recommendations = generate_recommendations(final_conceptual_report)\n",
    "\n",
    "# Sauvegarde rapport conceptuel\n",
    "conceptual_report_file = \"panini_conceptual_audit_report.json\"\n",
    "with open(conceptual_report_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        **final_conceptual_report,\n",
    "        \"recommendations\": recommendations\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nüìÑ Rapport conceptuel sauvegard√©: {conceptual_report_file}\")\n",
    "print(\"\\nüèÅ AUDIT CONCEPTUEL TERMIN√â!\")\n",
    "\n",
    "# R√©sum√© ex√©cutif\n",
    "print(f\"\\nüìä R√âSUM√â EX√âCUTIF:\")\n",
    "print(f\"   üéØ Score global: {final_conceptual_report['global_score']}/100\")\n",
    "print(f\"   üè• Sant√©: {final_conceptual_report['health_status']}\")\n",
    "print(f\"   ‚ö†Ô∏è Contradictions: {len(auditor.contradictions)}\")\n",
    "print(f\"   üï≥Ô∏è Gaps: {len(auditor.gaps)}\")\n",
    "print(f\"   üí° Recommandations: {len(recommendations)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
