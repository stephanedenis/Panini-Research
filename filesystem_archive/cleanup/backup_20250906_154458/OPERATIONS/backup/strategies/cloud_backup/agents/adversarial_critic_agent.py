#!/usr/bin/env python3
"""
üî• AGENT CRITIQUE ADVERSE AUTONOME
===============================

Agent sp√©cialis√© dans la critique syst√©matique et constructive de :
- Tous les aspects du projet PaniniFS et sous-projets
- Fondements th√©oriques et impl√©mentations
- Coh√©rence conceptuelle et technique
- Viabilit√© commerciale et scientifique

Approche adversaire pour am√©lioration continue autonome.
Fonctionnement 100% externalis√© et autonome.
"""

import json
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
import subprocess
import re
from pathlib import Path

@dataclass
class CriticalFinding:
    """Structure d'une critique identifi√©e"""
    category: str       # "theoretical", "technical", "conceptual", "commercial"
    severity: str       # "CRITICAL", "HIGH", "MEDIUM", "LOW"
    component: str      # Composant critiqu√©
    issue: str          # Description du probl√®me
    evidence: List[str] # Preuves/exemples
    impact: str         # Impact potentiel
    counter_argument: str # Argument adverse
    improvement_suggestion: str # Suggestion d'am√©lioration
    confidence: float   # Confiance en la critique (0-1)

@dataclass
class AdvocateResponse:
    """R√©ponse d√©fensive aux critiques"""
    original_critique: str
    defense_arguments: List[str]
    evidence_supporting: List[str]
    counter_critique: str
    strength_score: float

class AdversarialCriticAgent:
    """Agent critique adverse pour am√©lioration continue"""
    
    def __init__(self):
        self.session_id = f"critic_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.critique_categories = {
            'theoretical_foundations': {
                'focus': 'Solidit√© th√©orique, coh√©rence conceptuelle',
                'severity_bias': 'HIGH',  # Tr√®s strict sur th√©orie
                'questions': [
                    'Les fondements th√©oriques sont-ils solides?',
                    'Y a-t-il des contradictions conceptuelles?',
                    'La grammaire Panini est-elle vraiment applicable?',
                    'La th√©orie Mel\'ƒçuk est-elle correctement impl√©ment√©e?'
                ]
            },
            'technical_implementation': {
                'focus': 'Qualit√© code, architecture, performance',
                'severity_bias': 'MEDIUM',
                'questions': [
                    'L\'architecture est-elle scalable?',
                    'Le code est-il maintenir?',
                    'Les performances sont-elles acceptables?',
                    'La s√©curit√© est-elle adequate?'
                ]
            },
            'scientific_validity': {
                'focus': 'Rigueur scientifique, m√©thodologie',
                'severity_bias': 'HIGH',
                'questions': [
                    'La m√©thodologie est-elle rigoureuse?',
                    'Les claims sont-ils support√©s par des preuves?',
                    'Y a-t-il des biais dans l\'approche?',
                    'Les r√©sultats sont-ils reproductibles?'
                ]
            },
            'commercial_viability': {
                'focus': 'Viabilit√© march√©, adoption, concurrence',
                'severity_bias': 'MEDIUM',
                'questions': [
                    'Y a-t-il un march√© r√©el?',
                    'Quels sont les concurrents?',
                    'L\'adoption est-elle r√©aliste?',
                    'Le mod√®le √©conomique est-il viable?'
                ]
            },
            'usability_practicality': {
                'focus': 'Utilisabilit√©, praticit√©, adoption',
                'severity_bias': 'LOW',
                'questions': [
                    'Est-ce que c\'est vraiment utilisable?',
                    'L\'interface est-elle intuitive?',
                    'Les b√©n√©fices justifient-ils la complexit√©?',
                    'Y a-t-il des cas d\'usage concrets?'
                ]
            }
        }
        
        self.critical_findings = []
        self.advocate_responses = []
        self.project_components = []
        self.criticism_log = []
        
    def autonomous_criticism_cycle(self):
        """Cycle complet de critique adverse autonome"""
        print(f"üî• D√âMARRAGE CRITIQUE ADVERSE AUTONOME - Session {self.session_id}")
        
        # Phase 1: Scan complet du projet
        self._scan_project_components()
        
        # Phase 2: Critique par cat√©gorie
        for category, config in self.critique_categories.items():
            print(f"\nüí£ Critique {category}")
            self._criticize_category(category, config)
            
        # Phase 3: Jeu d'avocat du diable
        self._play_devils_advocate()
        
        # Phase 4: G√©n√©ration contre-arguments (d√©fense)
        self._generate_defensive_responses()
        
        # Phase 5: Rapport critique final
        self._generate_criticism_report()
        
        print(f"\n‚úÖ CRITIQUE ADVERSE TERMIN√âE - {len(self.critical_findings)} critiques identifi√©es")
        
    def _scan_project_components(self):
        """Scan de tous les composants du projet"""
        print("üîç Scan des composants du projet...")
        
        base_path = "/home/stephane/GitHub/PaniniFS-1"
        
        # Composants √† analyser
        components = [
            {'type': 'code', 'path': 'src/', 'description': 'Code source principal'},
            {'type': 'docs', 'path': 'docs/', 'description': 'Documentation'},
            {'type': 'config', 'path': '*.toml', 'description': 'Configuration'},
            {'type': 'scripts', 'path': 'Copilotage/', 'description': 'Scripts et agents'},
            {'type': 'publications', 'path': '*.md', 'description': 'Publications et README'},
            {'type': 'reports', 'path': '*report*.json', 'description': 'Rapports d\'audit'}
        ]
        
        for component in components:
            component_data = self._analyze_component(base_path, component)
            if component_data:
                self.project_components.append(component_data)
                
    def _analyze_component(self, base_path: str, component: Dict) -> Optional[Dict]:
        """Analyse d'un composant sp√©cifique"""
        try:
            full_path = os.path.join(base_path, component['path'])
            
            if component['type'] == 'code':
                return self._analyze_code_component(full_path, component)
            elif component['type'] == 'docs':
                return self._analyze_docs_component(full_path, component)
            elif component['type'] == 'config':
                return self._analyze_config_component(base_path, component)
            elif component['type'] == 'scripts':
                return self._analyze_scripts_component(full_path, component)
            elif component['type'] == 'publications':
                return self._analyze_publications_component(base_path, component)
            elif component['type'] == 'reports':
                return self._analyze_reports_component(base_path, component)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur analyse {component['type']}: {e}")
            
        return None
        
    def _analyze_code_component(self, path: str, component: Dict) -> Dict:
        """Analyse composant code"""
        if not os.path.exists(path):
            return None
            
        code_stats = {
            'component_type': 'code',
            'path': path,
            'description': component['description'],
            'file_count': 0,
            'line_count': 0,
            'rust_files': [],
            'complexity_issues': [],
            'quality_issues': []
        }
        
        # Scan fichiers Rust
        for root, dirs, files in os.walk(path):
            for file in files:
                if file.endswith('.rs'):
                    file_path = os.path.join(root, file)
                    code_stats['rust_files'].append(file_path)
                    code_stats['file_count'] += 1
                    
                    # Analyse basique du fichier
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            lines = content.split('\n')
                            code_stats['line_count'] += len(lines)
                            
                            # D√©tection probl√®mes qualit√©
                            self._detect_code_quality_issues(file_path, content, code_stats)
                            
                    except Exception as e:
                        code_stats['quality_issues'].append(f"Erreur lecture {file_path}: {e}")
                        
        return code_stats
        
    def _detect_code_quality_issues(self, file_path: str, content: str, stats: Dict):
        """D√©tecte probl√®mes qualit√© dans le code"""
        lines = content.split('\n')
        
        # D√©tection issues communes
        for i, line in enumerate(lines, 1):
            # Lignes trop longues
            if len(line) > 120:
                stats['quality_issues'].append(f"{file_path}:{i} - Ligne trop longue ({len(line)} chars)")
                
            # TODO/FIXME/HACK comments
            if any(keyword in line.upper() for keyword in ['TODO', 'FIXME', 'HACK', 'XXX']):
                stats['quality_issues'].append(f"{file_path}:{i} - Code non termin√©: {line.strip()}")
                
            # Unwrap() usage (dangerous in Rust)
            if '.unwrap()' in line:
                stats['quality_issues'].append(f"{file_path}:{i} - Usage dangereux de unwrap()")
                
            # Hardcoded paths
            if '/home/' in line or 'C:\\' in line:
                stats['quality_issues'].append(f"{file_path}:{i} - Chemin hardcod√© d√©tect√©")
                
        # Complexit√© fonction (approximative)
        function_complexity = content.count('fn ') + content.count('if ') + content.count('match ') + content.count('for ')
        if function_complexity > 50:
            stats['complexity_issues'].append(f"Complexit√© √©lev√©e d√©tect√©e: {function_complexity} constructs")
            
    def _analyze_docs_component(self, path: str, component: Dict) -> Dict:
        """Analyse composant documentation"""
        if not os.path.exists(path):
            return None
            
        docs_stats = {
            'component_type': 'docs',
            'path': path,
            'description': component['description'],
            'file_count': 0,
            'total_size': 0,
            'doc_files': [],
            'completeness_issues': [],
            'quality_issues': []
        }
        
        # Scan fichiers documentation
        for root, dirs, files in os.walk(path):
            for file in files:
                if file.endswith(('.md', '.html', '.txt')):
                    file_path = os.path.join(root, file)
                    docs_stats['doc_files'].append(file_path)
                    docs_stats['file_count'] += 1
                    
                    try:
                        size = os.path.getsize(file_path)
                        docs_stats['total_size'] += size
                        
                        # Analyse contenu
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            self._detect_docs_quality_issues(file_path, content, docs_stats)
                            
                    except Exception as e:
                        docs_stats['quality_issues'].append(f"Erreur lecture {file_path}: {e}")
                        
        return docs_stats
        
    def _detect_docs_quality_issues(self, file_path: str, content: str, stats: Dict):
        """D√©tecte probl√®mes qualit√© dans la documentation"""
        # Documentation trop courte
        if len(content) < 500:
            stats['completeness_issues'].append(f"{file_path} - Documentation tr√®s courte ({len(content)} chars)")
            
        # Liens cass√©s (simple d√©tection)
        links = re.findall(r'\[.*?\]\((.*?)\)', content)
        for link in links:
            if link.startswith('http') and 'example.com' in link:
                stats['quality_issues'].append(f"{file_path} - Lien exemple d√©tect√©: {link}")
                
        # Absence √©l√©ments cl√©s
        if 'index.md' in file_path or 'README.md' in file_path:
            required_sections = ['installation', 'usage', 'example']
            for section in required_sections:
                if section.lower() not in content.lower():
                    stats['completeness_issues'].append(f"{file_path} - Section manquante: {section}")
                    
    def _analyze_config_component(self, base_path: str, component: Dict) -> Dict:
        """Analyse fichiers de configuration"""
        config_stats = {
            'component_type': 'config',
            'path': component['path'],
            'description': component['description'],
            'config_files': [],
            'security_issues': [],
            'completeness_issues': []
        }
        
        # Recherche fichiers .toml
        for file in os.listdir(base_path):
            if file.endswith('.toml'):
                file_path = os.path.join(base_path, file)
                config_stats['config_files'].append(file_path)
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        self._detect_config_issues(file_path, content, config_stats)
                        
                except Exception as e:
                    config_stats['security_issues'].append(f"Erreur lecture {file_path}: {e}")
                    
        return config_stats
        
    def _detect_config_issues(self, file_path: str, content: str, stats: Dict):
        """D√©tecte probl√®mes dans configuration"""
        # Secrets potentiels
        if any(keyword in content.lower() for keyword in ['password', 'secret', 'token', 'key']):
            stats['security_issues'].append(f"{file_path} - Secrets potentiels d√©tect√©s")
            
        # URLs hardcod√©es
        if 'http://' in content:
            stats['security_issues'].append(f"{file_path} - URLs non-s√©curis√©es (HTTP)")
            
        # Configurations de dev en prod
        if 'debug = true' in content or 'localhost' in content:
            stats['security_issues'].append(f"{file_path} - Configuration d√©veloppement d√©tect√©e")
            
    def _analyze_scripts_component(self, path: str, component: Dict) -> Dict:
        """Analyse scripts et agents"""
        if not os.path.exists(path):
            return None
            
        scripts_stats = {
            'component_type': 'scripts',
            'path': path,
            'description': component['description'],
            'script_count': 0,
            'agent_count': 0,
            'autonomy_issues': [],
            'reliability_issues': []
        }
        
        # Scan scripts
        for root, dirs, files in os.walk(path):
            for file in files:
                if file.endswith(('.py', '.sh', '.bash')):
                    file_path = os.path.join(root, file)
                    scripts_stats['script_count'] += 1
                    
                    if 'agent' in file.lower():
                        scripts_stats['agent_count'] += 1
                        
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            self._detect_script_issues(file_path, content, scripts_stats)
                            
                    except Exception as e:
                        scripts_stats['reliability_issues'].append(f"Erreur lecture {file_path}: {e}")
                        
        return scripts_stats
        
    def _detect_script_issues(self, file_path: str, content: str, stats: Dict):
        """D√©tecte probl√®mes dans scripts"""
        # Gestion d'erreur insuffisante
        if content.count('try:') < content.count('except:') / 2:
            stats['reliability_issues'].append(f"{file_path} - Gestion d'erreur insuffisante")
            
        # Hardcoded paths
        if '/home/' in content or 'C:\\' in content:
            stats['autonomy_issues'].append(f"{file_path} - Chemins absolus hardcod√©s")
            
        # D√©pendances externes non g√©r√©es
        if 'import ' in content and 'pip install' not in content and 'requirements' not in content:
            stats['autonomy_issues'].append(f"{file_path} - D√©pendances non document√©es")
            
    def _analyze_publications_component(self, base_path: str, component: Dict) -> Dict:
        """Analyse publications et README"""
        pub_stats = {
            'component_type': 'publications',
            'path': component['path'],
            'description': component['description'],
            'publications': [],
            'readability_issues': [],
            'credibility_issues': []
        }
        
        # Recherche fichiers Markdown
        for file in os.listdir(base_path):
            if file.endswith('.md'):
                file_path = os.path.join(base_path, file)
                pub_stats['publications'].append(file_path)
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        self._detect_publication_issues(file_path, content, pub_stats)
                        
                except Exception as e:
                    pub_stats['readability_issues'].append(f"Erreur lecture {file_path}: {e}")
                    
        return pub_stats
        
    def _detect_publication_issues(self, file_path: str, content: str, stats: Dict):
        """D√©tecte probl√®mes dans publications"""
        # Claims non support√©s
        superlatives = ['r√©volutionnaire', 'meilleur', 'unique', 'premier', 'jamais vu']
        for superlative in superlatives:
            if superlative in content.lower():
                stats['credibility_issues'].append(f"{file_path} - Claim potentiellement excessif: {superlative}")
                
        # Absence citations/r√©f√©rences
        if len(content) > 2000 and content.count('[') < 3:
            stats['credibility_issues'].append(f"{file_path} - Manque de r√©f√©rences/citations")
            
        # Jargon technique non expliqu√©
        technical_terms = ['s√©mantique', 'Panini', 'Mel\'ƒçuk', 'compression']
        for term in technical_terms:
            if term in content and f"({term}" not in content and f"{term}:" not in content:
                stats['readability_issues'].append(f"{file_path} - Terme technique non d√©fini: {term}")
                
    def _analyze_reports_component(self, base_path: str, component: Dict) -> Dict:
        """Analyse rapports d'audit"""
        reports_stats = {
            'component_type': 'reports',
            'path': component['path'],
            'description': component['description'],
            'reports': [],
            'audit_issues': [],
            'consistency_issues': []
        }
        
        # Recherche rapports JSON
        for file in os.listdir(base_path):
            if 'report' in file and file.endswith('.json'):
                file_path = os.path.join(base_path, file)
                reports_stats['reports'].append(file_path)
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        report_data = json.load(f)
                        self._detect_report_issues(file_path, report_data, reports_stats)
                        
                except Exception as e:
                    reports_stats['audit_issues'].append(f"Erreur lecture {file_path}: {e}")
                    
        return reports_stats
        
    def _detect_report_issues(self, file_path: str, report_data: Dict, stats: Dict):
        """D√©tecte probl√®mes dans rapports"""
        # Scores trop optimistes
        if 'global_score' in report_data and report_data['global_score'] > 90:
            stats['audit_issues'].append(f"{file_path} - Score suspicieusement √©lev√©: {report_data['global_score']}")
            
        # Contradictions entre rapports
        if 'health_status' in report_data and report_data['health_status'] == 'CRITICAL':
            if 'global_score' in report_data and report_data['global_score'] > 50:
                stats['consistency_issues'].append(f"{file_path} - Contradiction score/status")
                
    def _criticize_category(self, category: str, config: Dict):
        """Critique une cat√©gorie sp√©cifique"""
        print(f"  üéØ Focus: {config['focus']}")
        
        if category == 'theoretical_foundations':
            self._criticize_theoretical_foundations(config)
        elif category == 'technical_implementation':
            self._criticize_technical_implementation(config)
        elif category == 'scientific_validity':
            self._criticize_scientific_validity(config)
        elif category == 'commercial_viability':
            self._criticize_commercial_viability(config)
        elif category == 'usability_practicality':
            self._criticize_usability_practicality(config)
            
    def _criticize_theoretical_foundations(self, config: Dict):
        """Critique sp√©cifique des fondements th√©oriques"""
        # Lecture rapport conceptuel pour base de critique
        try:
            with open('/home/stephane/GitHub/PaniniFS-1/panini_conceptual_audit_report.json', 'r') as f:
                conceptual_data = json.load(f)
                
            # Critique bas√©e sur donn√©es audit
            if conceptual_data.get('global_score', 0) < 50:
                self._add_critical_finding(
                    category='theoretical_foundations',
                    severity='CRITICAL',
                    component='Fondements conceptuels',
                    issue='Score conceptuel critique (47.5/100) r√©v√®le fondements th√©oriques d√©faillants',
                    evidence=[
                        f"Gap Panini-Mel'ƒçuk: pont th√©orique manquant",
                        f"Panini grammar: seulement 5 occurrences vs 52,443 filesystem",
                        f"Compression s√©mantique sans base th√©orique solide"
                    ],
                    impact='Cr√©dibilit√© scientifique compromise, base th√©orique instable',
                    counter_argument='Le nom PaniniFS implique ma√Ætrise grammaire Panini mais impl√©mentation minimal',
                    improvement_suggestion='D√©velopper documentation th√©orique compl√®te avant claims techniques',
                    confidence=0.95
                )
                
        except FileNotFoundError:
            self._add_critical_finding(
                category='theoretical_foundations',
                severity='HIGH',
                component='Audit conceptuel',
                issue='Absence de rapport conceptuel pour √©valuation th√©orique',
                evidence=['Fichier panini_conceptual_audit_report.json non trouv√©'],
                impact='Impossible d\'√©valuer solidit√© th√©orique',
                counter_argument='Audit conceptuel peut √™tre en cours',
                improvement_suggestion='Compl√©ter audit conceptuel avant √©valuation',
                confidence=0.8
            )
            
        # Critique g√©n√©rale grammaire Panini
        self._add_critical_finding(
            category='theoretical_foundations',
            severity='HIGH',
            component='Grammaire Panini',
            issue='Application grammaire Sanskrit du 4√®me si√®cle √† filesystem moderne questionnable',
            evidence=[
                'Grammaire Panini: contexte historique/linguistique sp√©cifique',
                'Adaptation computationnelle non d√©montr√©e',
                'Gap temporel 2400+ ans entre th√©orie et application'
            ],
            impact='Fondement th√©orique potentiellement inad√©quat',
            counter_argument='Principes grammaticaux peuvent √™tre intemporels',
            improvement_suggestion='D√©montrer applicabilit√© via prototypes fonctionnels',
            confidence=0.7
        )
        
    def _criticize_technical_implementation(self, config: Dict):
        """Critique impl√©mentation technique"""
        # Analyse composants code
        code_components = [c for c in self.project_components if c and c.get('component_type') == 'code']
        
        for component in code_components:
            if component.get('quality_issues'):
                self._add_critical_finding(
                    category='technical_implementation',
                    severity='MEDIUM',
                    component=f"Code quality - {component['path']}",
                    issue=f"{len(component['quality_issues'])} probl√®mes qualit√© d√©tect√©s",
                    evidence=component['quality_issues'][:3],  # Top 3
                    impact='Maintenabilit√© et fiabilit√© compromises',
                    counter_argument='Probl√®mes qualit√© normaux en d√©veloppement',
                    improvement_suggestion='Impl√©menter linting automatique et code review',
                    confidence=0.6
                )
                
            if component.get('complexity_issues'):
                self._add_critical_finding(
                    category='technical_implementation',
                    severity='MEDIUM',
                    component=f"Code complexity - {component['path']}",
                    issue='Complexit√© √©lev√©e d√©tect√©e dans certains modules',
                    evidence=component['complexity_issues'],
                    impact='Difficult√© maintenance et debug',
                    counter_argument='Complexit√© peut √™tre justifi√©e par domaine',
                    improvement_suggestion='Refactoring et modularisation',
                    confidence=0.5
                )
                
        # Critique architecture g√©n√©rale
        self._add_critical_finding(
            category='technical_implementation',
            severity='HIGH',
            component='Architecture syst√®me',
            issue='Filesystem s√©mantique: complexit√© vs b√©n√©fices non d√©montr√©s',
            evidence=[
                'Overhead s√©mantique sur op√©rations basiques filesystem',
                'Compression s√©mantique: ratio compression vs performance?',
                'Compatibilit√© avec outils filesystem existants?'
            ],
            impact='Adoption limit√©e par complexit√© injustifi√©e',
            counter_argument='Innovation n√©cessite complexit√© initiale',
            improvement_suggestion='Benchmarks performance vs filesystem traditionnels',
            confidence=0.8
        )
        
    def _criticize_scientific_validity(self, config: Dict):
        """Critique validit√© scientifique"""
        pub_components = [c for c in self.project_components if c and c.get('component_type') == 'publications']
        
        for component in pub_components:
            if component.get('credibility_issues'):
                self._add_critical_finding(
                    category='scientific_validity',
                    severity='HIGH',
                    component='Publications scientifiques',
                    issue='Claims non support√©s et manque de rigueur scientifique',
                    evidence=component['credibility_issues'][:3],
                    impact='Cr√©dibilit√© acad√©mique compromise',
                    counter_argument='Phase exploratoire permet hypoth√®ses audacieuses',
                    improvement_suggestion='Peer review et validation empirique',
                    confidence=0.85
                )
                
        # Critique m√©thodologique g√©n√©rale
        self._add_critical_finding(
            category='scientific_validity',
            severity='CRITICAL',
            component='M√©thodologie recherche',
            issue='Absence de validation exp√©rimentale et peer review',
            evidence=[
                'Aucun benchmark publi√© vs solutions existantes',
                'Pas de validation par communaut√© scientifique',
                'Claims r√©volutionnaires sans preuves empiriques'
            ],
            impact='Projet non cr√©dible scientifiquement',
            counter_argument='Innovation radicale difficile √† valider initialement',
            improvement_suggestion='Protocole exp√©rimental rigoureux et publications peer-reviewed',
            confidence=0.9
        )
        
    def _criticize_commercial_viability(self, config: Dict):
        """Critique viabilit√© commerciale"""
        self._add_critical_finding(
            category='commercial_viability',
            severity='HIGH',
            component='March√© cible',
            issue='March√© filesystem s√©mantique inexistant ou trop niche',
            evidence=[
                'Utilisateurs satisfaits filesystems actuels',
                'Courbe apprentissage √©lev√©e pour concepts s√©mantiques',
                'ROI unclear pour enterprises'
            ],
            impact='Adoption commerciale improbable',
            counter_argument='Innovation cr√©e nouveaux march√©s',
            improvement_suggestion='√âtude march√© et cas d\\'usage concrets',
            confidence=0.7
        )
        
        self._add_critical_finding(
            category='commercial_viability',
            severity='MEDIUM',
            component='Concurrence',
            issue='Concurrence Google/Microsoft/Apple sur filesystems',
            evidence=[
                'Resources R&D limit√©es vs GAFAM',
                'Standards filesystem √©tablis (NTFS, ext4, APFS)',
                'Network effects des solutions existantes'
            ],
            impact='P√©n√©tration march√© tr√®s difficile',
            counter_argument='Niche sp√©cialis√©e peut √™tre viable',
            improvement_suggestion='Positionnement diff√©renci√© et partenariats',
            confidence=0.6
        )
        
    def _criticize_usability_practicality(self, config: Dict):
        """Critique utilisabilit√© et praticit√©"""
        self._add_critical_finding(
            category='usability_practicality',
            severity='MEDIUM',
            component='Interface utilisateur',
            issue='Complexit√© conceptuelle barri√®re adoption mainstream',
            evidence=[
                'Concepts linguistiques avanc√©s requis',
                'Grammaire Panini inconnue utilisateurs moyens',
                'Abstraction s√©mantique vs simplicit√© filesystem'
            ],
            impact='Limitation adoption grand public',
            counter_argument='Interface peut masquer complexit√©',
            improvement_suggestion='UX design focused sur simplicit√©',
            confidence=0.65
        )
        
    def _add_critical_finding(self, category: str, severity: str, component: str, 
                            issue: str, evidence: List[str], impact: str, 
                            counter_argument: str, improvement_suggestion: str, confidence: float):
        """Ajoute une critique √† la liste"""
        finding = CriticalFinding(
            category=category,
            severity=severity,
            component=component,
            issue=issue,
            evidence=evidence,
            impact=impact,
            counter_argument=counter_argument,
            improvement_suggestion=improvement_suggestion,
            confidence=confidence
        )
        
        self.critical_findings.append(finding)
        
        # Log pour tra√ßabilit√©
        self.criticism_log.append({
            'timestamp': datetime.now().isoformat(),
            'category': category,
            'severity': severity,
            'issue': issue
        })
        
    def _play_devils_advocate(self):
        """Jeu d'avocat du diable - critique maximale"""
        print("\nüòà Mode Avocat du Diable - Critique maximale")
        
        # Critique existentielle du projet
        self._add_critical_finding(
            category='existential',
            severity='CRITICAL',
            component='Raison d\\'√™tre projet',
            issue='PaniniFS: solution en recherche de probl√®me?',
            evidence=[
                'Filesystems actuels fonctionnent bien',
                'Compression g√©n√©rique d√©j√† optimis√©e',
                'S√©mantique: b√©n√©fices unclear pour stockage'
            ],
            impact='Projet potentiellement inutile',
            counter_argument='Innovation disruptive toujours questionn√©e initialement',
            improvement_suggestion='D√©montrer probl√®me concret r√©solu par PaniniFS',
            confidence=0.6
        )
        
        # Critique du fondateur/√©quipe
        self._add_critical_finding(
            category='team_capability',
            severity='HIGH',
            component='Expertise √©quipe',
            issue='Linguistique 30 ans + informatique: expertise suffisante?',
            evidence=[
                'Domaine hyper-sp√©cialis√© n√©cessitant expertise pointe',
                '√âquipe r√©duite vs complexit√© projet',
                'Manque expertise filesystem industriel'
            ],
            impact='Capacit√© ex√©cution questionn√©e',
            counter_argument='Passion et vision compensent ressources',
            improvement_suggestion='Recrutement experts compl√©mentaires',
            confidence=0.5
        )
        
        # Critique timing
        self._add_critical_finding(
            category='market_timing',
            severity='MEDIUM',
            component='Timing march√©',
            issue='Trop t√¥t ou trop tard pour filesystem s√©mantique?',
            evidence=[
                'IA g√©n√©rativedomine: attention ailleurs',
                'Blockchain/Web3 ont monopolis√© innovation storage',
                'Cloud storage: paradigme diff√©rent'
            ],
            impact='Momentum march√© absent',
            counter_argument='Contrarian timing peut √™tre optimal',
            improvement_suggestion='Positionnement vs tendances actuelles',
            confidence=0.4
        )
        
    def _generate_defensive_responses(self):
        """G√©n√®re r√©ponses d√©fensives aux critiques"""
        print("\nüõ°Ô∏è G√©n√©ration r√©ponses d√©fensives...")
        
        for finding in self.critical_findings:
            defense = self._create_defense_response(finding)
            self.advocate_responses.append(defense)
            
    def _create_defense_response(self, critique: CriticalFinding) -> AdvocateResponse:
        """Cr√©e r√©ponse d√©fensive pour une critique"""
        
        # Arguments d√©fensifs g√©n√©riques par cat√©gorie
        defense_templates = {
            'theoretical_foundations': [
                'Innovation requiert exploration fondements non-conventionnels',
                'Grammaire Panini: principes structurels universels',
                'Th√©orie Mel\'ƒçuk: validation acad√©mique √©tablie'
            ],
            'technical_implementation': [
                'Prototype early-stage: qualit√© s\'am√©liore it√©rativement', 
                'Complexit√© justifi√©e par innovation breakthrough',
                'Performance: optimisation vient apr√®s proof-of-concept'
            ],
            'scientific_validity': [
                'Recherche exploratoire pr√©c√®de validation formelle',
                'Peer review process en cours initiation',
                'Innovation radicale difficile √† valider standards existants'
            ],
            'commercial_viability': [
                'March√©s disruptifs inexistants avant cr√©ation',
                'First-mover advantage dans nouveau paradigme',
                'Niche sp√©cialis√©e peut √™tre tr√®s profitable'
            ],
            'usability_practicality': [
                'Interface utilisateur peut masquer complexit√© interne',
                'Adoption progressive via early adopters',
                'Education utilisateurs partie strat√©gie'
            ]
        }
        
        category_defenses = defense_templates.get(critique.category, ['Innovation toujours questionn√©e initialement'])
        
        # Evidence supportive (contre-exemples)
        supporting_evidence = [
            'Bitcoin: critiqu√© initialement, r√©volutionna finance',
            'Internet: qualifi√© de gadget par experts',
            'Filesystem: UNIX r√©volutionnaire en son temps'
        ]
        
        # Contre-critique
        counter_critique = f"Critique '{critique.issue}' refl√®te biais status quo et resistance innovation"
        
        # Score force d√©fense
        strength = min(critique.confidence * 0.8, 0.9)  # D√©fense l√©g√®rement plus faible que critique
        
        return AdvocateResponse(
            original_critique=critique.issue,
            defense_arguments=category_defenses,
            evidence_supporting=supporting_evidence,
            counter_critique=counter_critique,
            strength_score=strength
        )
        
    def _generate_criticism_report(self):
        """G√©n√®re rapport critique complet"""
        print("\nüìä G√©n√©ration rapport critique...")
        
        # Classification critiques par s√©v√©rit√©
        critical_count = len([f for f in self.critical_findings if f.severity == 'CRITICAL'])
        high_count = len([f for f in self.critical_findings if f.severity == 'HIGH'])
        medium_count = len([f for f in self.critical_findings if f.severity == 'MEDIUM'])
        low_count = len([f for f in self.critical_findings if f.severity == 'LOW'])
        
        # Score critique global (inverse du score qualit√©)
        criticism_score = (critical_count * 4 + high_count * 3 + medium_count * 2 + low_count * 1) / len(self.critical_findings) if self.critical_findings else 0
        
        report = {
            'session_id': self.session_id,
            'timestamp': datetime.now().isoformat(),
            'criticism_summary': {
                'total_critiques': len(self.critical_findings),
                'critical_issues': critical_count,
                'high_issues': high_count,
                'medium_issues': medium_count,
                'low_issues': low_count,
                'criticism_score': criticism_score,
                'criticism_level': self._get_criticism_level(criticism_score)
            },
            'component_analysis': self._summarize_component_analysis(),
            'critical_findings': [asdict(f) for f in self.critical_findings],
            'defensive_responses': [asdict(r) for r in self.advocate_responses],
            'improvement_roadmap': self._generate_improvement_roadmap(),
            'risk_assessment': self._assess_project_risks(),
            'recommendations': self._generate_critic_recommendations()
        }
        
        # Sauvegarde rapport
        report_path = f"/home/stephane/GitHub/PaniniFS-1/adversarial_criticism_report_{self.session_id}.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        # Rapport humain-lisible
        self._generate_human_readable_criticism_report(report)
        
        print(f"‚úÖ Rapport critique sauvegard√©: {report_path}")
        
    def _get_criticism_level(self, score: float) -> str:
        """D√©termine niveau critique global"""
        if score >= 3.5:
            return 'DEVASTATING'
        elif score >= 2.5:
            return 'SEVERE'
        elif score >= 1.5:
            return 'MODERATE'
        else:
            return 'LIGHT'
            
    def _summarize_component_analysis(self) -> Dict:
        """R√©sume analyse des composants"""
        summary = {}
        
        for component in self.project_components:
            if not component:
                continue
                
            comp_type = component.get('component_type', 'unknown')
            
            if comp_type not in summary:
                summary[comp_type] = {
                    'components_analyzed': 0,
                    'total_issues': 0,
                    'critical_issues': []
                }
                
            summary[comp_type]['components_analyzed'] += 1
            
            # Comptage issues par composant
            for issue_type in ['quality_issues', 'security_issues', 'reliability_issues', 
                             'credibility_issues', 'audit_issues']:
                if issue_type in component:
                    issues = component[issue_type]
                    summary[comp_type]['total_issues'] += len(issues)
                    if len(issues) > 3:  # Beaucoup d'issues = critique
                        summary[comp_type]['critical_issues'].extend(issues[:2])  # Top 2
                        
        return summary
        
    def _generate_improvement_roadmap(self) -> List[Dict]:
        """G√©n√®re roadmap d'am√©lioration bas√©e sur critiques"""
        roadmap = []
        
        # Groupement critiques par priorit√©
        critical_findings = [f for f in self.critical_findings if f.severity == 'CRITICAL']
        high_findings = [f for f in self.critical_findings if f.severity == 'HIGH']
        
        # Phase 1: Critiques CRITICAL
        if critical_findings:
            phase1_actions = []
            for finding in critical_findings:
                phase1_actions.append({
                    'action': finding.improvement_suggestion,
                    'component': finding.component,
                    'urgency': 'IMMEDIATE'
                })
                
            roadmap.append({
                'phase': 'IMMEDIATE_FIXES',
                'timeline': '1-2 weeks',
                'priority': 'CRITICAL',
                'actions': phase1_actions,
                'success_criteria': 'Elimination issues critiques'
            })
            
        # Phase 2: Critiques HIGH
        if high_findings:
            phase2_actions = []
            for finding in high_findings:
                phase2_actions.append({
                    'action': finding.improvement_suggestion,
                    'component': finding.component,
                    'urgency': 'HIGH'
                })
                
            roadmap.append({
                'phase': 'MAJOR_IMPROVEMENTS',
                'timeline': '1-2 months',
                'priority': 'HIGH',
                'actions': phase2_actions,
                'success_criteria': 'R√©duction significative issues majeures'
            })
            
        # Phase 3: Consolidation
        roadmap.append({
            'phase': 'CONSOLIDATION',
            'timeline': '3-6 months',
            'priority': 'MEDIUM',
            'actions': [
                {'action': 'Monitoring continu qualit√©', 'component': 'Global', 'urgency': 'ONGOING'},
                {'action': 'Feedback loops avec critiques', 'component': 'Process', 'urgency': 'ONGOING'},
                {'action': 'Validation externe cr√©dibilit√©', 'component': 'Scientifique', 'urgency': 'MEDIUM'}
            ],
            'success_criteria': 'Am√©lioration continue √©tablie'
        })
        
        return roadmap
        
    def _assess_project_risks(self) -> Dict:
        """√âvalue risques projet bas√©s sur critiques"""
        risks = {
            'technical_risks': [],
            'scientific_risks': [],
            'commercial_risks': [],
            'existential_risks': [],
            'overall_risk_level': 'UNKNOWN'
        }
        
        # Classification risques par cat√©gorie critique
        for finding in self.critical_findings:
            risk_item = {
                'risk': finding.issue,
                'probability': 'HIGH' if finding.confidence > 0.7 else 'MEDIUM',
                'impact': finding.impact,
                'mitigation': finding.improvement_suggestion
            }
            
            if finding.category in ['technical_implementation']:
                risks['technical_risks'].append(risk_item)
            elif finding.category in ['theoretical_foundations', 'scientific_validity']:
                risks['scientific_risks'].append(risk_item)
            elif finding.category in ['commercial_viability', 'usability_practicality']:
                risks['commercial_risks'].append(risk_item)
            elif finding.category in ['existential', 'team_capability']:
                risks['existential_risks'].append(risk_item)
                
        # Niveau risque global
        critical_count = len([f for f in self.critical_findings if f.severity == 'CRITICAL'])
        
        if critical_count >= 3:
            risks['overall_risk_level'] = 'EXTREME'
        elif critical_count >= 1:
            risks['overall_risk_level'] = 'HIGH'
        elif len([f for f in self.critical_findings if f.severity == 'HIGH']) >= 5:
            risks['overall_risk_level'] = 'MEDIUM'
        else:
            risks['overall_risk_level'] = 'LOW'
            
        return risks
        
    def _generate_critic_recommendations(self) -> List[Dict]:
        """G√©n√®re recommandations de l'agent critique"""
        recommendations = []
        
        # Rec 1: Validation externe
        recommendations.append({
            'priority': 'CRITICAL',
            'category': 'Validation',
            'recommendation': 'Obtenir validation externe impartiale',
            'rationale': 'Auto-√©valuation insuffisante pour cr√©dibilit√©',
            'action_steps': [
                'Soumettre travaux √† peer review',
                'Pr√©senter √† conf√©rences acad√©miques',
                'Inviter experts externes audit ind√©pendant'
            ],
            'timeline': '3-6 months',
            'success_metrics': 'Acceptance ou feedback constructif experts'
        })
        
        # Rec 2: Proof of concept
        recommendations.append({
            'priority': 'HIGH',
            'category': 'D√©monstration',
            'recommendation': 'D√©velopper proof-of-concept convaincant',
            'rationale': 'Claims th√©oriques doivent √™tre d√©montr√©s pratiquement',
            'action_steps': [
                'Prototype fonctionnel filesystem s√©mantique',
                'Benchmarks performance vs solutions existantes',
                'Cas d\\'usage concrets avec m√©triques'
            ],
            'timeline': '2-4 months',
            'success_metrics': 'Performance mesurable et reproductible'
        })
        
        # Rec 3: Humilit√© scientifique
        recommendations.append({
            'priority': 'MEDIUM',
            'category': 'Communication',
            'recommendation': 'Adopter communication plus humble et nuanc√©e',
            'rationale': 'Claims excessifs nuisent cr√©dibilit√©',
            'action_steps': [
                'Remplacer superlatifs par formulations nuanc√©es',
                'Acknowledge limitations et incertitudes',
                'Pr√©senter comme recherche exploratoire vs solution finale'
            ],
            'timeline': '1 month',
            'success_metrics': 'R√©ception am√©lior√©e par audience technique'
        })
        
        return recommendations
        
    def _generate_human_readable_criticism_report(self, report: Dict):
        """G√©n√®re rapport critique lisible"""
        readable_path = f"/home/stephane/GitHub/PaniniFS-1/adversarial_criticism_summary_{self.session_id}.md"
        
        with open(readable_path, 'w', encoding='utf-8') as f:
            f.write(f"# üî• RAPPORT CRITIQUE ADVERSE - {self.session_id}\n\n")
            f.write(f"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Score critique
            criticism_summary = report['criticism_summary']
            f.write("## üéØ √âVALUATION CRITIQUE GLOBALE\n\n")
            f.write(f"- **Niveau critique**: {criticism_summary['criticism_level']}\n")
            f.write(f"- **Score critique**: {criticism_summary['criticism_score']:.1f}/4.0\n")
            f.write(f"- **Total critiques**: {criticism_summary['total_critiques']}\n")
            f.write(f"  - üö® Critiques: {criticism_summary['critical_issues']}\n")
            f.write(f"  - ‚ö†Ô∏è √âlev√©es: {criticism_summary['high_issues']}\n")
            f.write(f"  - üìã Moyennes: {criticism_summary['medium_issues']}\n")
            f.write(f"  - üí° Faibles: {criticism_summary['low_issues']}\n\n")
            
            # Top critiques
            f.write("## üö® CRITIQUES PRIORITAIRES\n\n")
            critical_findings = [f for f in self.critical_findings if f.severity in ['CRITICAL', 'HIGH']]
            
            for i, finding in enumerate(critical_findings[:5], 1):
                f.write(f"### {i}. {finding.component} ({finding.severity})\n")
                f.write(f"**Probl√®me**: {finding.issue}\n\n")
                f.write(f"**Preuves**:\n")
                for evidence in finding.evidence[:3]:
                    f.write(f"- {evidence}\n")
                f.write(f"\n**Impact**: {finding.impact}\n")
                f.write(f"**Am√©lioration sugg√©r√©e**: {finding.improvement_suggestion}\n\n")
                
            # Roadmap am√©lioration
            f.write("## üöÄ ROADMAP AM√âLIORATION\n\n")
            for phase in report['improvement_roadmap']:
                f.write(f"### {phase['phase']} ({phase['timeline']})\n")
                f.write(f"**Priorit√©**: {phase['priority']}\n")
                f.write(f"**Crit√®re succ√®s**: {phase['success_criteria']}\n\n")
                for action in phase['actions'][:3]:
                    f.write(f"- {action['action']} ({action['component']})\n")
                f.write("\n")
                
            # Risques
            f.write("## ‚ö†Ô∏è √âVALUATION RISQUES\n\n")
            risks = report['risk_assessment']
            f.write(f"**Niveau risque global**: {risks['overall_risk_level']}\n\n")
            
            for risk_type, risk_list in risks.items():
                if risk_type != 'overall_risk_level' and risk_list:
                    f.write(f"**{risk_type.replace('_', ' ').title()}**:\n")
                    for risk in risk_list[:2]:
                        f.write(f"- {risk['risk']} (Impact: {risk['impact']})\n")
                    f.write("\n")
                    
            # Recommandations
            f.write("## üí° RECOMMANDATIONS CRITIQUES\n\n")
            for rec in report['recommendations']:
                f.write(f"### {rec['recommendation']} ({rec['priority']})\n")
                f.write(f"**Rationale**: {rec['rationale']}\n")
                f.write(f"**Timeline**: {rec['timeline']}\n")
                f.write(f"**Succ√®s**: {rec['success_metrics']}\n\n")
                
            f.write("---\n\n")
            f.write("*Rapport g√©n√©r√© par Agent Critique Adverse - Am√©lioration continue autonome*\n")
            
        print(f"‚úÖ Rapport critique lisible sauvegard√©: {readable_path}")

def main():
    """Fonction principale - ex√©cution autonome"""
    agent = AdversarialCriticAgent()
    agent.autonomous_criticism_cycle()

if __name__ == "__main__":
    print("üî• D√âMARRAGE AGENT CRITIQUE ADVERSE AUTONOME")
    print("=" * 50)
    main()
