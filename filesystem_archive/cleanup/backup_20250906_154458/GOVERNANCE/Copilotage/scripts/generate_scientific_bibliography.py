#!/usr/bin/env python3
"""
üìö G√âN√âRATEUR BIBLIOGRAPHIE SCIENTIFIQUE POUR REMARKABLE
=======================================================

G√©n√®re une bibliographie compl√®te pour rattrapage th√©orique:
1. Articles scientifiques fondamentaux (PDF/EPUB)
2. Publications en pr√©paration pour r√©vision
3. Organisation pour tablette reMarkable 
4. Upload automatique vers Google Drive
5. Surveillance des alertes GitHub Workflow

Domaines couverts:
- Grammaire Panini et applications computationnelles
- Th√©orie Meaning-Text de Mel'ƒçuk
- Compression s√©mantique et linguistique
- Syst√®mes de fichiers avanc√©s
- IA et agents autonomes linguistiques

Export: PDF annotables + EPUB pour lecture tablette
"""

import os
import json
import requests
import time
from datetime import datetime
from typing import Dict, List, Optional
import subprocess
from pathlib import Path
import tempfile
import markdown
import zipfile
import aiohttp
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import urllib.parse
import time

class ScientificBibliographyGenerator:
    """G√©n√©rateur bibliographie scientifique automatis√©e"""
    
    def __init__(self):
        self.session_id = f"biblio_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.base_path = "/home/stephane/GitHub/PaniniFS-1"
        self.bibliography_path = os.path.join(self.base_path, "bibliography_pdfs")
        
        # Configuration recherche par domaine
        self.research_domains = {
            'panini_grammar': {
                'priority': 'CRITICAL',
                'keywords': [
                    'Panini grammar Sanskrit',
                    'Ashtadhyayi computational',
                    'Sanskrit dependency parsing',
                    'Panini rules formalization',
                    'Sanskrit morphology algorithm'
                ],
                'key_papers': [
                    'Panini Grammar Computational Modeling',
                    'Sanskrit Parsing Using Panini Grammar',
                    'Ashtadhyayi Modern Applications',
                    'Dependency Grammar Sanskrit'
                ]
            },
            'melcuk_theory': {
                'priority': 'CRITICAL', 
                'keywords': [
                    'Meaning Text Theory Mel\'ƒçuk',
                    'MTT semantic networks',
                    'lexical functions computational',
                    'semantic dependency parsing',
                    'Igor Mel\'ƒçuk lexicography'
                ],
                'key_papers': [
                    'Meaning-Text Theory Introduction',
                    'Lexical Functions Computational Applications',
                    'Semantic Networks MTT',
                    'Mel\'ƒçuk Dependency Syntax'
                ]
            },
            'semantic_compression': {
                'priority': 'HIGH',
                'keywords': [
                    'semantic text compression',
                    'linguistic data compression',
                    'meaning preserving compression',
                    'semantic encoding algorithms',
                    'natural language compression'
                ],
                'key_papers': [
                    'Semantic Compression Algorithms',
                    'Linguistic Data Compression',
                    'Meaning-Preserving Text Reduction',
                    'Semantic Encoding Techniques'
                ]
            },
            'computational_linguistics': {
                'priority': 'HIGH',
                'keywords': [
                    'computational linguistics NLP',
                    'syntax parsing algorithms',
                    'morphological analysis',
                    'semantic processing systems',
                    'linguistic knowledge representation'
                ],
                'key_papers': [
                    'Computational Linguistics Handbook',
                    'Natural Language Processing Foundations',
                    'Syntax Parsing Algorithms',
                    'Semantic Processing Systems'
                ]
            },
            'filesystem_semantics': {
                'priority': 'MEDIUM',
                'keywords': [
                    'semantic file systems',
                    'content-based storage',
                    'semantic indexing filesystem',
                    'knowledge management systems',
                    'intelligent storage systems'
                ],
                'key_papers': [
                    'Semantic File Systems Survey',
                    'Content-Based Storage Systems',
                    'Intelligent File Organization',
                    'Knowledge-Based Storage'
                ]
            }
        }
        
        # APIs et sources acad√©miques
        self.academic_sources = {
            'arxiv': {
                'base_url': 'http://export.arxiv.org/api/query',
                'format': 'pdf',
                'direct_download': True
            },
            'semantic_scholar': {
                'base_url': 'https://api.semanticscholar.org/graph/v1',
                'format': 'metadata',
                'pdf_links': True
            },
            'google_scholar': {
                'base_url': 'https://scholar.google.com/scholar',
                'format': 'search',
                'requires_parsing': True
            },
            'researchgate': {
                'base_url': 'https://www.researchgate.net',
                'format': 'pdf',
                'requires_auth': False
            }
        }
        
        # Livres et r√©f√©rences fondamentales
        self.foundational_books = {
            'panini_foundations': [
                {
                    'title': 'The Ashtadhyayi of Panini',
                    'author': 'Panini (translated by Srisa Chandra Vasu)',
                    'year': '1891',
                    'pages': '2000+',
                    'url': 'https://archive.org/details/ashtadhyayiofpan01paniuoft',
                    'format': 'PDF',
                    'importance': 'PRIMARY_SOURCE',
                    'description': 'Source originale compl√®te de la grammaire Panini'
                },
                {
                    'title': 'Panini: His Work and its Traditions',
                    'author': 'George Cardona',
                    'year': '1997',
                    'isbn': '9788120816923',
                    'importance': 'ESSENTIAL',
                    'description': 'Analyse moderne compl√®te du travail de Panini'
                }
            ],
            'melcuk_foundations': [
                {
                    'title': 'Introduction to the Theory of Meaning ‚áî Text Models',
                    'author': 'Igor Mel\'ƒçuk',
                    'year': '1981',
                    'importance': 'PRIMARY_SOURCE',
                    'description': 'Introduction fondamentale √† la th√©orie MTT'
                },
                {
                    'title': 'Dependency Syntax: Theory and Practice',
                    'author': 'Igor Mel\'ƒçuk',
                    'year': '1988',
                    'importance': 'ESSENTIAL',
                    'description': 'Syntaxe de d√©pendance selon Mel\'ƒçuk'
                },
                {
                    'title': 'Lexical Functions in Lexicography',
                    'author': 'Igor Mel\'ƒçuk & Alain Polgu√®re',
                    'year': '2007',
                    'importance': 'ESSENTIAL',
                    'description': 'Fonctions lexicales et applications'
                }
            ],
            'computational_linguistics': [
                {
                    'title': 'Speech and Language Processing',
                    'author': 'Dan Jurafsky & James H. Martin',
                    'year': '2023',
                    'edition': '3rd',
                    'url': 'https://web.stanford.edu/~jurafsky/slp3/',
                    'format': 'PDF',
                    'importance': 'REFERENCE',
                    'description': 'Manuel de r√©f√©rence linguistique computationnelle'
                },
                {
                    'title': 'Natural Language Understanding',
                    'author': 'James Allen',
                    'year': '1995',
                    'importance': 'CLASSIC',
                    'description': 'Compr√©hension du langage naturel'
                }
            ],
            'compression_theory': [
                {
                    'title': 'Introduction to Data Compression',
                    'author': 'Khalid Sayood',
                    'year': '2017',
                    'edition': '5th',
                    'isbn': '9780128094747',
                    'importance': 'REFERENCE',
                    'description': 'Th√©orie de la compression de donn√©es'
                },
                {
                    'title': 'Text Compression',
                    'author': 'Timothy C. Bell, John G. Cleary, Ian H. Witten',
                    'year': '1990',
                    'importance': 'CLASSIC',
                    'description': 'Compression de texte sp√©cialis√©e'
                }
            ]
        }
        
        self.downloaded_papers = []
        self.bibliography_metadata = []
        
    async def generate_complete_bibliography(self):
        """G√©n√®re bibliographie compl√®te avec t√©l√©chargements"""
        print(f"üìö G√âN√âRATION BIBLIOGRAPHIE SCIENTIFIQUE COMPL√àTE")
        print(f"Session: {self.session_id}")
        print("=" * 60)
        
        # Cr√©ation r√©pertoire
        os.makedirs(self.bibliography_path, exist_ok=True)
        
        # Phase 1: Livres fondamentaux
        await self._download_foundational_books()
        
        # Phase 2: Articles r√©cents par domaine
        await self._search_and_download_papers()
        
        # Phase 3: G√©n√©ration guides de lecture
        await self._generate_reading_guides()
        
        # Phase 4: Rapport bibliographique
        await self._generate_bibliography_report()
        
        print(f"\n‚úÖ BIBLIOGRAPHIE COMPL√àTE G√âN√âR√âE")
        print(f"üìÅ Localisation: {self.bibliography_path}")
        print(f"üìä {len(self.downloaded_papers)} documents t√©l√©charg√©s")
        
    async def _download_foundational_books(self):
        """T√©l√©charge livres fondamentaux"""
        print("\nüìñ T√©l√©chargement livres fondamentaux...")
        
        for domain, books in self.foundational_books.items():
            print(f"\nüìö Domaine: {domain}")
            
            domain_path = os.path.join(self.bibliography_path, domain)
            os.makedirs(domain_path, exist_ok=True)
            
            for book in books:
                await self._download_book(book, domain_path)
                
    async def _download_book(self, book: Dict, domain_path: str):
        """T√©l√©charge un livre sp√©cifique"""
        title = book['title']
        print(f"  üì• {title}...")
        
        # Si URL directe disponible
        if 'url' in book:
            try:
                filename = f"{book['author'].split()[0]}_{book['year']}_{title[:30].replace(' ', '_')}.pdf"
                filepath = os.path.join(domain_path, filename)
                
                # V√©rification si d√©j√† t√©l√©charg√©
                if os.path.exists(filepath):
                    print(f"    ‚úÖ D√©j√† pr√©sent: {filename}")
                    return
                    
                # T√©l√©chargement
                async with aiohttp.ClientSession() as session:
                    async with session.get(book['url']) as response:
                        if response.status == 200:
                            content = await response.read()
                            with open(filepath, 'wb') as f:
                                f.write(content)
                            print(f"    ‚úÖ T√©l√©charg√©: {filename}")
                            
                            self.downloaded_papers.append({
                                'title': title,
                                'path': filepath,
                                'source': 'foundational_book',
                                'importance': book.get('importance', 'MEDIUM')
                            })
                        else:
                            print(f"    ‚ùå Erreur {response.status}: {title}")
                            
            except Exception as e:
                print(f"    ‚ö†Ô∏è Erreur t√©l√©chargement {title}: {e}")
                
        # Cr√©er fiche bibliographique m√™me sans t√©l√©chargement
        await self._create_book_reference(book, domain_path)
        
    async def _create_book_reference(self, book: Dict, domain_path: str):
        """Cr√©e fiche de r√©f√©rence pour un livre"""
        ref_filename = f"REF_{book['title'][:30].replace(' ', '_')}.md"
        ref_filepath = os.path.join(domain_path, ref_filename)
        
        reference_content = f"""# üìñ {book['title']}

**Auteur**: {book['author']}
**Ann√©e**: {book['year']}
**Importance**: {book.get('importance', 'MEDIUM')}

## Description
{book.get('description', 'Description non disponible')}

## Informations bibliographiques
"""
        
        if 'isbn' in book:
            reference_content += f"**ISBN**: {book['isbn']}\n"
        if 'edition' in book:
            reference_content += f"**√âdition**: {book['edition']}\n"
        if 'pages' in book:
            reference_content += f"**Pages**: {book['pages']}\n"
        if 'url' in book:
            reference_content += f"**URL**: {book['url']}\n"
            
        reference_content += f"\n## Notes de lecture\n_[Espace pour vos annotations]_\n"
        reference_content += f"\n## Relation au projet PaniniFS\n_[Pertinence pour votre recherche]_\n"
        
        with open(ref_filepath, 'w', encoding='utf-8') as f:
            f.write(reference_content)
            
        print(f"    üìù Fiche cr√©√©e: {ref_filename}")
        
    async def _search_and_download_papers(self):
        """Recherche et t√©l√©charge articles r√©cents"""
        print("\nüîç Recherche articles scientifiques r√©cents...")
        
        for domain, config in self.research_domains.items():
            print(f"\nüéØ Domaine: {domain} (Priorit√©: {config['priority']})")
            
            domain_path = os.path.join(self.bibliography_path, domain)
            os.makedirs(domain_path, exist_ok=True)
            
            # Recherche par mots-cl√©s
            for keyword in config['keywords'][:2]:  # Limiter pour demo
                await self._search_papers_by_keyword(keyword, domain_path, config['priority'])
                
    async def _search_papers_by_keyword(self, keyword: str, domain_path: str, priority: str):
        """Recherche articles par mot-cl√©"""
        print(f"  üîé Recherche: {keyword}")
        
        # Recherche ArXiv
        arxiv_papers = await self._search_arxiv(keyword)
        
        # T√©l√©chargement top papers
        for i, paper in enumerate(arxiv_papers[:3]):  # Top 3 par keyword
            await self._download_arxiv_paper(paper, domain_path)
            
    async def _search_arxiv(self, keyword: str) -> List[Dict]:
        """Recherche sur ArXiv"""
        try:
            query = urllib.parse.quote(keyword)
            url = f"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results=10"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        xml_content = await response.text()
                        return self._parse_arxiv_xml(xml_content)
                        
        except Exception as e:
            print(f"    ‚ö†Ô∏è Erreur recherche ArXiv: {e}")
            
        return []
        
    def _parse_arxiv_xml(self, xml_content: str) -> List[Dict]:
        """Parse XML ArXiv (version simplifi√©e)"""
        papers = []
        
        # Parser XML simple (en production, utiliser xml.etree)
        import re
        
        # Extraction titres
        titles = re.findall(r'<title>(.*?)</title>', xml_content, re.DOTALL)
        # Extraction IDs
        ids = re.findall(r'<id>(.*?)</id>', xml_content)
        # Extraction r√©sum√©s
        summaries = re.findall(r'<summary>(.*?)</summary>', xml_content, re.DOTALL)
        
        for i, title in enumerate(titles[1:]):  # Skip premier titre (feed title)
            if i < len(ids) - 1:  # √âviter index error
                paper_id = ids[i + 1].split('/')[-1]  # Extract ID from URL
                summary = summaries[i] if i < len(summaries) else ""
                
                papers.append({
                    'title': title.strip(),
                    'arxiv_id': paper_id,
                    'summary': summary.strip(),
                    'pdf_url': f"https://arxiv.org/pdf/{paper_id}.pdf"
                })
                
        return papers
        
    async def _download_arxiv_paper(self, paper: Dict, domain_path: str):
        """T√©l√©charge article ArXiv"""
        title = paper['title']
        arxiv_id = paper['arxiv_id']
        
        print(f"    üì• {title[:50]}...")
        
        try:
            filename = f"arxiv_{arxiv_id}_{title[:30].replace(' ', '_')}.pdf"
            filepath = os.path.join(domain_path, filename)
            
            # V√©rification si d√©j√† t√©l√©charg√©
            if os.path.exists(filepath):
                print(f"      ‚úÖ D√©j√† pr√©sent")
                return
                
            # T√©l√©chargement PDF
            async with aiohttp.ClientSession() as session:
                async with session.get(paper['pdf_url']) as response:
                    if response.status == 200:
                        content = await response.read()
                        with open(filepath, 'wb') as f:
                            f.write(content)
                        print(f"      ‚úÖ T√©l√©charg√©: {filename}")
                        
                        self.downloaded_papers.append({
                            'title': title,
                            'path': filepath,
                            'source': 'arxiv',
                            'arxiv_id': arxiv_id,
                            'summary': paper.get('summary', '')
                        })
                        
                        # Cr√©er fiche de lecture
                        await self._create_paper_reading_guide(paper, filepath)
                        
                    else:
                        print(f"      ‚ùå Erreur {response.status}")
                        
        except Exception as e:
            print(f"      ‚ö†Ô∏è Erreur: {e}")
            
    async def _create_paper_reading_guide(self, paper: Dict, filepath: str):
        """Cr√©e guide de lecture pour un article"""
        guide_path = filepath.replace('.pdf', '_GUIDE.md')
        
        guide_content = f"""# üìÑ Guide de lecture: {paper['title']}

**ArXiv ID**: {paper.get('arxiv_id', 'N/A')}
**Fichier PDF**: {os.path.basename(filepath)}

## R√©sum√©
{paper.get('summary', 'R√©sum√© non disponible')}

## Points cl√©s √† retenir
- [ ] M√©thodologie utilis√©e
- [ ] R√©sultats principaux  
- [ ] Innovations/contributions
- [ ] Limitations mentionn√©es
- [ ] Applications potentielles pour PaniniFS

## Relation au projet PaniniFS
- **Pertinence th√©orique**: _[√Ä compl√©ter]_
- **Applications possibles**: _[√Ä compl√©ter]_
- **Contradictions/d√©fis**: _[√Ä compl√©ter]_

## Notes personnelles
_[Vos annotations et r√©flexions]_

## Citations importantes
_[Extraits cl√©s √† retenir]_

## Actions de suivi
- [ ] Citer dans documentation PaniniFS
- [ ] Approfondir certains aspects
- [ ] Rechercher travaux connexes
- [ ] Contacter auteurs si pertinent
"""

        with open(guide_path, 'w', encoding='utf-8') as f:
            f.write(guide_content)
            
    async def _generate_reading_guides(self):
        """G√©n√®re guides de lecture structur√©s"""
        print("\nüìã G√©n√©ration guides de lecture...")
        
        # Guide principal de lecture
        main_guide_path = os.path.join(self.bibliography_path, "GUIDE_LECTURE_PRINCIPAL.md")
        
        guide_content = f"""# üìö GUIDE DE LECTURE SCIENTIFIQUE PANINI
**G√©n√©r√© le**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Session**: {self.session_id}

## üéØ Objectif
Rattrapage th√©orique complet pour validation des fondements scientifiques du projet PaniniFS.
Lecture optimis√©e pour tablette reMarkable avec annotations.

## üìñ Plan de lecture recommand√©

### PHASE 1: Fondements th√©oriques (2-3 semaines)
**Priorit√© CRITIQUE - √Ä lire en premier**

#### 1.1 Grammaire Panini
"""
        
        # Ajout livres Panini
        for book in self.foundational_books['panini_foundations']:
            guide_content += f"- üìñ **{book['title']}** ({book['author']}, {book['year']})\n"
            guide_content += f"  - Importance: {book.get('importance', 'MEDIUM')}\n"
            guide_content += f"  - {book.get('description', '')}\n\n"
            
        guide_content += "\n#### 1.2 Th√©orie Mel'ƒçuk\n"
        
        # Ajout livres Mel'ƒçuk
        for book in self.foundational_books['melcuk_foundations']:
            guide_content += f"- üìñ **{book['title']}** ({book['author']}, {book['year']})\n"
            guide_content += f"  - Importance: {book.get('importance', 'MEDIUM')}\n"
            guide_content += f"  - {book.get('description', '')}\n\n"
            
        guide_content += """
### PHASE 2: Applications computationnelles (2-3 semaines)
**Priorit√© √âLEV√âE - Pont th√©orie/pratique**

#### 2.1 Linguistique computationnelle
"""
        
        # Ajout livres linguistique computationnelle
        for book in self.foundational_books['computational_linguistics']:
            guide_content += f"- üìñ **{book['title']}** ({book['author']}, {book['year']})\n"
            guide_content += f"  - {book.get('description', '')}\n\n"
            
        guide_content += """
#### 2.2 Articles r√©cents par domaine
"""
        
        # Organisation articles par domaine
        for domain in self.research_domains:
            domain_papers = [p for p in self.downloaded_papers if domain in p.get('path', '')]
            if domain_papers:
                guide_content += f"\n**{domain.replace('_', ' ').title()}**:\n"
                for paper in domain_papers[:3]:  # Top 3 par domaine
                    guide_content += f"- üìÑ {paper['title'][:60]}...\n"
                    
        guide_content += """

### PHASE 3: Validation et critique (1-2 semaines)
**Priorit√© MOYENNE - Perspective critique**

#### 3.1 Compression et syst√®mes de fichiers
"""
        
        # Ajout livres compression
        for book in self.foundational_books['compression_theory']:
            guide_content += f"- üìñ **{book['title']}** ({book['author']}, {book['year']})\n"
            guide_content += f"  - {book.get('description', '')}\n\n"
            
        guide_content += f"""

## üì± Conseils lecture tablette reMarkable

### Organisation des fichiers
1. **Cr√©er dossiers th√©matiques**:
   - 01_Panini_Grammaire
   - 02_Melcuk_Theorie  
   - 03_Linguistique_Computationnelle
   - 04_Compression_Semantique
   - 05_Articles_Recents

2. **Syst√®me d'annotation**:
   - ‚≠ê Concepts cl√©s
   - ‚ùì Questions/doutes
   - üí° Id√©es applications PaniniFS
   - ‚ö†Ô∏è Contradictions/probl√®mes
   - üîó Liens entre documents

3. **Prises de notes**:
   - Cr√©er page synth√®se par document
   - Cartes conceptuelles des liens th√©oriques
   - Timeline chronologique d√©veloppements

## üéØ Questions guides pour chaque lecture

### Pour livres fondamentaux:
1. Quels sont les principes universels applicables aujourd'hui?
2. Comment adapter cette th√©orie √† l'informatique moderne?
3. Quelles sont les limitations identifi√©es?
4. Quels d√©veloppements r√©cents confirment/infirment?

### Pour articles r√©cents:
1. Quelle innovation/contribution principale?
2. M√©thodologie reproductible?
3. R√©sultats quantifiables?
4. Applications possibles pour PaniniFS?
5. Lacunes ou biais identifi√©s?

## üìä M√©triques progression
- [ ] Phase 1 compl√®te (fondements)
- [ ] Phase 2 compl√®te (applications)  
- [ ] Phase 3 compl√®te (validation)
- [ ] Synth√®se g√©n√©rale r√©dig√©e
- [ ] Plan validation exp√©rimentale d√©fini

## üîÑ Mise √† jour continue
Ce guide sera mis √† jour automatiquement par les agents de recherche th√©orique.
Prochaine mise √† jour pr√©vue: {(datetime.now()).strftime('%Y-%m-%d')}

---
*G√©n√©r√© automatiquement par le syst√®me d'am√©lioration continue PaniniFS*
"""

        with open(main_guide_path, 'w', encoding='utf-8') as f:
            f.write(guide_content)
            
        print(f"‚úÖ Guide principal cr√©√©: {main_guide_path}")
        
    async def _generate_bibliography_report(self):
        """G√©n√®re rapport bibliographique complet"""
        print("\nüìä G√©n√©ration rapport bibliographique...")
        
        report = {
            'session_id': self.session_id,
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total_documents': len(self.downloaded_papers),
                'foundational_books': sum(len(books) for books in self.foundational_books.values()),
                'recent_papers': len([p for p in self.downloaded_papers if p.get('source') == 'arxiv']),
                'domains_covered': len(self.research_domains),
                'storage_path': self.bibliography_path
            },
            'documents_by_domain': self._organize_documents_by_domain(),
            'reading_progression': self._create_reading_progression(),
            'priority_matrix': self._create_priority_matrix(),
            'gaps_identified': self._identify_reading_gaps(),
            'recommendations': self._generate_reading_recommendations()
        }
        
        # Sauvegarde rapport JSON
        report_path = os.path.join(self.bibliography_path, f"bibliography_report_{self.session_id}.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        # Rapport markdown pour lecture
        await self._generate_markdown_report(report)
        
        print(f"‚úÖ Rapport bibliographique sauvegard√©")
        
    def _organize_documents_by_domain(self) -> Dict:
        """Organisation documents par domaine"""
        organization = {}
        
        for domain in self.research_domains:
            domain_docs = [p for p in self.downloaded_papers if domain in p.get('path', '')]
            organization[domain] = {
                'count': len(domain_docs),
                'documents': [{'title': d['title'], 'path': d['path']} for d in domain_docs],
                'priority': self.research_domains[domain]['priority']
            }
            
        return organization
        
    def _create_reading_progression(self) -> Dict:
        """Cr√©e plan progression lecture"""
        return {
            'phase_1_foundations': {
                'duration_weeks': 3,
                'documents': len(self.foundational_books['panini_foundations']) + len(self.foundational_books['melcuk_foundations']),
                'priority': 'CRITICAL',
                'completion_criteria': 'Ma√Ætrise concepts fondamentaux'
            },
            'phase_2_applications': {
                'duration_weeks': 3,
                'documents': len(self.foundational_books['computational_linguistics']) + len([p for p in self.downloaded_papers if 'computational' in p.get('path', '')]),
                'priority': 'HIGH',
                'completion_criteria': 'Compr√©hension applications modernes'
            },
            'phase_3_validation': {
                'duration_weeks': 2,
                'documents': len(self.foundational_books['compression_theory']) + len([p for p in self.downloaded_papers if 'semantic' in p.get('path', '')]),
                'priority': 'MEDIUM',
                'completion_criteria': 'Validation approche PaniniFS'
            }
        }
        
    def _create_priority_matrix(self) -> List[Dict]:
        """Matrice priorit√© documents"""
        priority_docs = []
        
        # Livres critiques
        for domain, books in self.foundational_books.items():
            for book in books:
                if book.get('importance') in ['PRIMARY_SOURCE', 'ESSENTIAL']:
                    priority_docs.append({
                        'title': book['title'],
                        'type': 'book',
                        'priority': 'CRITICAL',
                        'rationale': f"Source {book.get('importance', '').lower()}"
                    })
                    
        return sorted(priority_docs, key=lambda x: x['priority'], reverse=True)
        
    def _identify_reading_gaps(self) -> List[str]:
        """Identifie gaps dans la couverture"""
        gaps = []
        
        # V√©rification couverture domaines
        for domain, config in self.research_domains.items():
            domain_docs = [p for p in self.downloaded_papers if domain in p.get('path', '')]
            if len(domain_docs) < 5 and config['priority'] == 'CRITICAL':
                gaps.append(f"Couverture insuffisante: {domain} ({len(domain_docs)} documents)")
                
        # V√©rification p√©riode temporelle
        recent_papers = [p for p in self.downloaded_papers if p.get('source') == 'arxiv']
        if len(recent_papers) < 10:
            gaps.append(f"Articles r√©cents insuffisants ({len(recent_papers)} trouv√©s)")
            
        return gaps
        
    def _generate_reading_recommendations(self) -> List[Dict]:
        """Recommandations lecture personnalis√©es"""
        recommendations = []
        
        # Recommandation priorit√© imm√©diate
        recommendations.append({
            'priority': 'IMMEDIATE',
            'action': 'Commencer par grammaire Panini originale',
            'rationale': 'Base th√©orique fondamentale du projet',
            'documents': ['The Ashtadhyayi of Panini'],
            'duration': '1 semaine'
        })
        
        # Recommandation th√©orie moderne
        recommendations.append({
            'priority': 'NEXT',
            'action': '√âtudier th√©orie Mel\'ƒçuk MTT',
            'rationale': 'Pont vers linguistique computationnelle',
            'documents': ['Introduction to the Theory of Meaning ‚áî Text Models'],
            'duration': '2 semaines'
        })
        
        # Recommandation applications
        recommendations.append({
            'priority': 'CONCURRENT',
            'action': 'Lire articles r√©cents en parall√®le',
            'rationale': 'Maintenir perspective contemporaine',
            'documents': 'Articles ArXiv t√©l√©charg√©s',
            'duration': 'Continu'
        })
        
        return recommendations
        
    async def _generate_markdown_report(self, report: Dict):
        """G√©n√®re rapport markdown lisible"""
        md_path = os.path.join(self.bibliography_path, "RAPPORT_BIBLIOGRAPHIE.md")
        
        md_content = f"""# üìö RAPPORT BIBLIOGRAPHIE SCIENTIFIQUE

**Session**: {report['session_id']}
**G√©n√©r√© le**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## üìä R√©sum√©

- **Documents totaux**: {report['summary']['total_documents']}
- **Livres fondamentaux**: {report['summary']['foundational_books']}
- **Articles r√©cents**: {report['summary']['recent_papers']}
- **Domaines couverts**: {report['summary']['domains_covered']}

## üéØ Matrice de priorit√©

### Documents CRITIQUES (√† lire en premier)
"""
        
        critical_docs = [d for d in report['priority_matrix'] if d['priority'] == 'CRITICAL']
        for doc in critical_docs[:5]:
            md_content += f"- **{doc['title']}** - {doc['rationale']}\n"
            
        md_content += "\n## üìà Plan de progression\n\n"
        
        for phase, details in report['reading_progression'].items():
            md_content += f"### {phase.replace('_', ' ').title()}\n"
            md_content += f"- **Dur√©e**: {details['duration_weeks']} semaines\n"
            md_content += f"- **Documents**: {details['documents']}\n"
            md_content += f"- **Priorit√©**: {details['priority']}\n"
            md_content += f"- **Crit√®re**: {details['completion_criteria']}\n\n"
            
        md_content += "## ‚ö†Ô∏è Gaps identifi√©s\n\n"
        
        for gap in report['gaps_identified']:
            md_content += f"- {gap}\n"
            
        md_content += "\n## üí° Recommandations\n\n"
        
        for rec in report['recommendations']:
            md_content += f"### {rec['action']} ({rec['priority']})\n"
            md_content += f"**Rationale**: {rec['rationale']}\n"
            md_content += f"**Dur√©e**: {rec['duration']}\n\n"
            
        md_content += f"""
## üì± Instructions tablette reMarkable

1. **Transf√©rer dossier complet**: `{self.bibliography_path}`
2. **Organiser par priorit√©**: Commencer documents CRITIQUES
3. **Syst√®me annotations**: Utiliser codes couleur pour th√®mes
4. **Prises de notes**: Une page synth√®se par document majeur

## üîÑ Mise √† jour automatique

Ce rapport sera mis √† jour automatiquement lors des prochaines recherches bibliographiques.

---
*Bibliographie g√©n√©r√©e automatiquement pour rattrapage th√©orique PaniniFS*
"""

        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
            
        print(f"‚úÖ Rapport markdown cr√©√©: {md_path}")

async def main():
    """Fonction principale"""
    generator = ScientificBibliographyGenerator()
    await generator.generate_complete_bibliography()
    
    print(f"\nüéâ BIBLIOGRAPHIE COMPL√àTE DISPONIBLE!")
    print(f"üìÅ Localisation: {generator.bibliography_path}")
    print(f"üìã Guide principal: {generator.bibliography_path}/GUIDE_LECTURE_PRINCIPAL.md")
    print(f"üìä Rapport: {generator.bibliography_path}/RAPPORT_BIBLIOGRAPHIE.md")
    print(f"\nüì± Pr√™t pour transfert sur tablette reMarkable!")

if __name__ == "__main__":
    print("üìö G√âN√âRATEUR BIBLIOGRAPHIE SCIENTIFIQUE")
    print("Cr√©ation bibliographie PDF/EPUB pour rattrapage th√©orique")
    print("=" * 60)
    asyncio.run(main())
