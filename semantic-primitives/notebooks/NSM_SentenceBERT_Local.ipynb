{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74e0bd1d",
   "metadata": {},
   "source": [
    "# üöÄ Analyse NSM-Greimas avec Sentence-BERT (100% Local, Gratuit)\n",
    "\n",
    "**Mod√®le** : `paraphrase-multilingual-mpnet-base-v2` (278M params)\n",
    "**Co√ªt** : $0 (aucun API)\n",
    "**Dur√©e** : ~5 minutes total\n",
    "**GPU** : Optionnel (fonctionne sur CPU)\n",
    "\n",
    "---\n",
    "\n",
    "## Pourquoi Sentence-BERT ?\n",
    "\n",
    "| Avantage | Description |\n",
    "|----------|-------------|\n",
    "| ‚úÖ **Gratuit** | Aucun co√ªt API, illimit√© |\n",
    "| ‚úÖ **Rapide** | 5x plus rapide que DeepSeek API |\n",
    "| ‚úÖ **Qualit√©** | SOTA embeddings s√©mantiques |\n",
    "| ‚úÖ **Multilingue** | 50+ langues (FR/EN/Sanskrit) |\n",
    "| ‚úÖ **Reproductible** | Mod√®le fig√©, r√©sultats stables |\n",
    "| ‚úÖ **Scientifique** | 12,000+ citations, benchmark valid√© |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ea3323",
   "metadata": {},
   "source": [
    "## üì¶ Setup (2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79140c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation d√©pendances\n",
    "!pip install -q sentence-transformers scikit-learn matplotlib seaborn plotly pandas tqdm\n",
    "\n",
    "print(\"‚úÖ Packages install√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a159e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo Panini Research\n",
    "import os\n",
    "if not os.path.exists('Panini-Research'):\n",
    "    !git clone https://github.com/stephanedenis/Panini-Research.git\n",
    "    print(\"‚úÖ Repo clon√©\")\n",
    "else:\n",
    "    print(\"‚úÖ Repo d√©j√† pr√©sent\")\n",
    "\n",
    "# Ajouter au path Python\n",
    "import sys\n",
    "sys.path.append('/content/Panini-Research/semantic-primitives')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b35e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Diagnostic : V√©rifier environnement Colab\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"üîç DIAGNOSTIC ENVIRONNEMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# V√©rifier repo clon√©\n",
    "repo_exists = os.path.exists('/content/Panini-Research')\n",
    "print(f\"1Ô∏è‚É£ Repo clon√© : {repo_exists}\")\n",
    "\n",
    "if repo_exists:\n",
    "    # V√©rifier fichier donnees_nsm.py\n",
    "    fichier = '/content/Panini-Research/semantic-primitives/notebooks/donnees_nsm.py'\n",
    "    fichier_exists = os.path.exists(fichier)\n",
    "    print(f\"2Ô∏è‚É£ Fichier donnees_nsm.py : {fichier_exists}\")\n",
    "    \n",
    "    if fichier_exists:\n",
    "        size = os.path.getsize(fichier)\n",
    "        print(f\"   üì¶ Taille : {size:,} bytes\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Fichier manquant : {fichier}\")\n",
    "        print(f\"   üí° Solution : !cd /content/Panini-Research && git pull origin main\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Repo pas clon√©\")\n",
    "    print(f\"   üí° Solution : Ex√©cuter cellule pr√©c√©dente (git clone)\")\n",
    "\n",
    "# V√©rifier path\n",
    "notebooks_path = '/content/Panini-Research/semantic-primitives/notebooks'\n",
    "print(f\"3Ô∏è‚É£ Path notebooks : {notebooks_path}\")\n",
    "print(f\"   Existe : {os.path.exists(notebooks_path)}\")\n",
    "\n",
    "if os.path.exists(notebooks_path):\n",
    "    # Lister fichiers .py\n",
    "    py_files = [f for f in os.listdir(notebooks_path) if f.endswith('.py')]\n",
    "    print(f\"   üìÑ Fichiers .py : {', '.join(py_files) if py_files else 'aucun'}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Diagnostic termin√©. Si erreurs, v√©rifier solutions ci-dessus.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import donn√©es NSM (primitives, carr√©s, corpus)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ajouter path notebooks\n",
    "notebooks_path = '/content/Panini-Research/semantic-primitives/notebooks'\n",
    "if notebooks_path not in sys.path:\n",
    "    sys.path.insert(0, notebooks_path)\n",
    "    print(f\"‚úÖ Path ajout√© : {notebooks_path}\")\n",
    "\n",
    "# V√©rifier fichier existe\n",
    "fichier_donnees = os.path.join(notebooks_path, 'donnees_nsm.py')\n",
    "if not os.path.exists(fichier_donnees):\n",
    "    print(f\"‚ùå ERREUR : Fichier manquant : {fichier_donnees}\")\n",
    "    print(f\"üí° Solution : Ex√©cuter cellule diagnostic ci-dessus\")\n",
    "    raise FileNotFoundError(f\"Fichier donnees_nsm.py introuvable dans {notebooks_path}\")\n",
    "\n",
    "# Import donn√©es\n",
    "try:\n",
    "    from donnees_nsm import NSM_PRIMITIVES, COULEURS_CATEGORIES, CARRES_SEMIOTIQUES, CORPUS_TEST\n",
    "    \n",
    "    print(f\"‚úÖ {len(NSM_PRIMITIVES)} primitives NSM charg√©es\")\n",
    "    print(f\"‚úÖ {len(CARRES_SEMIOTIQUES)} carr√©s s√©miotiques charg√©s\")\n",
    "    print(f\"‚úÖ {len(CORPUS_TEST)} phrases corpus charg√©es\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERREUR IMPORT : {e}\")\n",
    "    print(f\"üí° V√©rifiez que le fichier donnees_nsm.py est pr√©sent\")\n",
    "    print(f\"üí° Path actuel : {sys.path[:3]}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f0d891",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Si erreur ModuleNotFoundError ci-dessous\n",
    "\n",
    "**Solution 1** : Pull derni√®re version\n",
    "```python\n",
    "!cd /content/Panini-Research && git pull origin main\n",
    "```\n",
    "\n",
    "**Solution 2** : T√©l√©charger fichier direct\n",
    "```python\n",
    "!wget -O /content/donnees_nsm.py https://raw.githubusercontent.com/stephanedenis/Panini-Research/main/research/semantic-primitives/notebooks/donnees_nsm.py\n",
    "import sys\n",
    "sys.path.insert(0, '/content')\n",
    "```\n",
    "\n",
    "Puis r√©ex√©cuter la cellule ci-dessous ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dfbdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß SOLUTION RAPIDE : T√©l√©charger donnees_nsm.py directement\n",
    "# Ex√©cutez cette cellule SI ET SEULEMENT SI vous avez eu \"ModuleNotFoundError: No module named 'donnees_nsm'\"\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# URL du fichier sur GitHub\n",
    "url = \"https://raw.githubusercontent.com/stephanedenis/Panini-Research/main/research/semantic-primitives/notebooks/donnees_nsm.py\"\n",
    "destination = \"/content/donnees_nsm.py\"\n",
    "\n",
    "print(\"üì• T√©l√©chargement donnees_nsm.py depuis GitHub...\")\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve(url, destination)\n",
    "    print(f\"‚úÖ Fichier t√©l√©charg√© : {destination}\")\n",
    "    \n",
    "    # V√©rifier taille\n",
    "    size = os.path.getsize(destination)\n",
    "    print(f\"üì¶ Taille : {size:,} bytes\")\n",
    "    \n",
    "    # Ajouter au path\n",
    "    import sys\n",
    "    if '/content' not in sys.path:\n",
    "        sys.path.insert(0, '/content')\n",
    "        print(\"‚úÖ Path /content ajout√©\")\n",
    "    \n",
    "    print(\"\\nüéØ Maintenant, r√©ex√©cutez la cellule d'import ci-dessous !\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur t√©l√©chargement : {e}\")\n",
    "    print(\"\\nüí° Alternative : Copiez-collez le contenu du fichier directement\")\n",
    "    print(\"   URL : https://github.com/stephanedenis/Panini-Research/blob/main/research/semantic-primitives/notebooks/donnees_nsm.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab84c939",
   "metadata": {},
   "source": [
    "## ü§ñ Chargement Sentence-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# V√©rifier GPU disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üñ•Ô∏è Device : {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"   GPU : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Chargement mod√®le (2 min)\n",
    "print(\"\\nüì• Chargement Sentence-BERT multilingue...\")\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2', device=device)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le charg√©\")\n",
    "print(f\"   Dimensions embeddings : {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"   Max sequence length : {model.max_seq_length}\")\n",
    "print(f\"   Param√®tres : 278M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a82aa9",
   "metadata": {},
   "source": [
    "## üß™ Exp√©rience 1 : Clustering Primitives NSM\n",
    "\n",
    "**Hypoth√®se H1** : Les primitives NSM forment des clusters dans l'espace d'embeddings\n",
    "\n",
    "**M√©triques** :\n",
    "- Puret√© clustering (seuil > 0.7)\n",
    "- Silhouette score (seuil > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac7af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Encodage primitives NSM (30 sec)\n",
    "print(\"üî¢ Encodage 60 primitives NSM...\")\n",
    "\n",
    "primitives_list = list(NSM_PRIMITIVES.items())\n",
    "primitives_text = [p.forme_francaise for nom, p in primitives_list]\n",
    "primitives_noms = [nom for nom, p in primitives_list]\n",
    "primitives_categories = [p.categorie for nom, p in primitives_list]\n",
    "\n",
    "# Encodage batch (efficace)\n",
    "embeddings = model.encode(\n",
    "    primitives_text,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True  # Normalisation L2 automatique\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Embeddings encod√©s : shape {embeddings.shape}\")\n",
    "print(f\"   Mean norm : {np.mean(np.linalg.norm(embeddings, axis=1)):.3f}\")\n",
    "print(f\"   Std norm : {np.std(np.linalg.norm(embeddings, axis=1)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de328b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation t-SNE 2D\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìä Calcul t-SNE 2D...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=20, max_iter=1000)\n",
    "coords_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Scatter avec couleurs cat√©gories\n",
    "for i, (nom, primitive) in enumerate(primitives_list):\n",
    "    x, y = coords_2d[i]\n",
    "    color = COULEURS_CATEGORIES.get(primitive.categorie, 'gray')\n",
    "    plt.scatter(x, y, c=color, s=150, alpha=0.7, edgecolors='black', linewidths=1)\n",
    "    plt.annotate(\n",
    "        nom, \n",
    "        (x, y), \n",
    "        xytext=(5, 5), \n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7)\n",
    "    )\n",
    "\n",
    "# L√©gende cat√©gories\n",
    "from matplotlib.patches import Patch\n",
    "categories_uniques = set(primitives_categories)\n",
    "legend_elements = [Patch(facecolor=COULEURS_CATEGORIES.get(cat, 'gray'), label=cat) \n",
    "                   for cat in categories_uniques]\n",
    "plt.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "\n",
    "plt.title(\"t-SNE 2D : Primitives NSM - Sentence-BERT Multilingue\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Dimension 1\", fontsize=12)\n",
    "plt.ylabel(\"Dimension 2\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('tsne_primitives_sbert.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualisation sauvegard√©e : tsne_primitives_sbert.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f616a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering K-means\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Mapper cat√©gories vers labels num√©riques\n",
    "categories_uniques = sorted(set(primitives_categories))\n",
    "cat_to_label = {cat: i for i, cat in enumerate(categories_uniques)}\n",
    "labels_true = [cat_to_label[cat] for cat in primitives_categories]\n",
    "\n",
    "print(f\"üìä Clustering K-means (k={len(categories_uniques)})...\")\n",
    "kmeans = KMeans(n_clusters=len(categories_uniques), random_state=42, n_init=20)\n",
    "labels_pred = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Calcul puret√©\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(labels_true, labels_pred)\n",
    "row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "purete = cm[row_ind, col_ind].sum() / len(labels_true)\n",
    "\n",
    "# Silhouette\n",
    "silhouette = silhouette_score(embeddings, labels_pred)\n",
    "\n",
    "print(f\"\\nüìà R√©sultats Exp√©rience 1 :\")\n",
    "print(f\"   Puret√© clustering : {purete:.3f} (seuil > 0.7)\")\n",
    "print(f\"   Silhouette score : {silhouette:.3f} (seuil > 0.5)\")\n",
    "print(f\"\")\n",
    "print(f\"   H1 : {'‚úÖ VALID√âE' if purete > 0.7 and silhouette > 0.5 else '‚ùå R√âFUT√âE'}\")\n",
    "print(f\"   ‚Üí Primitives NSM {'forment' if purete > 0.7 else 'ne forment pas'} des clusters distincts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b50eed",
   "metadata": {},
   "source": [
    "## üß™ Exp√©rience 2 : Carr√©s S√©miotiques de Greimas\n",
    "\n",
    "**Hypoth√®se H2** : Les structures oppositionnelles de Greimas sont g√©om√©triquement encod√©es\n",
    "\n",
    "**Validation** : Distance(S1, S2) > Distance(S1, S1) (oppos√©s plus √©loign√©s que contraires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "print(\"üî¢ Analyse 20 carr√©s s√©miotiques...\")\n",
    "\n",
    "# Cr√©er dictionnaire nom ‚Üí embedding\n",
    "primitives_embeddings_dict = {nom: emb for nom, emb in zip(primitives_noms, embeddings)}\n",
    "\n",
    "resultats_carres = []\n",
    "\n",
    "for nom_carre, carre in CARRES_SEMIOTIQUES.items():\n",
    "    # R√©cup√©rer embeddings des 4 positions\n",
    "    s1_emb = primitives_embeddings_dict.get(carre[\"S1\"])\n",
    "    s2_emb = primitives_embeddings_dict.get(carre[\"S2\"])\n",
    "    non_s1_emb = primitives_embeddings_dict.get(carre[\"non_S1\"])\n",
    "    non_s2_emb = primitives_embeddings_dict.get(carre[\"non_S2\"])\n",
    "    \n",
    "    if any(e is None for e in [s1_emb, s2_emb, non_s1_emb, non_s2_emb]):\n",
    "        print(f\"‚ö†Ô∏è Carr√© {nom_carre} : Primitives manquantes\")\n",
    "        continue\n",
    "    \n",
    "    # Distances\n",
    "    dist_contraires = cosine_distances([s1_emb], [s2_emb])[0][0]  # S1 ‚Üî S2\n",
    "    dist_contradictoires_1 = cosine_distances([s1_emb], [non_s1_emb])[0][0]  # S1 ‚Üî ¬¨S1\n",
    "    dist_contradictoires_2 = cosine_distances([s2_emb], [non_s2_emb])[0][0]  # S2 ‚Üî ¬¨S2\n",
    "    dist_complementaires = cosine_distances([non_s1_emb], [s2_emb])[0][0]  # ¬¨S1 ‚Üî S2\n",
    "    \n",
    "    # Validation Greimas : contraires < contradictoires\n",
    "    valide = (\n",
    "        dist_contraires < dist_contradictoires_1 and \n",
    "        dist_contraires < dist_contradictoires_2\n",
    "    )\n",
    "    \n",
    "    resultats_carres.append({\n",
    "        'nom': nom_carre,\n",
    "        's1': carre[\"S1\"],\n",
    "        's2': carre[\"S2\"],\n",
    "        'dist_contraires': dist_contraires,\n",
    "        'dist_contradictoires': (dist_contradictoires_1 + dist_contradictoires_2) / 2,\n",
    "        'valide': valide\n",
    "    })\n",
    "\n",
    "# Statistiques\n",
    "nb_valides = sum(r['valide'] for r in resultats_carres)\n",
    "taux_validation = nb_valides / len(resultats_carres)\n",
    "\n",
    "print(f\"\\nüìà R√©sultats Exp√©rience 2 :\")\n",
    "print(f\"   Carr√©s analys√©s : {len(resultats_carres)}\")\n",
    "print(f\"   Carr√©s valides : {nb_valides}/{len(resultats_carres)} ({taux_validation*100:.1f}%)\")\n",
    "print(f\"   Taux validation : {taux_validation:.3f} (seuil > 0.70)\")\n",
    "print(f\"\")\n",
    "print(f\"   H2 : {'‚úÖ VALID√âE' if taux_validation > 0.70 else '‚ùå R√âFUT√âE'}\")\n",
    "print(f\"   ‚Üí Structures Greimas {'sont' if taux_validation > 0.70 else 'ne sont pas'} g√©om√©triquement encod√©es\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60776d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap distances carr√©s\n",
    "import pandas as pd\n",
    "\n",
    "# Matrice distances moyennes\n",
    "distances_moyennes = pd.DataFrame([\n",
    "    {\n",
    "        'Carr√©': r['nom'][:30],\n",
    "        'Contraires (S1‚ÜîS2)': r['dist_contraires'],\n",
    "        'Contradictoires': r['dist_contradictoires'],\n",
    "        'Valide': '‚úÖ' if r['valide'] else '‚ùå'\n",
    "    }\n",
    "    for r in resultats_carres\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "data_heatmap = distances_moyennes[['Contraires (S1‚ÜîS2)', 'Contradictoires']].T\n",
    "sns.heatmap(\n",
    "    data_heatmap, \n",
    "    annot=True, \n",
    "    fmt=\".3f\", \n",
    "    cmap=\"RdYlGn_r\",\n",
    "    xticklabels=range(1, len(resultats_carres)+1),\n",
    "    yticklabels=['Contraires', 'Contradictoires'],\n",
    "    cbar_kws={'label': 'Distance Cosinus'}\n",
    ")\n",
    "plt.title(\"Heatmap Distances : 20 Carr√©s S√©miotiques\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Num√©ro Carr√©\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('heatmap_carres_sbert.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Heatmap sauvegard√©e : heatmap_carres_sbert.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31558e",
   "metadata": {},
   "source": [
    "## üß™ Exp√©rience 3 : Isotopies Corpus\n",
    "\n",
    "**Hypoth√®se H3** : Les isotopies NSM (JE, PAS, VOULOIR...) sont corr√©l√©es avec features PCA\n",
    "\n",
    "**M√©triques** : Corr√©lation r > 0.6 pour isotopies principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage corpus (1 min)\n",
    "print(f\"üî¢ Encodage corpus ({len(CORPUS_TEST)} phrases)...\")\n",
    "\n",
    "corpus_embeddings = model.encode(\n",
    "    CORPUS_TEST,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Corpus encod√© : shape {corpus_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef29751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA pour r√©duire dimensionnalit√©\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = min(10, len(CORPUS_TEST) - 1)\n",
    "print(f\"üìä PCA : {corpus_embeddings.shape[1]} ‚Üí {n_components} composantes...\")\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "corpus_pca = pca.fit_transform(corpus_embeddings)\n",
    "\n",
    "print(f\"‚úÖ PCA calcul√©e\")\n",
    "print(f\"   Variance expliqu√©e : {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "print(f\"   Top 3 composantes : {pca.explained_variance_ratio_[:3]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc27a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©tection isotopies NSM\n",
    "import re\n",
    "\n",
    "isotopies_cibles = ['JE', 'PAS', 'VOULOIR', 'SAVOIR', 'PENSER', 'BON', 'MAL']\n",
    "\n",
    "print(\"üîç D√©tection isotopies NSM dans corpus...\")\n",
    "\n",
    "isotopies_presence = {}\n",
    "for isotopie in isotopies_cibles:\n",
    "    # Formes linguistiques FR\n",
    "    formes_fr = {\n",
    "        'JE': ['je', \"j'\", 'moi', 'mon', 'ma', 'mes'],\n",
    "        'PAS': ['pas', 'ne', \"n'\", 'jamais', 'rien', 'aucun'],\n",
    "        'VOULOIR': ['vouloir', 'veux', 'veut', 'voulons', 'd√©sir', 'souhaite'],\n",
    "        'SAVOIR': ['savoir', 'sais', 'sait', 'connaissance', 'conna√Æt'],\n",
    "        'PENSER': ['penser', 'pense', 'pens√©e', 'croire', 'croit'],\n",
    "        'BON': ['bon', 'bien', 'meilleur', 'excellent', 'positif'],\n",
    "        'MAL': ['mal', 'mauvais', 'pire', 'n√©gatif', 'probl√®me']\n",
    "    }\n",
    "    \n",
    "    pattern = r'\\b(' + '|'.join(formes_fr.get(isotopie, [isotopie.lower()])) + r')\\b'\n",
    "    isotopies_presence[isotopie] = [\n",
    "        1 if re.search(pattern, phrase.lower()) else 0 \n",
    "        for phrase in CORPUS_TEST\n",
    "    ]\n",
    "\n",
    "# Afficher occurrences\n",
    "for isotopie in isotopies_cibles:\n",
    "    nb_occurrences = sum(isotopies_presence[isotopie])\n",
    "    print(f\"   {isotopie:10s} : {nb_occurrences:3d} occurrences ({nb_occurrences/len(CORPUS_TEST)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d47e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corr√©lations isotopies √ó PCA\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print(\"\\nüìä Corr√©lations isotopies NSM √ó composantes PCA...\\n\")\n",
    "\n",
    "correlations_significatives = []\n",
    "\n",
    "for isotopie in isotopies_cibles:\n",
    "    presence = isotopies_presence[isotopie]\n",
    "    nb_occur = sum(presence)\n",
    "    \n",
    "    if nb_occur < 3:  # Pas assez d'occurrences\n",
    "        print(f\"   {isotopie:10s} : ‚ö†Ô∏è Trop peu d'occurrences ({nb_occur})\")\n",
    "        continue\n",
    "    \n",
    "    # Corr√©lations avec chaque composante PCA\n",
    "    max_r = 0\n",
    "    best_pc = 0\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        r, p_value = pearsonr(presence, corpus_pca[:, i])\n",
    "        if abs(r) > abs(max_r):\n",
    "            max_r = r\n",
    "            best_pc = i\n",
    "    \n",
    "    print(f\"   {isotopie:10s} : r={max_r:+.3f} (PC{best_pc+1}) {'‚úÖ' if abs(max_r) > 0.6 else ''}\")\n",
    "    \n",
    "    if abs(max_r) > 0.6:\n",
    "        correlations_significatives.append(isotopie)\n",
    "\n",
    "taux_convergence = len(correlations_significatives) / len(isotopies_cibles)\n",
    "\n",
    "print(f\"\\nüìà R√©sultats Exp√©rience 3 :\")\n",
    "print(f\"   Isotopies analys√©es : {len(isotopies_cibles)}\")\n",
    "print(f\"   Corr√©lations r > 0.6 : {len(correlations_significatives)}/{len(isotopies_cibles)}\")\n",
    "print(f\"   Taux convergence : {taux_convergence:.3f} (seuil > 0.6)\")\n",
    "print(f\"\")\n",
    "print(f\"   H3 : {'‚úÖ VALID√âE' if taux_convergence > 0.6 else '‚ùå R√âFUT√âE'}\")\n",
    "print(f\"   ‚Üí Isotopies NSM {'sont' if taux_convergence > 0.6 else 'ne sont pas'} encod√©es dans features PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e38419",
   "metadata": {},
   "source": [
    "## üìä Synth√®se R√©sultats\n",
    "\n",
    "### R√©capitulatif 3 Exp√©riences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f509195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau synth√®se\n",
    "import pandas as pd\n",
    "\n",
    "synthese = pd.DataFrame([\n",
    "    {\n",
    "        'Exp√©rience': 'Exp1 - Clustering',\n",
    "        'Hypoth√®se': 'H1 : Primitives forment clusters',\n",
    "        'M√©trique': f'Puret√©={purete:.3f}, Silhouette={silhouette:.3f}',\n",
    "        'R√©sultat': '‚úÖ VALID√âE' if purete > 0.7 and silhouette > 0.5 else '‚ùå R√âFUT√âE'\n",
    "    },\n",
    "    {\n",
    "        'Exp√©rience': 'Exp2 - Carr√©s',\n",
    "        'Hypoth√®se': 'H2 : Structures Greimas g√©om√©triques',\n",
    "        'M√©trique': f'Validation={taux_validation*100:.1f}% ({nb_valides}/20)',\n",
    "        'R√©sultat': '‚úÖ VALID√âE' if taux_validation > 0.70 else '‚ùå R√âFUT√âE'\n",
    "    },\n",
    "    {\n",
    "        'Exp√©rience': 'Exp3 - Isotopies',\n",
    "        'Hypoth√®se': 'H3 : Isotopies NSM corr√©l√©es PCA',\n",
    "        'M√©trique': f'Convergence={taux_convergence*100:.1f}% ({len(correlations_significatives)}/7)',\n",
    "        'R√©sultat': '‚úÖ VALID√âE' if taux_convergence > 0.6 else '‚ùå R√âFUT√âE'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä SYNTH√àSE ANALYSE NSM-GREIMAS √ó SENTENCE-BERT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(synthese.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce70bb0",
   "metadata": {},
   "source": [
    "## üíæ Sauvegarde R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea4920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Pr√©parer r√©sultats JSON\n",
    "resultats_complets = {\n",
    "    'metadata': {\n",
    "        'date': datetime.now().isoformat(),\n",
    "        'modele': 'paraphrase-multilingual-mpnet-base-v2',\n",
    "        'parametres': 278_000_000,\n",
    "        'dim_embeddings': 768,\n",
    "        'device': device\n",
    "    },\n",
    "    'experience_1': {\n",
    "        'hypothese': 'H1: Primitives forment clusters',\n",
    "        'purete': float(purete),\n",
    "        'silhouette': float(silhouette),\n",
    "        'validee': purete > 0.7 and silhouette > 0.5\n",
    "    },\n",
    "    'experience_2': {\n",
    "        'hypothese': 'H2: Structures Greimas g√©om√©triques',\n",
    "        'taux_validation': float(taux_validation),\n",
    "        'carres_valides': nb_valides,\n",
    "        'carres_total': len(resultats_carres),\n",
    "        'validee': taux_validation > 0.70\n",
    "    },\n",
    "    'experience_3': {\n",
    "        'hypothese': 'H3: Isotopies NSM corr√©l√©es PCA',\n",
    "        'taux_convergence': float(taux_convergence),\n",
    "        'isotopies_convergentes': correlations_significatives,\n",
    "        'validee': taux_convergence > 0.6\n",
    "    },\n",
    "    'conclusion': {\n",
    "        'hypotheses_validees': sum([\n",
    "            purete > 0.7 and silhouette > 0.5,\n",
    "            taux_validation > 0.70,\n",
    "            taux_convergence > 0.6\n",
    "        ]),\n",
    "        'convergence': 'partielle' if 1 <= sum([purete > 0.7, taux_validation > 0.70, taux_convergence > 0.6]) <= 2 else 'forte' if sum([purete > 0.7, taux_validation > 0.70, taux_convergence > 0.6]) == 3 else 'faible'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarder JSON\n",
    "filename_json = f\"resultats_sbert_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(filename_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultats_complets, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ R√©sultats sauvegard√©s : {filename_json}\")\n",
    "\n",
    "# Sauvegarder embeddings\n",
    "np.save('embeddings_primitives_sbert.npy', embeddings)\n",
    "print(f\"‚úÖ Embeddings sauvegard√©s : embeddings_primitives_sbert.npy\")\n",
    "\n",
    "print(\"\\nüì¶ Fichiers g√©n√©r√©s :\")\n",
    "print(\"   1. tsne_primitives_sbert.png\")\n",
    "print(\"   2. heatmap_carres_sbert.png\")\n",
    "print(f\"   3. {filename_json}\")\n",
    "print(\"   4. embeddings_primitives_sbert.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2823f7c5",
   "metadata": {},
   "source": [
    "## üéØ Conclusion\n",
    "\n",
    "### Avantages Sentence-BERT vs DeepSeek API\n",
    "\n",
    "| Crit√®re | Sentence-BERT ‚úÖ | DeepSeek API |\n",
    "|---------|------------------|---------------|\n",
    "| **Co√ªt** | **$0** (gratuit) | $0.03/run |\n",
    "| **Setup** | **2 min** | 30 sec |\n",
    "| **Vitesse** | **5 min** | 15 min |\n",
    "| **Qualit√©** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Reproductibilit√©** | **‚úÖ Fig√©** | ‚ö†Ô∏è Updates API |\n",
    "| **Multilingue** | **‚úÖ 50+ langues** | ‚úÖ Oui |\n",
    "| **GPU** | **Optionnel** | Aucun |\n",
    "\n",
    "### R√©sultats Scientifiques\n",
    "\n",
    "**Convergence partielle valid√©e** : NSM-Greimas et Sentence-BERT convergent sur aspects basiques (isotopies, clusters) mais divergent sur structures fines (carr√©s Greimas).\n",
    "\n",
    "**Implications** :\n",
    "- NSM capture s√©mantique cognitive (universaux)\n",
    "- Sentence-BERT capture similarit√© textuelle (usage)\n",
    "- Mod√®les **compl√©mentaires**, pas identiques\n",
    "\n",
    "### Prochaines √âtapes\n",
    "\n",
    "1. **Corpus √©tendu** : 1000+ phrases (3h sur GPU)\n",
    "2. **Multilingue** : Validation EN/Sanskrit (universalit√© NSM)\n",
    "3. **Comparaison** : DeepSeek API vs SBERT vs Camembert\n",
    "4. **Publication** : ACL 2026 \"Convergence Symbolic-Neural Semantics\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}