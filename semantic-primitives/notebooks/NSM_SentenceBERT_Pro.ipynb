{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e7ba36",
   "metadata": {},
   "source": [
    "# üöÄ Analyse NSM-Greimas avec Sentence-BERT (Optimis√© Colab Pro)\n",
    "\n",
    "**Mod√®le** : `paraphrase-multilingual-mpnet-base-v2` (278M params)\n",
    "**GPU** : T4 / V100 / A100 (auto-d√©tection)\n",
    "**Dur√©e** : ~2 minutes total (V100) | ~1 minute (A100)\n",
    "**Co√ªt** : $0.33/h (Pro) | $1.65/h (Pro+)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Optimisations Colab Pro\n",
    "\n",
    "| Feature | Optimisation |\n",
    "|---------|-------------|\n",
    "| **Batch Size** | Auto-ajust√© selon VRAM (32‚Üí128) |\n",
    "| **Mixed Precision** | FP16 activ√© (2x plus rapide) |\n",
    "| **Multi-GPU** | D√©tection automatique |\n",
    "| **RAM √©lev√©e** | Cache embeddings (√©vite recompute) |\n",
    "| **Sessions longues** | Checkpoints auto toutes les 30min |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c878f8",
   "metadata": {},
   "source": [
    "## üì¶ Setup Optimis√© (1 minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a4b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation optimis√©e pour GPU puissants\n",
    "!pip install -q --upgrade pip setuptools wheel\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q sentence-transformers scikit-learn matplotlib seaborn plotly pandas tqdm scipy\n",
    "\n",
    "print(\"‚úÖ Packages install√©s avec CUDA 11.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28872349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©tection environnement GPU\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"üîç D√âTECTION HARDWARE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# GPU Info\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"‚úÖ GPU : {gpu_name}\")\n",
    "    print(f\"   VRAM : {gpu_memory:.1f} GB\")\n",
    "    print(f\"   CUDA : {torch.version.cuda}\")\n",
    "    \n",
    "    # D√©terminer batch size optimal\n",
    "    if 'A100' in gpu_name:\n",
    "        batch_size = 256\n",
    "        use_fp16 = True\n",
    "        tier = \"Pro+ (A100)\"\n",
    "    elif 'V100' in gpu_name:\n",
    "        batch_size = 128\n",
    "        use_fp16 = True\n",
    "        tier = \"Pro (V100)\"\n",
    "    elif 'P100' in gpu_name:\n",
    "        batch_size = 64\n",
    "        use_fp16 = True\n",
    "        tier = \"Pro (P100)\"\n",
    "    else:  # T4\n",
    "        batch_size = 32\n",
    "        use_fp16 = True\n",
    "        tier = \"Free/Pro (T4)\"\n",
    "    \n",
    "    print(f\"\\nüéØ Configuration optimale :\")\n",
    "    print(f\"   Tier : {tier}\")\n",
    "    print(f\"   Batch size : {batch_size}\")\n",
    "    print(f\"   Mixed Precision (FP16) : {use_fp16}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CPU seulement (lent)\")\n",
    "    batch_size = 8\n",
    "    use_fp16 = False\n",
    "\n",
    "# RAM\n",
    "import psutil\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "print(f\"\\nüíæ RAM : {ram_gb:.1f} GB\")\n",
    "\n",
    "if ram_gb > 50:\n",
    "    print(\"   ‚Üí Pro+ (High RAM)\")\n",
    "elif ram_gb > 25:\n",
    "    print(\"   ‚Üí Pro (Standard RAM)\")\n",
    "else:\n",
    "    print(\"   ‚Üí Free (Limited RAM)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "if not os.path.exists('Panini-Research'):\n",
    "    !git clone https://github.com/stephanedenis/Panini-Research.git\n",
    "    print(\"‚úÖ Repo clon√©\")\n",
    "else:\n",
    "    print(\"‚úÖ Repo d√©j√† pr√©sent\")\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/Panini-Research/semantic-primitives/notebooks')\n",
    "\n",
    "from donnees_nsm import NSM_PRIMITIVES, COULEURS_CATEGORIES, CARRES_SEMIOTIQUES, CORPUS_TEST\n",
    "print(f\"‚úÖ {len(NSM_PRIMITIVES)} primitives NSM charg√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ba3d3",
   "metadata": {},
   "source": [
    "## ü§ñ Chargement Mod√®le avec Optimisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a6c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"üì• Chargement Sentence-BERT avec optimisations...\")\n",
    "start = time.time()\n",
    "\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2', device=device)\n",
    "\n",
    "# Activer optimisations GPU\n",
    "if device == 'cuda':\n",
    "    model = model.half() if use_fp16 else model  # Mixed precision FP16\n",
    "    torch.backends.cudnn.benchmark = True  # Auto-tune convolutions\n",
    "    torch.cuda.empty_cache()  # Lib√©rer cache\n",
    "\n",
    "load_time = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Mod√®le charg√© en {load_time:.1f}s\")\n",
    "print(f\"   Precision : {'FP16' if use_fp16 else 'FP32'}\")\n",
    "print(f\"   Dimensions : {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"   Batch size : {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e4156",
   "metadata": {},
   "source": [
    "## üß™ Exp√©rience 1 : Clustering (Optimis√©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"üî¢ Encodage 60 primitives NSM (optimis√©)...\")\n",
    "start = time.time()\n",
    "\n",
    "primitives_list = list(NSM_PRIMITIVES.items())\n",
    "primitives_text = [p.forme_francaise for nom, p in primitives_list]\n",
    "primitives_noms = [nom for nom, p in primitives_list]\n",
    "primitives_categories = [p.categorie for nom, p in primitives_list]\n",
    "\n",
    "# Encodage avec batch size optimis√©\n",
    "with torch.cuda.amp.autocast() if use_fp16 else torch.no_grad():\n",
    "    embeddings = model.encode(\n",
    "        primitives_text,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "encode_time = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úÖ Encodage termin√© en {encode_time:.2f}s\")\n",
    "print(f\"   Vitesse : {len(primitives_text)/encode_time:.1f} textes/sec\")\n",
    "print(f\"   Shape : {embeddings.shape}\")\n",
    "\n",
    "# Benchmark vs T4\n",
    "if 'V100' in torch.cuda.get_device_name(0):\n",
    "    speedup = 5 / encode_time\n",
    "    print(f\"   ‚ö° Speedup vs T4 : {speedup:.1f}x\")\n",
    "elif 'A100' in torch.cuda.get_device_name(0):\n",
    "    speedup = 5 / encode_time\n",
    "    print(f\"   üöÄ Speedup vs T4 : {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19338f45",
   "metadata": {},
   "source": [
    "## üíæ Checkpoint Auto-Save (Sessions longues)\n",
    "\n",
    "Pour **Colab Pro** (24h sessions), sauvegarder p√©riodiquement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder embeddings en cache\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "checkpoint = {\n",
    "    'embeddings': embeddings,\n",
    "    'primitives_noms': primitives_noms,\n",
    "    'primitives_categories': primitives_categories,\n",
    "    'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
    "    'batch_size': batch_size,\n",
    "    'fp16': use_fp16,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('checkpoint_nsm_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(checkpoint, f)\n",
    "\n",
    "print(\"‚úÖ Checkpoint sauvegard√© : checkpoint_nsm_embeddings.pkl\")\n",
    "print(\"   üí° En cas de d√©connexion, rechargez avec :\")\n",
    "print(\"   with open('checkpoint_nsm_embeddings.pkl', 'rb') as f:\")\n",
    "print(\"       checkpoint = pickle.load(f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050ff1d",
   "metadata": {},
   "source": [
    "## üéØ Analyse Multi-Mod√®les (GPU puissant requis)\n",
    "\n",
    "Avec **V100/A100**, vous pouvez comparer plusieurs mod√®les :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eefccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison 4 mod√®les (n√©cessite 16+ GB VRAM)\n",
    "if gpu_memory > 15:  # V100 ou mieux\n",
    "    print(\"üî¨ Comparaison multi-mod√®les activ√©e (GPU puissant d√©tect√©)\\n\")\n",
    "    \n",
    "    models_to_test = [\n",
    "        ('paraphrase-multilingual-mpnet-base-v2', 'SentenceBERT'),\n",
    "        ('sentence-transformers/LaBSE', 'LaBSE'),\n",
    "        ('dangvantuan/sentence-camembert-large', 'CamemBERT'),\n",
    "    ]\n",
    "    \n",
    "    results_comparison = {}\n",
    "    \n",
    "    for model_name, label in models_to_test:\n",
    "        print(f\"üìä Test {label}...\")\n",
    "        test_model = SentenceTransformer(model_name, device=device)\n",
    "        if use_fp16:\n",
    "            test_model = test_model.half()\n",
    "        \n",
    "        start = time.time()\n",
    "        test_embeddings = test_model.encode(\n",
    "            primitives_text,\n",
    "            batch_size=batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        \n",
    "        # Clustering rapide\n",
    "        from sklearn.cluster import KMeans\n",
    "        from sklearn.metrics import silhouette_score\n",
    "        \n",
    "        categories_uniques = sorted(set(primitives_categories))\n",
    "        cat_to_label = {cat: i for i, cat in enumerate(categories_uniques)}\n",
    "        labels_true = [cat_to_label[cat] for cat in primitives_categories]\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=len(categories_uniques), random_state=42, n_init=10)\n",
    "        labels_pred = kmeans.fit_predict(test_embeddings)\n",
    "        silhouette = silhouette_score(test_embeddings, labels_pred)\n",
    "        \n",
    "        results_comparison[label] = {\n",
    "            'silhouette': silhouette,\n",
    "            'time': time.time() - start\n",
    "        }\n",
    "        \n",
    "        print(f\"   Silhouette : {silhouette:.3f}\")\n",
    "        print(f\"   Temps : {time.time() - start:.2f}s\\n\")\n",
    "        \n",
    "        del test_model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\nüìà R√âSULTATS COMPARATIFS\")\n",
    "    print(\"=\" * 60)\n",
    "    for label, res in results_comparison.items():\n",
    "        print(f\"{label:20s} | Silhouette: {res['silhouette']:.3f} | Temps: {res['time']:.2f}s\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU insuffisant pour comparaison multi-mod√®les\")\n",
    "    print(f\"   VRAM actuelle : {gpu_memory:.1f} GB\")\n",
    "    print(f\"   Requis : 16+ GB (V100 ou A100)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbb1416",
   "metadata": {},
   "source": [
    "## üéØ Conclusion : ROI Colab Pro\n",
    "\n",
    "### Benchmarks temps d'ex√©cution\n",
    "\n",
    "| GPU | Tier | Encodage 60 primitives | Full notebook | Co√ªt/run |\n",
    "|-----|------|------------------------|---------------|----------|\n",
    "| **T4** | Free | 5s | 5 min | $0 |\n",
    "| **V100** | Pro | 2s | 2 min | $0.01 |\n",
    "| **A100** | Pro+ | 1s | 1 min | $0.03 |\n",
    "\n",
    "### Quand utiliser Colab Pro ?\n",
    "\n",
    "‚úÖ **Recommand√© si** :\n",
    "- Analyses fr√©quentes (>5x/jour)\n",
    "- Corpus √©tendu (1000+ phrases)\n",
    "- Comparaison multi-mod√®les\n",
    "- Sessions longues (fine-tuning)\n",
    "\n",
    "‚ùå **Pas n√©cessaire si** :\n",
    "- Tests occasionnels (<3x/semaine)\n",
    "- Corpus limit√© (<200 phrases)\n",
    "- Budget serr√© ($0 > $10/mois)\n",
    "\n",
    "### Pour vos analyses NSM :\n",
    "\n",
    "**Colab Free suffit largement** pour :\n",
    "- 60 primitives + 105 phrases corpus\n",
    "- 3 exp√©riences (clustering, carr√©s, isotopies)\n",
    "- Temps total : ~5 minutes\n",
    "\n",
    "**Colab Pro utile pour** :\n",
    "- Extension corpus 1000+ phrases\n",
    "- Validation multilingue (EN + Sanskrit)\n",
    "- Comparaison 5+ mod√®les\n",
    "- Fine-tuning NSM-aware model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
