{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5471ea9c",
   "metadata": {},
   "source": [
    "# üîó Connexion VS Code ‚Üî Colab GPU Pro\n",
    "\n",
    "**Objectif** : Connecter VS Code local √† VM Colab (GPU L4/A100 avec Pro)\n",
    "\n",
    "**R√©sultat** :\n",
    "- ‚úÖ √âdition dans VS Code (confort, Copilot, breakpoints)\n",
    "- ‚úÖ Ex√©cution sur GPU Colab **L4 (24GB)** ou **A100 (40GB)**\n",
    "- ‚úÖ Session 24h continue (vs 12h gratuit)\n",
    "- ‚úÖ Tous vos fichiers locaux accessibles\n",
    "- ‚úÖ Auto-d√©tection GPU + optimisation batch size\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ GPU disponibles avec Colab Pro\n",
    "\n",
    "| GPU | VRAM | Vitesse NSM | Usage recommand√© |\n",
    "|-----|------|-------------|------------------|\n",
    "| T4 | 16 GB | 5 min | Debug/test rapide |\n",
    "| **L4** | **24 GB** | **3 min** | ‚≠ê **Optimal (rapport qualit√©/prix)** |\n",
    "| **A100** | **40 GB** | **1.5 min** | üöÄ **Production/benchmarks** |\n",
    "\n",
    "**TPU v5e/v6e** : D√©conseill√© pour PyTorch (SentenceBERT incompatible)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Pr√©requis\n",
    "\n",
    "**Sur VS Code local** (vous avez d√©j√†) :\n",
    "- ‚úÖ VS Code install√©\n",
    "- ‚úÖ Extension \"Remote - SSH\" install√©e\n",
    "\n",
    "**Sur Colab** (√† ex√©cuter ci-dessous) :\n",
    "- Installation `colab-ssh`\n",
    "- Cr√©ation tunnel Cloudflare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775d224",
   "metadata": {},
   "source": [
    "## üöÄ √âtape 1 : Installation et tunnel SSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb9147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation colab-ssh\n",
    "!pip install -q colab-ssh\n",
    "\n",
    "print(\"‚úÖ colab-ssh install√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2583bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer tunnel SSH avec Cloudflare\n",
    "from colab_ssh import launch_ssh_cloudflared\n",
    "import getpass\n",
    "\n",
    "# D√©finir mot de passe pour connexion\n",
    "password = \"panini2025\"  # ‚ö†Ô∏è Changez ce mot de passe !\n",
    "\n",
    "print(\"üîß Lancement tunnel SSH...\")\n",
    "print(\"‚è≥ Patientez 30 secondes...\\n\")\n",
    "\n",
    "# Lancer SSH\n",
    "launch_ssh_cloudflared(password=password)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TUNNEL SSH ACTIF\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìã INSTRUCTIONS POUR VS CODE :\")\n",
    "print(\"\\n1. Copiez la commande SSH ci-dessus (commence par 'ssh root@...')\")\n",
    "print(\"2. Dans VS Code :\")\n",
    "print(\"   - Ouvrir Command Palette (Ctrl+Shift+P)\")\n",
    "print(\"   - Taper 'Remote-SSH: Connect to Host...'\")\n",
    "print(\"   - Coller la commande SSH\")\n",
    "print(\"   - Entrer mot de passe : panini2025\")\n",
    "print(\"\\n3. Une fois connect√© :\")\n",
    "print(\"   - Ouvrir Terminal VS Code ‚Üí vous √™tes sur VM Colab !\")\n",
    "print(\"   - GPU T4 disponible : nvidia-smi\")\n",
    "print(\"   - Cloner repo : git clone https://github.com/stephanedenis/Panini-Research.git\")\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT : Gardez cette fen√™tre Colab ouverte !\")\n",
    "print(\"   Si vous fermez Colab ‚Üí connexion VS Code perdue\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926da59",
   "metadata": {},
   "source": [
    "## üîç V√©rification GPU disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2919ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier GPU et recommandations\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç D√âTECTION GPU ET OPTIMISATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"\\n‚úÖ PyTorch d√©tecte GPU : {gpu_name}\")\n",
    "    print(f\"   VRAM : {vram_gb:.1f} GB\")\n",
    "    \n",
    "    # Recommandations selon GPU\n",
    "    if \"A100\" in gpu_name:\n",
    "        batch_size = 256\n",
    "        temps_estime = \"1.5 min\"\n",
    "        recommandation = \"üöÄ PRODUCTION - Optimal pour benchmarks multi-mod√®les\"\n",
    "    elif \"L4\" in gpu_name:\n",
    "        batch_size = 128\n",
    "        temps_estime = \"3 min\"\n",
    "        recommandation = \"‚≠ê OPTIMAL - Meilleur rapport qualit√©/prix\"\n",
    "    elif \"T4\" in gpu_name:\n",
    "        batch_size = 64\n",
    "        temps_estime = \"5 min\"\n",
    "        recommandation = \"‚úÖ DEBUG - Rapide pour tests\"\n",
    "    elif \"V100\" in gpu_name:\n",
    "        batch_size = 128\n",
    "        temps_estime = \"3.5 min\"\n",
    "        recommandation = \"‚úÖ PRODUCTION - Alternative A100\"\n",
    "    elif \"P100\" in gpu_name:\n",
    "        batch_size = 96\n",
    "        temps_estime = \"4 min\"\n",
    "        recommandation = \"‚ö†Ô∏è LEGACY - Upgrade vers L4 recommand√©\"\n",
    "    else:\n",
    "        batch_size = 32\n",
    "        temps_estime = \"8 min\"\n",
    "        recommandation = \"‚ö†Ô∏è GPU inconnu - Param√®tres conservateurs\"\n",
    "    \n",
    "    print(f\"\\nüìä PARAM√àTRES OPTIMAUX pour NSM (60 primitives) :\")\n",
    "    print(f\"   Batch size : {batch_size}\")\n",
    "    print(f\"   Temps estim√© : {temps_estime}\")\n",
    "    print(f\"   Recommandation : {recommandation}\")\n",
    "    \n",
    "    # Export variable pour usage ult√©rieur\n",
    "    import os\n",
    "    os.environ['NSM_BATCH_SIZE'] = str(batch_size)\n",
    "    print(f\"\\n‚úÖ Variable NSM_BATCH_SIZE={batch_size} export√©e\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå Aucun GPU d√©tect√©\")\n",
    "    print(\"   üí° Solution : Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    print(\"   üí° Pour Pro : Choisir L4 ou A100 dans les options\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1538876",
   "metadata": {},
   "source": [
    "## üì¶ Installation environnement NSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer packages n√©cessaires\n",
    "!pip install -q sentence-transformers scikit-learn matplotlib seaborn plotly pandas tqdm scipy\n",
    "\n",
    "print(\"‚úÖ Environnement Python pr√™t\")\n",
    "print(\"\\nüí° Maintenant dans VS Code :\")\n",
    "print(\"   1. Ouvrir Terminal\")\n",
    "print(\"   2. git clone https://github.com/stephanedenis/Panini-Research.git\")\n",
    "print(\"   3. cd Panini-Research/semantic-primitives/notebooks\")\n",
    "print(\"   4. Ouvrir NSM_SentenceBERT_Local.ipynb\")\n",
    "print(\"   5. Ex√©cuter cellules ‚Üí GPU T4 utilis√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951648e5",
   "metadata": {},
   "source": [
    "## ‚è∞ Maintenir session active\n",
    "\n",
    "Cette cellule emp√™che Colab de d√©connecter apr√®s 90 min d'inactivit√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58763e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep-alive automatique\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"üîÑ Keep-alive actif\")\n",
    "print(\"üí° Cette cellule maintient la session Colab active\")\n",
    "print(\"‚ö†Ô∏è NE PAS ARR√äTER cette cellule tant que vous travaillez dans VS Code\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(300)  # Ping toutes les 5 min\n",
    "        clear_output(wait=True)\n",
    "        current_time = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"‚úÖ Session active - {current_time}\")\n",
    "        print(\"üíª VS Code connect√© via SSH\")\n",
    "        print(\"üîó Tunnel : ACTIF\")\n",
    "        print(\"‚è±Ô∏è Prochain ping dans 5 minutes...\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Keep-alive arr√™t√©\")\n",
    "    print(\"   Session Colab peut se d√©connecter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a80e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Workflow complet\n",
    "\n",
    "### Dans Colab (cette fen√™tre) :\n",
    "1. ‚úÖ Ex√©cuter cellule 2 ‚Üí tunnel SSH cr√©√©\n",
    "2. ‚úÖ Ex√©cuter cellule 6 ‚Üí keep-alive actif\n",
    "3. ‚ö†Ô∏è **Laisser fen√™tre ouverte** (minimiser OK, fermer = d√©connexion)\n",
    "\n",
    "### Dans VS Code (votre machine) :\n",
    "1. `Ctrl+Shift+P` ‚Üí \"Remote-SSH: Connect to Host\"\n",
    "2. Coller commande SSH affich√©e dans cellule 2\n",
    "3. Mot de passe : `panini2025`\n",
    "4. Nouvelle fen√™tre VS Code s'ouvre ‚Üí **vous √™tes sur VM Colab !**\n",
    "5. Terminal VS Code : `git clone https://github.com/stephanedenis/Panini-Research.git`\n",
    "6. Ouvrir `NSM_SentenceBERT_Local.ipynb`\n",
    "7. Ex√©cuter cellules ‚Üí GPU T4 utilis√© automatiquement\n",
    "\n",
    "### Avantages :\n",
    "- ‚úÖ **√âdition** : VS Code (Copilot, breakpoints, Git)\n",
    "- ‚úÖ **Ex√©cution** : GPU T4 Colab (gratuit, rapide)\n",
    "- ‚úÖ **Debug** : Breakpoints fonctionnent sur code GPU\n",
    "- ‚úÖ **Fichiers** : Acc√®s direct aux fichiers Colab\n",
    "- ‚úÖ **Terminal** : Comme si vous √©tiez en local\n",
    "\n",
    "### Limitations :\n",
    "- ‚ö†Ô∏è Session 12h max (puis reconnexion)\n",
    "- ‚ö†Ô∏è Latence r√©seau (100-300ms)\n",
    "- ‚ö†Ô∏è Fen√™tre Colab doit rester ouverte\n",
    "\n",
    "---\n",
    "\n",
    "## üêõ Troubleshooting\n",
    "\n",
    "**Probl√®me** : \"Connection timed out\"\n",
    "- Solution : R√©ex√©cuter cellule 2 (tunnel SSH)\n",
    "\n",
    "**Probl√®me** : \"Permission denied\"\n",
    "- Solution : V√©rifier mot de passe (panini2025)\n",
    "\n",
    "**Probl√®me** : \"Tunnel closed\"\n",
    "- Solution : Colab d√©connect√© ‚Üí r√©ex√©cuter notebook depuis d√©but\n",
    "\n",
    "**Probl√®me** : \"GPU not available\" dans VS Code\n",
    "- Solution : V√©rifier Runtime ‚Üí Change runtime type ‚Üí GPU\n",
    "- Pro : Choisir L4 ou A100 dans menu d√©roulant\n",
    "\n",
    "**Probl√®me** : \"Out of memory\" sur T4\n",
    "- Solution : R√©duire batch_size (32 au lieu de 64)\n",
    "- Ou : Upgrade vers L4 (24GB) ou A100 (40GB)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Benchmark performances (60 primitives NSM)\n",
    "\n",
    "| Environnement | Temps encodage | Batch size | Co√ªt |\n",
    "|---------------|----------------|------------|------|\n",
    "| **Local CPU** | 30 sec | 16 | $0 |\n",
    "| **Colab T4** (gratuit) | 5 sec | 64 | $0 |\n",
    "| **Colab L4** (Pro) | 3 sec | 128 | $10/mois |\n",
    "| **Colab A100** (Pro) | 1.5 sec | 256 | $10/mois |\n",
    "\n",
    "**Latence SSH** : +0.5 sec (n√©gligeable)\n",
    "\n",
    "**Recommandation** :\n",
    "- Debug/test : **T4 gratuit** (suffisant)\n",
    "- Production/benchmarks : **L4 Pro** (optimal)\n",
    "- Multi-mod√®les : **A100 Pro** (si comparaison 5+ mod√®les simultan√©s)\n",
    "\n",
    "**√âvitez TPU** : PyTorch/SentenceBERT incompatible (TensorFlow only)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
