#!/usr/bin/env python3
"""
INTÃ‰GRATEUR SYSTÃˆMES VALIDATION & QUALITÃ‰
========================================

IntÃ©gration des systÃ¨mes de validation continue et framework qualitÃ©
pour un processus unifiÃ© de dÃ©veloppement et amÃ©lioration PanLang.

Objectif: Pipeline automatique validation â†’ analyse â†’ amÃ©lioration â†’ validation
"""

import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path

# Imports des systÃ¨mes
from systeme_validation_continue import ContinuousValidationSystem
from framework_criteres_qualite import PanLangQualityFramework

@dataclass
class IntegratedReport:
    """Rapport intÃ©grÃ© validation + qualitÃ©"""
    timestamp: str
    validation_results: Dict[str, Any]
    quality_assessment: Dict[str, Any]
    integrated_score: float
    priority_actions: List[str]
    improvement_roadmap: Dict[str, Any]
    next_validation_trigger: str

class IntegratedValidationQualitySystem:
    """SystÃ¨me intÃ©grÃ© validation continue + framework qualitÃ©"""
    
    def __init__(self):
        self.workspace_path = Path("/home/stephane/GitHub/PaniniFS-Research")
        self.integration_dir = self.workspace_path / "validation_integree"
        self.integration_dir.mkdir(exist_ok=True)
        
        # Initialisation sous-systÃ¨mes
        self.validator = ContinuousValidationSystem()
        self.quality_framework = PanLangQualityFramework()
        
        # Correction poids framework qualitÃ©
        self._fix_quality_framework_weights()
        
        # Historique intÃ©grÃ©
        self.integrated_history = self.integration_dir / "historique_integre.jsonl"
        self.reports_dir = self.integration_dir / "rapports"
        self.reports_dir.mkdir(exist_ok=True)
    
    def _fix_quality_framework_weights(self):
        """Corrige les poids du framework qualitÃ© pour sommer Ã  1.0"""
        
        total_weight = sum(c.weight for c in self.quality_framework.quality_criteria.values())
        correction_factor = 1.0 / total_weight
        
        print(f"ğŸ”§ CORRECTION POIDS: facteur {correction_factor:.3f}")
        
        for criterion in self.quality_framework.quality_criteria.values():
            criterion.weight *= correction_factor
        
        # VÃ©rification
        new_total = sum(c.weight for c in self.quality_framework.quality_criteria.values())
        print(f"âœ… Nouveau total poids: {new_total:.6f}")
    
    def run_integrated_validation(self, trigger: str = "manual") -> IntegratedReport:
        """ExÃ©cute validation intÃ©grÃ©e complÃ¨te"""
        
        print("ğŸ”„ VALIDATION INTÃ‰GRÃ‰E PANLANG")
        print("=" * 35)
        
        timestamp = datetime.now().isoformat()
        
        # 1. Validation continue technique
        print("ğŸ“Š Phase 1: Validation continue...")
        validation_metrics = self.validator.validate_current_architecture(trigger)
        
        # 2. Ã‰valuation qualitÃ© framework
        print("\nğŸ¯ Phase 2: Ã‰valuation framework qualitÃ©...")
        
        # RÃ©cupÃ©ration modÃ¨le actuel
        current_model = self.quality_framework.benchmark_models["panlang_panksepp_actuel"]
        quality_assessment = self.quality_framework.evaluate_model_candidate(current_model)
        
        # 3. IntÃ©gration scores
        print("\nğŸ”— Phase 3: IntÃ©gration rÃ©sultats...")
        integrated_score = self._calculate_integrated_score(validation_metrics, quality_assessment)
        
        # 4. Analyse prioritÃ©s
        priority_actions = self._analyze_integrated_priorities(validation_metrics, quality_assessment)
        
        # 5. Roadmap amÃ©lioration
        improvement_roadmap = self._generate_integrated_roadmap(validation_metrics, quality_assessment)
        
        # 6. DÃ©clencheur suivant
        next_trigger = self._determine_next_trigger(integrated_score, improvement_roadmap)
        
        # CrÃ©ation rapport intÃ©grÃ©
        integrated_report = IntegratedReport(
            timestamp=timestamp,
            validation_results=asdict(validation_metrics),
            quality_assessment=asdict(quality_assessment),
            integrated_score=integrated_score,
            priority_actions=priority_actions,
            improvement_roadmap=improvement_roadmap,
            next_validation_trigger=next_trigger
        )
        
        # Sauvegarde et affichage
        self._save_integrated_report(integrated_report)
        self._display_integrated_results(integrated_report)
        
        return integrated_report
    
    def _calculate_integrated_score(self, validation_metrics, quality_assessment) -> float:
        """Calcule score intÃ©grÃ© validation + qualitÃ©"""
        
        # Score validation continue (0.764 actuel)
        validation_score = validation_metrics.overall_quality_score
        
        # Score qualitÃ© framework (normalisÃ© Ã  1.0)
        quality_score = min(1.0, quality_assessment.overall_score)
        
        # Moyenne pondÃ©rÃ©e (60% technique, 40% qualitÃ©)
        integrated_score = validation_score * 0.6 + quality_score * 0.4
        
        return integrated_score
    
    def _analyze_integrated_priorities(self, validation_metrics, quality_assessment) -> List[str]:
        """Analyse prioritÃ©s intÃ©grÃ©es"""
        
        priorities = []
        
        # PrioritÃ©s validation continue
        if validation_metrics.multilingual_coverage < 0.6:
            priorities.append("CRITIQUE: Ã‰tendre corpus multilingue (couverture 50%)")
        
        if validation_metrics.cross_linguistic_consistency < 0.75:
            priorities.append("HAUTE: AmÃ©liorer consistance inter-lingue (70%)")
        
        if validation_metrics.semantic_precision < 0.85:
            priorities.append("HAUTE: Affiner prÃ©cision sÃ©mantique (88.1%)")
        
        # PrioritÃ©s framework qualitÃ©  
        quality_gaps = []
        for criterion_name, score in quality_assessment.criterion_scores.items():
            criterion = self.quality_framework.quality_criteria[criterion_name]
            if score < criterion.target_value - 0.05:
                gap = criterion.target_value - score
                weight = criterion.weight
                quality_gaps.append((gap * weight, criterion_name, criterion.name, gap))
        
        quality_gaps.sort(reverse=True)
        
        for weighted_gap, criterion_key, criterion_name, gap in quality_gaps[:3]:
            priorities.append(f"QUALITÃ‰: {criterion_name} (Ã©cart {gap:.3f})")
        
        return priorities[:8]  # Top 8 prioritÃ©s
    
    def _generate_integrated_roadmap(self, validation_metrics, quality_assessment) -> Dict[str, Any]:
        """GÃ©nÃ¨re roadmap intÃ©grÃ©e"""
        
        current_integrated = self._calculate_integrated_score(validation_metrics, quality_assessment)
        
        # Objectifs par domaine
        validation_targets = {
            'multilingual_coverage': 0.85,
            'semantic_precision': 0.90,
            'cross_linguistic_consistency': 0.85,
            'creative_generation_score': 0.80
        }
        
        quality_targets = {
            'universalite_linguistique': 0.90,
            'precision_semantique': 0.92,
            'contraintes_cognitives': 0.95
        }
        
        # Timeline basÃ©e sur Ã©carts
        total_validation_gap = sum(
            max(0, target - getattr(validation_metrics, field))
            for field, target in validation_targets.items()
        )
        
        total_quality_gap = sum(
            max(0, target - quality_assessment.criterion_scores.get(field, 0.5))
            for field, target in quality_targets.items()
        )
        
        if total_validation_gap + total_quality_gap < 0.3:
            timeline = "2-3 semaines"
            effort = "Optimisation fine"
        elif total_validation_gap + total_quality_gap < 0.6:
            timeline = "1-2 mois"  
            effort = "AmÃ©liorations ciblÃ©es"
        else:
            timeline = "2-3 mois"
            effort = "DÃ©veloppements majeurs"
        
        roadmap = {
            'current_integrated_score': current_integrated,
            'target_integrated_score': 0.90,
            'estimated_timeline': timeline,
            'effort_level': effort,
            'validation_priorities': validation_targets,
            'quality_priorities': quality_targets,
            'success_criteria': [
                'Score intÃ©grÃ© > 0.90',
                'Couverture multilingue > 85%',
                'PrÃ©cision sÃ©mantique > 90%',
                'Tous critÃ¨res qualitÃ© au-dessus seuils'
            ]
        }
        
        return roadmap
    
    def _determine_next_trigger(self, integrated_score: float, roadmap: Dict[str, Any]) -> str:
        """DÃ©termine dÃ©clencheur validation suivante"""
        
        if integrated_score < 0.7:
            return "architecture_critical_improvement"
        elif integrated_score < 0.8:
            return "targeted_improvements"
        else:
            return "optimization_cycle"
    
    def _save_integrated_report(self, report: IntegratedReport):
        """Sauvegarde rapport intÃ©grÃ©"""
        
        # Historique JSONL
        with open(self.integrated_history, 'a', encoding='utf-8') as f:
            f.write(json.dumps(asdict(report), ensure_ascii=False) + '\n')
        
        # Rapport dÃ©taillÃ©
        timestamp_str = report.timestamp.replace(':', '-').replace('T', '_')[:19]
        detailed_report_path = self.reports_dir / f"rapport_integre_{timestamp_str}.json"
        
        with open(detailed_report_path, 'w', encoding='utf-8') as f:
            json.dump(asdict(report), f, indent=2, ensure_ascii=False)
        
        # Rapport Markdown lisible
        md_report_path = self.reports_dir / f"rapport_integre_{timestamp_str}.md"
        self._generate_markdown_report(report, md_report_path)
    
    def _generate_markdown_report(self, report: IntegratedReport, output_path: Path):
        """GÃ©nÃ¨re rapport Markdown lisible"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(f"# RAPPORT VALIDATION INTÃ‰GRÃ‰E PANLANG\n\n")
            f.write(f"**Timestamp:** {report.timestamp}  \n")
            f.write(f"**Score IntÃ©grÃ©:** {report.integrated_score:.3f}  \n")
            f.write(f"**Prochain DÃ©clencheur:** {report.next_validation_trigger}  \n\n")
            
            f.write(f"## ğŸ“Š VALIDATION TECHNIQUE\n\n")
            validation = report.validation_results
            f.write(f"- **Score Global:** {validation['overall_quality_score']:.3f}\n")
            f.write(f"- **Couverture Multilingue:** {validation['multilingual_coverage']:.3f}\n")
            f.write(f"- **PrÃ©cision SÃ©mantique:** {validation['semantic_precision']:.3f}\n")
            f.write(f"- **CohÃ©rence Compositionnelle:** {validation['compositional_coherence']:.3f}\n\n")
            
            f.write(f"## ğŸ¯ Ã‰VALUATION QUALITÃ‰\n\n")
            quality = report.quality_assessment
            f.write(f"- **Score Global:** {quality['overall_score']:.3f}\n")
            f.write(f"- **Forces:** {len(quality.get('strengths', []))}\n")
            f.write(f"- **Faiblesses:** {len(quality.get('weaknesses', []))}\n\n")
            
            f.write(f"## ğŸš€ PRIORITÃ‰S D'ACTION\n\n")
            for i, action in enumerate(report.priority_actions, 1):
                f.write(f"{i}. {action}\n")
            f.write(f"\n")
            
            f.write(f"## ğŸ—ºï¸ ROADMAP AMÃ‰LIORATION\n\n")
            roadmap = report.improvement_roadmap
            f.write(f"- **Timeline:** {roadmap['estimated_timeline']}\n")
            f.write(f"- **Effort:** {roadmap['effort_level']}\n")
            f.write(f"- **Score Cible:** {roadmap['target_integrated_score']:.3f}\n\n")
    
    def _display_integrated_results(self, report: IntegratedReport):
        """Affiche rÃ©sultats intÃ©grÃ©s"""
        
        print(f"\nğŸŠ RÃ‰SULTATS VALIDATION INTÃ‰GRÃ‰E")
        print("=" * 40)
        
        print(f"ğŸ† Score IntÃ©grÃ©: {report.integrated_score:.3f}")
        
        if report.integrated_score >= 0.9:
            level = "EXCELLENCE â­â­â­â­"
        elif report.integrated_score >= 0.8:
            level = "TRÃˆS BON â­â­â­"
        elif report.integrated_score >= 0.7:
            level = "BON â­â­"
        else:
            level = "Ã€ AMÃ‰LIORER â­"
        
        print(f"ğŸ–ï¸ Niveau: {level}")
        
        print(f"\nğŸš¨ TOP PRIORITÃ‰S:")
        for i, priority in enumerate(report.priority_actions[:5], 1):
            print(f"   {i}. {priority}")
        
        print(f"\nğŸ—ºï¸ ROADMAP:")
        roadmap = report.improvement_roadmap
        print(f"   â±ï¸ Timeline: {roadmap['estimated_timeline']}")
        print(f"   ğŸ¯ Score cible: {roadmap['target_integrated_score']:.3f}")
        print(f"   ğŸ“ˆ Ã‰cart: {roadmap['target_integrated_score'] - report.integrated_score:.3f}")
        
        print(f"\nğŸ”„ Prochain dÃ©clencheur: {report.next_validation_trigger}")
    
    def monitor_continuous_improvement(self, iterations: int = 5) -> List[IntegratedReport]:
        """Monitoring amÃ©lioration continue"""
        
        print("ğŸ”„ MONITORING AMÃ‰LIORATION CONTINUE")
        print("=" * 40)
        
        reports = []
        
        for i in range(iterations):
            print(f"\nğŸ“‹ ITÃ‰RATION {i+1}/{iterations}")
            
            trigger = "continuous_monitoring" if i > 0 else "baseline"
            report = self.run_integrated_validation(trigger)
            
            reports.append(report)
            
            # Simulation amÃ©lioration (en rÃ©alitÃ©, changements seraient appliquÃ©s)
            if i < iterations - 1:
                print(f"â³ Simulation application amÃ©liorations...")
                time.sleep(2)  # Simulation travail
        
        # Analyse Ã©volution
        self._analyze_improvement_trajectory(reports)
        
        return reports
    
    def _analyze_improvement_trajectory(self, reports: List[IntegratedReport]):
        """Analyse trajectoire d'amÃ©lioration"""
        
        print(f"\nğŸ“ˆ ANALYSE TRAJECTOIRE AMÃ‰LIORATION")
        print("=" * 40)
        
        scores = [r.integrated_score for r in reports]
        
        if len(scores) >= 2:
            initial_score = scores[0]
            final_score = scores[-1]
            improvement = final_score - initial_score
            
            print(f"ğŸ“Š Score initial: {initial_score:.3f}")
            print(f"ğŸ“Š Score final: {final_score:.3f}")
            print(f"ğŸ“ˆ AmÃ©lioration: {improvement:+.3f}")
            
            if improvement > 0.05:
                trend = "EXCELLENT - AmÃ©lioration forte â­â­â­"
            elif improvement > 0.02:
                trend = "BON - AmÃ©lioration modÃ©rÃ©e â­â­"
            elif improvement > 0.00:
                trend = "FAIBLE - AmÃ©lioration marginale â­"
            else:
                trend = "STAGNANT - RÃ©vision stratÃ©gie nÃ©cessaire âš ï¸"
            
            print(f"ğŸ¯ Tendance: {trend}")

def main():
    """SystÃ¨me principal intÃ©grÃ©"""
    
    print("ğŸš€ INITIALISATION SYSTÃˆME INTÃ‰GRÃ‰ VALIDATION & QUALITÃ‰")
    print("=" * 60)
    
    # CrÃ©ation systÃ¨me intÃ©grÃ©
    integrated_system = IntegratedValidationQualitySystem()
    
    # Validation intÃ©grÃ©e baseline
    baseline_report = integrated_system.run_integrated_validation("system_baseline")
    
    print(f"\nğŸ’¾ SYSTÃˆME INTÃ‰GRÃ‰ OPÃ‰RATIONNEL")
    print(f"ğŸ“ RÃ©pertoire: {integrated_system.integration_dir}")
    print(f"ğŸ“Š Historique: {integrated_system.integrated_history}")
    print(f"ğŸ“‹ Rapports: {integrated_system.reports_dir}")
    
    print(f"\nâœ¨ PROCESSUS DE TRAVAIL Ã‰TABLI:")
    print(f"   1. Modifications modÃ¨le â†’ Validation automatique")
    print(f"   2. Ã‰valuation qualitÃ© â†’ Identification lacunes")
    print(f"   3. Roadmap intÃ©grÃ©e â†’ Priorisation amÃ©liorations")
    print(f"   4. Application amÃ©liorations â†’ Nouvelle validation")
    
    return integrated_system, baseline_report

if __name__ == "__main__":
    main()