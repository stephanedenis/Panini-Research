#!/usr/bin/env python3
"""
INT√âGRATEUR SYST√àMES VALIDATION & QUALIT√â
========================================

Int√©gration des syst√®mes de validation continue et framework qualit√©
pour un processus unifi√© de d√©veloppement et am√©lioration PanLang.

Objectif: Pipeline automatique validation ‚Üí analyse ‚Üí am√©lioration ‚Üí validation
"""

import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path

# Imports des syst√®mes
from systeme_validation_continue import ContinuousValidationSystem
from framework_criteres_qualite import PanLangQualityFramework

@dataclass
class IntegratedReport:
    """Rapport int√©gr√© validation + qualit√©"""
    timestamp: str
    validation_results: Dict[str, Any]
    quality_assessment: Dict[str, Any]
    integrated_score: float
    priority_actions: List[str]
    improvement_roadmap: Dict[str, Any]
    next_validation_trigger: str

class IntegratedValidationQualitySystem:
    """Syst√®me int√©gr√© validation continue + framework qualit√©"""
    
    def __init__(self):
        self.workspace_path = Path("/home/stephane/GitHub/PaniniFS-Research")
        self.integration_dir = self.workspace_path / "validation_integree"
        self.integration_dir.mkdir(exist_ok=True)
        
        # Initialisation sous-syst√®mes
        self.validator = ContinuousValidationSystem()
        self.quality_framework = PanLangQualityFramework()
        
        # Correction poids framework qualit√©
        self._fix_quality_framework_weights()
        
        # Historique int√©gr√©
        self.integrated_history = self.integration_dir / "historique_integre.jsonl"
        self.reports_dir = self.integration_dir / "rapports"
        self.reports_dir.mkdir(exist_ok=True)
    
    def _fix_quality_framework_weights(self):
        """Corrige les poids du framework qualit√© pour sommer √† 1.0"""
        
        total_weight = sum(c.weight for c in self.quality_framework.quality_criteria.values())
        correction_factor = 1.0 / total_weight
        
        print(f"üîß CORRECTION POIDS: facteur {correction_factor:.3f}")
        
        for criterion in self.quality_framework.quality_criteria.values():
            criterion.weight *= correction_factor
        
        # V√©rification
        new_total = sum(c.weight for c in self.quality_framework.quality_criteria.values())
        print(f"‚úÖ Nouveau total poids: {new_total:.6f}")
    
    def run_integrated_validation(self, trigger: str = "manual") -> IntegratedReport:
        """Ex√©cute validation int√©gr√©e compl√®te"""
        
        print("üîÑ VALIDATION INT√âGR√âE PANLANG")
        print("=" * 35)
        
        timestamp = datetime.now().isoformat()
        
        # 1. Validation continue technique
        print("üìä Phase 1: Validation continue...")
        validation_metrics = self.validator.validate_current_architecture(trigger)
        
        # 2. √âvaluation qualit√© framework
        print("\nüéØ Phase 2: √âvaluation framework qualit√©...")
        
        # R√©cup√©ration mod√®le actuel
        current_model = self.quality_framework.benchmark_models["panlang_panksepp_actuel"]
        quality_assessment = self.quality_framework.evaluate_model_candidate(current_model)
        
        # 3. Int√©gration scores
        print("\nüîó Phase 3: Int√©gration r√©sultats...")
        integrated_score = self._calculate_integrated_score(validation_metrics, quality_assessment)
        
        # 4. Analyse priorit√©s
        priority_actions = self._analyze_integrated_priorities(validation_metrics, quality_assessment)
        
        # 5. Roadmap am√©lioration
        improvement_roadmap = self._generate_integrated_roadmap(validation_metrics, quality_assessment)
        
        # 6. D√©clencheur suivant
        next_trigger = self._determine_next_trigger(integrated_score, improvement_roadmap)
        
        # Cr√©ation rapport int√©gr√©
        integrated_report = IntegratedReport(
            timestamp=timestamp,
            validation_results=asdict(validation_metrics),
            quality_assessment=asdict(quality_assessment),
            integrated_score=integrated_score,
            priority_actions=priority_actions,
            improvement_roadmap=improvement_roadmap,
            next_validation_trigger=next_trigger
        )
        
        # Sauvegarde et affichage
        self._save_integrated_report(integrated_report)
        self._display_integrated_results(integrated_report)
        
        return integrated_report
    
    def _calculate_integrated_score(self, validation_metrics, quality_assessment) -> float:
        """Calcule score int√©gr√© validation + qualit√©"""
        
        # Score validation continue (0.764 actuel)
        validation_score = validation_metrics.overall_quality_score
        
        # Score qualit√© framework (normalis√© √† 1.0)
        quality_score = min(1.0, quality_assessment.overall_score)
        
        # Moyenne pond√©r√©e (60% technique, 40% qualit√©)
        integrated_score = validation_score * 0.6 + quality_score * 0.4
        
        return integrated_score
    
    def _analyze_integrated_priorities(self, validation_metrics, quality_assessment) -> List[str]:
        """Analyse priorit√©s int√©gr√©es"""
        
        priorities = []
        
        # Priorit√©s validation continue
        if validation_metrics.multilingual_coverage < 0.6:
            priorities.append("CRITIQUE: √âtendre corpus multilingue (couverture 50%)")
        
        if validation_metrics.cross_linguistic_consistency < 0.75:
            priorities.append("HAUTE: Am√©liorer consistance inter-lingue (70%)")
        
        if validation_metrics.semantic_precision < 0.85:
            priorities.append("HAUTE: Affiner pr√©cision s√©mantique (88.1%)")
        
        # Priorit√©s framework qualit√©  
        quality_gaps = []
        for criterion_name, score in quality_assessment.criterion_scores.items():
            criterion = self.quality_framework.quality_criteria[criterion_name]
            if score < criterion.target_value - 0.05:
                gap = criterion.target_value - score
                weight = criterion.weight
                quality_gaps.append((gap * weight, criterion_name, criterion.name, gap))
        
        quality_gaps.sort(reverse=True)
        
        for weighted_gap, criterion_key, criterion_name, gap in quality_gaps[:3]:
            priorities.append(f"QUALIT√â: {criterion_name} (√©cart {gap:.3f})")
        
        return priorities[:8]  # Top 8 priorit√©s
    
    def _generate_integrated_roadmap(self, validation_metrics, quality_assessment) -> Dict[str, Any]:
        """G√©n√®re roadmap int√©gr√©e"""
        
        current_integrated = self._calculate_integrated_score(validation_metrics, quality_assessment)
        
        # Objectifs par domaine
        validation_targets = {
            'multilingual_coverage': 0.85,
            'semantic_precision': 0.90,
            'cross_linguistic_consistency': 0.85,
            'creative_generation_score': 0.80
        }
        
        quality_targets = {
            'universalite_linguistique': 0.90,
            'precision_semantique': 0.92,
            'contraintes_cognitives': 0.95
        }
        
        # Timeline bas√©e sur √©carts
        total_validation_gap = sum(
            max(0, target - getattr(validation_metrics, field))
            for field, target in validation_targets.items()
        )
        
        total_quality_gap = sum(
            max(0, target - quality_assessment.criterion_scores.get(field, 0.5))
            for field, target in quality_targets.items()
        )
        
        if total_validation_gap + total_quality_gap < 0.3:
            timeline = "2-3 semaines"
            effort = "Optimisation fine"
        elif total_validation_gap + total_quality_gap < 0.6:
            timeline = "1-2 mois"  
            effort = "Am√©liorations cibl√©es"
        else:
            timeline = "2-3 mois"
            effort = "D√©veloppements majeurs"
        
        roadmap = {
            'current_integrated_score': current_integrated,
            'target_integrated_score': 0.90,
            'estimated_timeline': timeline,
            'effort_level': effort,
            'validation_priorities': validation_targets,
            'quality_priorities': quality_targets,
            'success_criteria': [
                'Score int√©gr√© > 0.90',
                'Couverture multilingue > 85%',
                'Pr√©cision s√©mantique > 90%',
                'Tous crit√®res qualit√© au-dessus seuils'
            ]
        }
        
        return roadmap
    
    def _determine_next_trigger(self, integrated_score: float, roadmap: Dict[str, Any]) -> str:
        """D√©termine d√©clencheur validation suivante"""
        
        if integrated_score < 0.7:
            return "architecture_critical_improvement"
        elif integrated_score < 0.8:
            return "targeted_improvements"
        else:
            return "optimization_cycle"
    
    def _save_integrated_report(self, report: IntegratedReport):
        """Sauvegarde rapport int√©gr√©"""
        
        # Historique JSONL
        with open(self.integrated_history, 'a', encoding='utf-8') as f:
            f.write(json.dumps(asdict(report), ensure_ascii=False) + '\n')
        
        # Rapport d√©taill√©
        timestamp_str = report.timestamp.replace(':', '-').replace('T', '_')[:19]
        detailed_report_path = self.reports_dir / f"rapport_integre_{timestamp_str}.json"
        
        with open(detailed_report_path, 'w', encoding='utf-8') as f:
            json.dump(asdict(report), f, indent=2, ensure_ascii=False)
        
        # Rapport Markdown lisible
        md_report_path = self.reports_dir / f"rapport_integre_{timestamp_str}.md"
        self._generate_markdown_report(report, md_report_path)
    
    def _generate_markdown_report(self, report: IntegratedReport, output_path: Path):
        """G√©n√®re rapport Markdown lisible"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(f"# RAPPORT VALIDATION INT√âGR√âE PANLANG\n\n")
            f.write(f"**Timestamp:** {report.timestamp}  \n")
            f.write(f"**Score Int√©gr√©:** {report.integrated_score:.3f}  \n")
            f.write(f"**Prochain D√©clencheur:** {report.next_validation_trigger}  \n\n")
            
            f.write(f"## üìä VALIDATION TECHNIQUE\n\n")
            validation = report.validation_results
            f.write(f"- **Score Global:** {validation['overall_quality_score']:.3f}\n")
            f.write(f"- **Couverture Multilingue:** {validation['multilingual_coverage']:.3f}\n")
            f.write(f"- **Pr√©cision S√©mantique:** {validation['semantic_precision']:.3f}\n")
            f.write(f"- **Coh√©rence Compositionnelle:** {validation['compositional_coherence']:.3f}\n\n")
            
            f.write(f"## üéØ √âVALUATION QUALIT√â\n\n")
            quality = report.quality_assessment
            f.write(f"- **Score Global:** {quality['overall_score']:.3f}\n")
            f.write(f"- **Forces:** {len(quality.get('strengths', []))}\n")
            f.write(f"- **Faiblesses:** {len(quality.get('weaknesses', []))}\n\n")
            
            f.write(f"## üöÄ PRIORIT√âS D'ACTION\n\n")
            for i, action in enumerate(report.priority_actions, 1):
                f.write(f"{i}. {action}\n")
            f.write(f"\n")
            
            f.write(f"## üó∫Ô∏è ROADMAP AM√âLIORATION\n\n")
            roadmap = report.improvement_roadmap
            f.write(f"- **Timeline:** {roadmap['estimated_timeline']}\n")
            f.write(f"- **Effort:** {roadmap['effort_level']}\n")
            f.write(f"- **Score Cible:** {roadmap['target_integrated_score']:.3f}\n\n")
    
    def _display_integrated_results(self, report: IntegratedReport):
        """Affiche r√©sultats int√©gr√©s"""
        
        print(f"\nüéä R√âSULTATS VALIDATION INT√âGR√âE")
        print("=" * 40)
        
        print(f"üèÜ Score Int√©gr√©: {report.integrated_score:.3f}")
        
        if report.integrated_score >= 0.9:
            level = "EXCELLENCE ‚≠ê‚≠ê‚≠ê‚≠ê"
        elif report.integrated_score >= 0.8:
            level = "TR√àS BON ‚≠ê‚≠ê‚≠ê"
        elif report.integrated_score >= 0.7:
            level = "BON ‚≠ê‚≠ê"
        else:
            level = "√Ä AM√âLIORER ‚≠ê"
        
        print(f"üéñÔ∏è Niveau: {level}")
        
        print(f"\nüö® TOP PRIORIT√âS:")
        for i, priority in enumerate(report.priority_actions[:5], 1):
            print(f"   {i}. {priority}")
        
        print(f"\nüó∫Ô∏è ROADMAP:")
        roadmap = report.improvement_roadmap
        print(f"   ‚è±Ô∏è Timeline: {roadmap['estimated_timeline']}")
        print(f"   üéØ Score cible: {roadmap['target_integrated_score']:.3f}")
        print(f"   üìà √âcart: {roadmap['target_integrated_score'] - report.integrated_score:.3f}")
        
        print(f"\nüîÑ Prochain d√©clencheur: {report.next_validation_trigger}")
    
    def monitor_continuous_improvement(self, iterations: int = 5) -> List[IntegratedReport]:
        """Monitoring am√©lioration continue"""
        
        print("üîÑ MONITORING AM√âLIORATION CONTINUE")
        print("=" * 40)
        
        reports = []
        
        for i in range(iterations):
            print(f"\nüìã IT√âRATION {i+1}/{iterations}")
            
            trigger = "continuous_monitoring" if i > 0 else "baseline"
            report = self.run_integrated_validation(trigger)
            
            reports.append(report)
            
            # Simulation am√©lioration (en r√©alit√©, changements seraient appliqu√©s)
            if i < iterations - 1:
                print(f"‚è≥ Simulation application am√©liorations...")
                time.sleep(2)  # Simulation travail
        
        # Analyse √©volution
        self._analyze_improvement_trajectory(reports)
        
        return reports
    
    def _analyze_improvement_trajectory(self, reports: List[IntegratedReport]):
        """Analyse trajectoire d'am√©lioration"""
        
        print(f"\nüìà ANALYSE TRAJECTOIRE AM√âLIORATION")
        print("=" * 40)
        
        scores = [r.integrated_score for r in reports]
        
        if len(scores) >= 2:
            initial_score = scores[0]
            final_score = scores[-1]
            improvement = final_score - initial_score
            
            print(f"üìä Score initial: {initial_score:.3f}")
            print(f"üìä Score final: {final_score:.3f}")
            print(f"üìà Am√©lioration: {improvement:+.3f}")
            
            if improvement > 0.05:
                trend = "EXCELLENT - Am√©lioration forte ‚≠ê‚≠ê‚≠ê"
            elif improvement > 0.02:
                trend = "BON - Am√©lioration mod√©r√©e ‚≠ê‚≠ê"
            elif improvement > 0.00:
                trend = "FAIBLE - Am√©lioration marginale ‚≠ê"
            else:
                trend = "STAGNANT - R√©vision strat√©gie n√©cessaire ‚ö†Ô∏è"
            
            print(f"üéØ Tendance: {trend}")

def main():
    """Syst√®me principal int√©gr√©"""
    
    print("üöÄ INITIALISATION SYST√àME INT√âGR√â VALIDATION & QUALIT√â")
    print("=" * 60)
    
    # Cr√©ation syst√®me int√©gr√©
    integrated_system = IntegratedValidationQualitySystem()
    
    # Validation int√©gr√©e baseline
    baseline_report = integrated_system.run_integrated_validation("system_baseline")
    
    print(f"\nüíæ SYST√àME INT√âGR√â OP√âRATIONNEL")
    print(f"üìÅ R√©pertoire: {integrated_system.integration_dir}")
    print(f"üìä Historique: {integrated_system.integrated_history}")
    print(f"üìã Rapports: {integrated_system.reports_dir}")
    
    print(f"\n‚ú® PROCESSUS DE TRAVAIL √âTABLI:")
    print(f"   1. Modifications mod√®le ‚Üí Validation automatique")
    print(f"   2. √âvaluation qualit√© ‚Üí Identification lacunes")
    print(f"   3. Roadmap int√©gr√©e ‚Üí Priorisation am√©liorations")
    print(f"   4. Application am√©liorations ‚Üí Nouvelle validation")
    
    return integrated_system, baseline_report

if __name__ == "__main__":
    main()