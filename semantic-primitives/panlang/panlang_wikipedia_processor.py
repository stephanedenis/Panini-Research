#!/usr/bin/env python3
"""
DÃ‰COMPRESSEUR ET ANALYSEUR WIKIPEDIA POUR PANLANG
=================================================

DÃ©compresse et analyse tous les dumps Wikipedia pour crÃ©er PanLang,
la langue Ã©rudite universelle qui infuse toute la connaissance humaine.

PanLang forge une intuition correcte pour son Ã©poque en intÃ©grant :
- Racines dhÄtu sanskrites originelles
- Classifications taxonomiques exhaustives
- Concepts scientifiques multilingues
- Sagesse culturelle pan-mondiale
"""

import bz2
import gzip
import xml.etree.ElementTree as ET
import json
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple
import concurrent.futures
import time
from dataclasses import dataclass, asdict
import sqlite3

@dataclass
class ConceptPrimitive:
    """Primitive conceptuelle extraite de Wikipedia"""
    terme_original: str
    langue_source: str
    domaine: str
    hierarchie: List[str]
    proprietes: Dict[str, str]
    racines_etymologiques: List[str]
    certitude_score: float
    occurrences_multilingues: Dict[str, str]

class PanLangWikipediaProcessor:
    """Processeur pour crÃ©er PanLang Ã  partir de Wikipedia"""
    
    def __init__(self, dumps_dir: str = "wikipedia_dumps"):
        self.dumps_dir = Path(dumps_dir)
        self.output_dir = Path("panlang_primitives")
        self.output_dir.mkdir(exist_ok=True)
        
        # Base de donnÃ©es pour primitives
        self.db_path = self.output_dir / "panlang_primitives.db"
        self.init_database()
        
        # Patterns d'extraction
        self.extraction_patterns = self._init_extraction_patterns()
        
        # Statistiques
        self.stats = {
            "files_processed": 0,
            "primitives_extracted": 0,
            "languages_covered": set(),
            "domains_covered": set()
        }
    
    def init_database(self):
        """Initialise la base de donnÃ©es SQLite pour les primitives"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS primitives (
            id INTEGER PRIMARY KEY,
            terme_original TEXT,
            langue_source TEXT,
            domaine TEXT,
            hierarchie TEXT,  -- JSON
            proprietes TEXT,  -- JSON
            racines_etymologiques TEXT,  -- JSON
            certitude_score REAL,
            occurrences_multilingues TEXT,  -- JSON
            date_extraction TEXT
        )
        """)
        
        cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_domaine ON primitives(domaine)
        """)
        
        cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_langue ON primitives(langue_source)
        """)
        
        conn.commit()
        conn.close()
    
    def _init_extraction_patterns(self) -> Dict[str, Dict]:
        """Patterns d'extraction par domaine et par langue"""
        return {
            # BIOLOGIE - Taxonomies universelles
            "taxonomie": {
                "patterns": [
                    r"\{\{Taxobox",
                    r"\{\{Infobox.*biolog",  
                    r"regne\s*=\s*([^|\n]+)",
                    r"kingdom\s*=\s*([^|\n]+)",
                    r"ĞºĞ»Ğ°ÑÑ\s*=\s*([^|\n]+)",  # Russe
                    r"ç•Œ\s*=\s*([^|\n]+)",      # Chinois
                ],
                "hierarchie": ["regne", "embranchement", "classe", "ordre", "famille", "genre", "espece"]
            },
            
            # LINGUISTIQUE - Racines Ã©tymologiques
            "etymologie": {
                "patterns": [
                    r"Ã©tymologie.*?([A-Za-z\u0900-\u097F\u0100-\u017F]+)",  # Sanskrit/Latin
                    r"racine.*?([A-Za-z\u0900-\u097F]+)",
                    r"à¤§à¤¾à¤¤à¥.*?([^\s]+)",  # DhÄtu sanskrit
                    r"root.*?([A-Za-z]+)",
                    r"è¯æ ¹.*?([^ã€‚]+)",    # Chinois
                ],
                "langues_anciennes": ["sa", "la", "el", "ar"]
            },
            
            # CHIMIE - Formules et propriÃ©tÃ©s
            "chimie": {
                "patterns": [
                    r"formule.*?([A-Z][a-z]*[0-9]*)",
                    r"formula.*?([A-Z][a-z]*[0-9]*)",
                    r"masse.*molaire.*?([0-9]+\.?[0-9]*)",
                    r"åŒ–å­¦å¼.*?([A-Z][a-z0-9]+)",  # Chinois
                ],
                "proprietes": ["masse_molaire", "point_fusion", "point_ebullition", "densite"]
            },
            
            # GÃ‰OGRAPHIE - Localisation hiÃ©rarchique
            "geographie": {
                "patterns": [
                    r"pays\s*=\s*([^|\n]+)",
                    r"country\s*=\s*([^|\n]+)",
                    r"continent\s*=\s*([^|\n]+)",
                    r"ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹\s*=\s*([^|\n]+)",  # Russe
                ],
                "hierarchie": ["continent", "pays", "region", "ville"]
            }
        }
    
    def decompress_files(self) -> Dict[str, List[Path]]:
        """DÃ©compresse tous les fichiers Wikipedia"""
        print("ğŸ—œï¸  DÃ‰COMPRESSION DES ARCHIVES WIKIPEDIA")
        print("=" * 50)
        
        decompressed = {"xml": [], "sql": []}
        
        for compressed_file in self.dumps_dir.glob("*.bz2"):
            print(f"ğŸ“¦ DÃ©compression: {compressed_file.name}")
            decompressed_path = compressed_file.with_suffix('')
            
            if not decompressed_path.exists():
                with bz2.open(compressed_file, 'rb') as src, \
                     open(decompressed_path, 'wb') as dst:
                    dst.write(src.read())
            
            if decompressed_path.suffix == '.xml':
                decompressed["xml"].append(decompressed_path)
            
        for compressed_file in self.dumps_dir.glob("*.gz"):
            print(f"ğŸ“¦ DÃ©compression: {compressed_file.name}")
            decompressed_path = compressed_file.with_suffix('')
            
            if not decompressed_path.exists():
                with gzip.open(compressed_file, 'rb') as src, \
                     open(decompressed_path, 'wb') as dst:
                    dst.write(src.read())
            
            if decompressed_path.suffix == '.sql':
                decompressed["sql"].append(decompressed_path)
                
        print(f"âœ… DÃ©compressÃ©: {len(decompressed['xml'])} XML + {len(decompressed['sql'])} SQL")
        return decompressed
    
    def extract_sanskrit_dhatus(self, sanskrit_xml: Path) -> List[ConceptPrimitive]:
        """Extrait les racines dhÄtu du Sanskrit - PRIORITÃ‰ ABSOLUE"""
        print("ğŸ•‰ï¸  EXTRACTION RACINES DHÄ€TU SANSKRITES")
        print("-" * 40)
        
        dhatus = []
        
        try:
            # Parser XML progressif pour gros fichiers
            context = ET.iterparse(sanskrit_xml, events=('start', 'end'))
            context = iter(context)
            event, root = next(context)
            
            page_count = 0
            dhatu_count = 0
            
            for event, elem in context:
                if event == 'end' and elem.tag.endswith('page'):
                    title = elem.find('.//title')
                    text = elem.find('.//text')
                    
                    if title is not None and text is not None:
                        title_text = title.text or ""
                        content = text.text or ""
                        
                        # DÃ©tection dhÄtu
                        if any(marker in content.lower() for marker in 
                               ['à¤§à¤¾à¤¤à¥', 'dhatu', 'racine', 'root', 'verb']):
                            
                            # Extraction patterns dhÄtu
                            dhatu_matches = re.findall(
                                r'à¤§à¤¾à¤¤à¥[:\s]*([^\s\nà¥¤]+)', content, re.IGNORECASE
                            )
                            
                            for dhatu_root in dhatu_matches:
                                primitive = ConceptPrimitive(
                                    terme_original=dhatu_root,
                                    langue_source="sa",
                                    domaine="etymologie_dhatu",
                                    hierarchie=["racine", "verbale", "sanskrit"],
                                    proprietes={
                                        "source_title": title_text,
                                        "context": content[:200] + "..."
                                    },
                                    racines_etymologiques=[dhatu_root],
                                    certitude_score=0.9,
                                    occurrences_multilingues={"sa": dhatu_root}
                                )
                                dhatus.append(primitive)
                                dhatu_count += 1
                        
                        page_count += 1
                        if page_count % 100 == 0:
                            print(f"   ğŸ“„ {page_count} pages, {dhatu_count} dhÄtu trouvÃ©s")
                    
                    # LibÃ©ration mÃ©moire
                    elem.clear()
                    
        except ET.ParseError as e:
            print(f"âš ï¸  Erreur parsing XML: {e}")
        
        print(f"âœ… Extraits: {dhatu_count} racines dhÄtu")
        return dhatus
    
    def extract_taxonomies(self, xml_files: List[Path]) -> List[ConceptPrimitive]:
        """Extrait les taxonomies biologiques multilingues"""
        print("ğŸ§¬ EXTRACTION TAXONOMIES BIOLOGIQUES")
        print("-" * 40)
        
        taxonomies = []
        
        for xml_file in xml_files:
            lang = xml_file.name[:2]  # Code langue
            print(f"   ğŸ” Langue: {lang}")
            
            # Extraction rapide par regex pour dÃ©mo
            try:
                with open(xml_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content_sample = f.read(10000000)  # 10MB Ã©chantillon
                    
                # Recherche Taxobox
                taxobox_matches = re.finditer(
                    r'\{\{Taxobox.*?(?=\{\{|\n\n)', 
                    content_sample, re.DOTALL | re.IGNORECASE
                )
                
                for match in list(taxobox_matches)[:50]:  # Limite pour dÃ©mo
                    taxobox = match.group(0)
                    
                    # Extraction hiÃ©rarchie
                    regne = re.search(r'regne\s*=\s*([^|\n]+)', taxobox, re.IGNORECASE)
                    classe = re.search(r'classe\s*=\s*([^|\n]+)', taxobox, re.IGNORECASE)
                    
                    if regne and classe:
                        primitive = ConceptPrimitive(
                            terme_original=regne.group(1).strip(),
                            langue_source=lang,
                            domaine="taxonomie",
                            hierarchie=["regne", regne.group(1).strip(), classe.group(1).strip()],
                            proprietes={"source": "taxobox"},
                            racines_etymologiques=[],
                            certitude_score=0.8,
                            occurrences_multilingues={lang: regne.group(1)}
                        )
                        taxonomies.append(primitive)
                        
            except Exception as e:
                print(f"âš ï¸  Erreur {xml_file.name}: {e}")
                
        return taxonomies
    
    def save_primitives(self, primitives: List[ConceptPrimitive]):
        """Sauvegarde les primitives dans la base de donnÃ©es"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for primitive in primitives:
            cursor.execute("""
            INSERT INTO primitives (
                terme_original, langue_source, domaine, hierarchie,
                proprietes, racines_etymologiques, certitude_score,
                occurrences_multilingues, date_extraction
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))
            """, (
                primitive.terme_original,
                primitive.langue_source, 
                primitive.domaine,
                json.dumps(primitive.hierarchie),
                json.dumps(primitive.proprietes),
                json.dumps(primitive.racines_etymologiques),
                primitive.certitude_score,
                json.dumps(primitive.occurrences_multilingues)
            ))
        
        conn.commit()
        conn.close()
        
        print(f"ğŸ’¾ SauvegardÃ©: {len(primitives)} primitives")
    
    def generate_panlang_report(self) -> Dict:
        """GÃ©nÃ¨re le rapport PanLang"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Statistiques par domaine
        cursor.execute("""
        SELECT domaine, COUNT(*), AVG(certitude_score)
        FROM primitives GROUP BY domaine
        """)
        domaines = cursor.fetchall()
        
        # Statistiques par langue
        cursor.execute("""
        SELECT langue_source, COUNT(*)
        FROM primitives GROUP BY langue_source
        """)
        langues = cursor.fetchall()
        
        conn.close()
        
        rapport = {
            "titre": "PANLANG - Langue Ã‰rudite Universelle",
            "description": "Primitives extraites de Wikipedia multilingue pour forge d'intuition correcte",
            "domaines": {domaine: {"count": count, "certitude_moy": round(cert, 3)} 
                        for domaine, count, cert in domaines},
            "langues": {lang: count for lang, count in langues},
            "total_primitives": sum(count for _, count in langues),
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # Sauvegarde rapport
        with open(self.output_dir / "panlang_report.json", "w", encoding="utf-8") as f:
            json.dump(rapport, f, indent=2, ensure_ascii=False)
            
        return rapport

def main():
    """Fonction principale - CrÃ©ation de PanLang"""
    print("ğŸŒ CRÃ‰ATION PANLANG - LANGUE Ã‰RUDITE UNIVERSELLE")
    print("=" * 60)
    print("Vision: Infuser toute la connaissance humaine")
    print("Objectif: Forger une intuition correcte pour l'Ã©poque")
    print()
    
    processor = PanLangWikipediaProcessor()
    
    # 1. DÃ©compression
    files = processor.decompress_files()
    
    # 2. Extraction prioritaire Sanskrit (dhÄtu)
    sanskrit_files = [f for f in files["xml"] if "sawiki" in f.name]
    if sanskrit_files:
        dhatus = processor.extract_sanskrit_dhatus(sanskrit_files[0])
        processor.save_primitives(dhatus)
        print(f"âœ… Racines dhÄtu intÃ©grÃ©es: {len(dhatus)}")
    
    # 3. Extraction taxonomies multilingues
    other_xml = [f for f in files["xml"] if "sawiki" not in f.name]
    if other_xml:
        taxonomies = processor.extract_taxonomies(other_xml[:3])  # 3 langues pour dÃ©mo
        processor.save_primitives(taxonomies)
        print(f"âœ… Taxonomies intÃ©grÃ©es: {len(taxonomies)}")
    
    # 4. Rapport final
    rapport = processor.generate_panlang_report()
    
    print("\nğŸ“Š RAPPORT PANLANG")
    print("-" * 20)
    print(f"ğŸ“š Total primitives: {rapport['total_primitives']}")
    print(f"ğŸŒ Langues couvertes: {len(rapport['langues'])}")
    print(f"ğŸ”¬ Domaines: {len(rapport['domaines'])}")
    
    for domaine, stats in rapport["domaines"].items():
        print(f"   {domaine}: {stats['count']} primitives (certitude: {stats['certitude_moy']})")
    
    print(f"\nğŸ’¾ Base de donnÃ©es: {processor.db_path}")
    print(f"ğŸ“„ Rapport complet: {processor.output_dir}/panlang_report.json")
    print("\nğŸ¯ PanLang - Fondations Ã©tablies pour la langue Ã©rudite universelle!")

if __name__ == "__main__":
    main()