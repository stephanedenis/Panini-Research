#!/usr/bin/env python3
"""
D√âCOMPRESSEUR ET ANALYSEUR WIKIPEDIA POUR PANLANG
=================================================

D√©compresse et analyse tous les dumps Wikipedia pour cr√©er PanLang,
la langue √©rudite universelle qui infuse toute la connaissance humaine.

PanLang forge une intuition correcte pour son √©poque en int√©grant :
- Racines dhƒÅtu sanskrites originelles
- Classifications taxonomiques exhaustives
- Concepts scientifiques multilingues
- Sagesse culturelle pan-mondiale
"""

import bz2
import gzip
import xml.etree.ElementTree as ET
import json
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple
import concurrent.futures
import time
from dataclasses import dataclass, asdict
import sqlite3

@dataclass
class ConceptPrimitive:
    """Primitive conceptuelle extraite de Wikipedia"""
    terme_original: str
    langue_source: str
    domaine: str
    hierarchie: List[str]
    proprietes: Dict[str, str]
    racines_etymologiques: List[str]
    certitude_score: float
    occurrences_multilingues: Dict[str, str]

class PanLangWikipediaProcessor:
    """Processeur pour cr√©er PanLang √† partir de Wikipedia"""
    
    def __init__(self, dumps_dir: str = "wikipedia_dumps"):
        self.dumps_dir = Path(dumps_dir)
        self.output_dir = Path("panlang_primitives")
        self.output_dir.mkdir(exist_ok=True)
        
        # Base de donn√©es pour primitives
        self.db_path = self.output_dir / "panlang_primitives.db"
        self.init_database()
        
        # Patterns d'extraction
        self.extraction_patterns = self._init_extraction_patterns()
        
        # Statistiques
        self.stats = {
            "files_processed": 0,
            "primitives_extracted": 0,
            "languages_covered": set(),
            "domains_covered": set()
        }
    
    def init_database(self):
        """Initialise la base de donn√©es SQLite pour les primitives"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS primitives (
            id INTEGER PRIMARY KEY,
            terme_original TEXT,
            langue_source TEXT,
            domaine TEXT,
            hierarchie TEXT,  -- JSON
            proprietes TEXT,  -- JSON
            racines_etymologiques TEXT,  -- JSON
            certitude_score REAL,
            occurrences_multilingues TEXT,  -- JSON
            date_extraction TEXT
        )
        """)
        
        cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_domaine ON primitives(domaine)
        """)
        
        cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_langue ON primitives(langue_source)
        """)
        
        conn.commit()
        conn.close()
    
    def _init_extraction_patterns(self) -> Dict[str, Dict]:
        """Patterns d'extraction par domaine et par langue"""
        return {
            # BIOLOGIE - Taxonomies universelles
            "taxonomie": {
                "patterns": [
                    r"\{\{Taxobox",
                    r"\{\{Infobox.*biolog",  
                    r"regne\s*=\s*([^|\n]+)",
                    r"kingdom\s*=\s*([^|\n]+)",
                    r"–∫–ª–∞—Å—Å\s*=\s*([^|\n]+)",  # Russe
                    r"Áïå\s*=\s*([^|\n]+)",      # Chinois
                ],
                "hierarchie": ["regne", "embranchement", "classe", "ordre", "famille", "genre", "espece"]
            },
            
            # LINGUISTIQUE - Racines √©tymologiques
            "etymologie": {
                "patterns": [
                    r"√©tymologie.*?([A-Za-z\u0900-\u097F\u0100-\u017F]+)",  # Sanskrit/Latin
                    r"racine.*?([A-Za-z\u0900-\u097F]+)",
                    r"‡§ß‡§æ‡§§‡•Å.*?([^\s]+)",  # DhƒÅtu sanskrit
                    r"root.*?([A-Za-z]+)",
                    r"ËØçÊ†π.*?([^„ÄÇ]+)",    # Chinois
                ],
                "langues_anciennes": ["sa", "la", "el", "ar"]
            },
            
            # CHIMIE - Formules et propri√©t√©s
            "chimie": {
                "patterns": [
                    r"formule.*?([A-Z][a-z]*[0-9]*)",
                    r"formula.*?([A-Z][a-z]*[0-9]*)",
                    r"masse.*molaire.*?([0-9]+\.?[0-9]*)",
                    r"ÂåñÂ≠¶Âºè.*?([A-Z][a-z0-9]+)",  # Chinois
                ],
                "proprietes": ["masse_molaire", "point_fusion", "point_ebullition", "densite"]
            },
            
            # G√âOGRAPHIE - Localisation hi√©rarchique
            "geographie": {
                "patterns": [
                    r"pays\s*=\s*([^|\n]+)",
                    r"country\s*=\s*([^|\n]+)",
                    r"continent\s*=\s*([^|\n]+)",
                    r"–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã\s*=\s*([^|\n]+)",  # Russe
                ],
                "hierarchie": ["continent", "pays", "region", "ville"]
            }
        }
    
    def decompress_files(self) -> Dict[str, List[Path]]:
        """D√©compresse tous les fichiers Wikipedia"""
        print("üóúÔ∏è  D√âCOMPRESSION DES ARCHIVES WIKIPEDIA")
        print("=" * 50)
        
        decompressed = {"xml": [], "sql": []}
        
        for compressed_file in self.dumps_dir.glob("*.bz2"):
            print(f"üì¶ D√©compression: {compressed_file.name}")
            decompressed_path = compressed_file.with_suffix('')
            
            if not decompressed_path.exists():
                with bz2.open(compressed_file, 'rb') as src, \
                     open(decompressed_path, 'wb') as dst:
                    dst.write(src.read())
            
            if decompressed_path.suffix == '.xml':
                decompressed["xml"].append(decompressed_path)
            
        for compressed_file in self.dumps_dir.glob("*.gz"):
            print(f"üì¶ D√©compression: {compressed_file.name}")
            decompressed_path = compressed_file.with_suffix('')
            
            if not decompressed_path.exists():
                with gzip.open(compressed_file, 'rb') as src, \
                     open(decompressed_path, 'wb') as dst:
                    dst.write(src.read())
            
            if decompressed_path.suffix == '.sql':
                decompressed["sql"].append(decompressed_path)
                
        print(f"‚úÖ D√©compress√©: {len(decompressed['xml'])} XML + {len(decompressed['sql'])} SQL")
        return decompressed
    
    def extract_sanskrit_dhatus(self, sanskrit_xml: Path) -> List[ConceptPrimitive]:
        """Extrait les racines dhƒÅtu du Sanskrit - PRIORIT√â ABSOLUE"""
        print("üïâÔ∏è  EXTRACTION RACINES DHƒÄTU SANSKRITES")
        print("-" * 40)
        
        dhatus = []
        
        try:
            # Parser XML progressif pour gros fichiers
            context = ET.iterparse(sanskrit_xml, events=('start', 'end'))
            context = iter(context)
            event, root = next(context)
            
            page_count = 0
            dhatu_count = 0
            
            for event, elem in context:
                if event == 'end' and elem.tag.endswith('page'):
                    title = elem.find('.//title')
                    text = elem.find('.//text')
                    
                    if title is not None and text is not None:
                        title_text = title.text or ""
                        content = text.text or ""
                        
                        # D√©tection dhƒÅtu
                        if any(marker in content.lower() for marker in 
                               ['‡§ß‡§æ‡§§‡•Å', 'dhatu', 'racine', 'root', 'verb']):
                            
                            # Extraction patterns dhƒÅtu
                            dhatu_matches = re.findall(
                                r'‡§ß‡§æ‡§§‡•Å[:\s]*([^\s\n‡•§]+)', content, re.IGNORECASE
                            )
                            
                            for dhatu_root in dhatu_matches:
                                primitive = ConceptPrimitive(
                                    terme_original=dhatu_root,
                                    langue_source="sa",
                                    domaine="etymologie_dhatu",
                                    hierarchie=["racine", "verbale", "sanskrit"],
                                    proprietes={
                                        "source_title": title_text,
                                        "context": content[:200] + "..."
                                    },
                                    racines_etymologiques=[dhatu_root],
                                    certitude_score=0.9,
                                    occurrences_multilingues={"sa": dhatu_root}
                                )
                                dhatus.append(primitive)
                                dhatu_count += 1
                        
                        page_count += 1
                        if page_count % 100 == 0:
                            print(f"   üìÑ {page_count} pages, {dhatu_count} dhƒÅtu trouv√©s")
                    
                    # Lib√©ration m√©moire
                    elem.clear()
                    
        except ET.ParseError as e:
            print(f"‚ö†Ô∏è  Erreur parsing XML: {e}")
        
        print(f"‚úÖ Extraits: {dhatu_count} racines dhƒÅtu")
        return dhatus
    
    def extract_taxonomies(self, xml_files: List[Path]) -> List[ConceptPrimitive]:
        """Extrait les taxonomies biologiques multilingues"""
        print("üß¨ EXTRACTION TAXONOMIES BIOLOGIQUES")
        print("-" * 40)
        
        taxonomies = []
        
        for xml_file in xml_files:
            lang = xml_file.name[:2]  # Code langue
            print(f"   üîç Langue: {lang}")
            
            # Extraction rapide par regex pour d√©mo
            try:
                with open(xml_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content_sample = f.read(10000000)  # 10MB √©chantillon
                    
                # Recherche Taxobox
                taxobox_matches = re.finditer(
                    r'\{\{Taxobox.*?(?=\{\{|\n\n)', 
                    content_sample, re.DOTALL | re.IGNORECASE
                )
                
                for match in list(taxobox_matches)[:50]:  # Limite pour d√©mo
                    taxobox = match.group(0)
                    
                    # Extraction hi√©rarchie
                    regne = re.search(r'regne\s*=\s*([^|\n]+)', taxobox, re.IGNORECASE)
                    classe = re.search(r'classe\s*=\s*([^|\n]+)', taxobox, re.IGNORECASE)
                    
                    if regne and classe:
                        primitive = ConceptPrimitive(
                            terme_original=regne.group(1).strip(),
                            langue_source=lang,
                            domaine="taxonomie",
                            hierarchie=["regne", regne.group(1).strip(), classe.group(1).strip()],
                            proprietes={"source": "taxobox"},
                            racines_etymologiques=[],
                            certitude_score=0.8,
                            occurrences_multilingues={lang: regne.group(1)}
                        )
                        taxonomies.append(primitive)
                        
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur {xml_file.name}: {e}")
                
        return taxonomies
    
    def save_primitives(self, primitives: List[ConceptPrimitive]):
        """Sauvegarde les primitives dans la base de donn√©es"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for primitive in primitives:
            cursor.execute("""
            INSERT INTO primitives (
                terme_original, langue_source, domaine, hierarchie,
                proprietes, racines_etymologiques, certitude_score,
                occurrences_multilingues, date_extraction
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))
            """, (
                primitive.terme_original,
                primitive.langue_source, 
                primitive.domaine,
                json.dumps(primitive.hierarchie),
                json.dumps(primitive.proprietes),
                json.dumps(primitive.racines_etymologiques),
                primitive.certitude_score,
                json.dumps(primitive.occurrences_multilingues)
            ))
        
        conn.commit()
        conn.close()
        
        print(f"üíæ Sauvegard√©: {len(primitives)} primitives")
    
    def generate_panlang_report(self) -> Dict:
        """G√©n√®re le rapport PanLang"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Statistiques par domaine
        cursor.execute("""
        SELECT domaine, COUNT(*), AVG(certitude_score)
        FROM primitives GROUP BY domaine
        """)
        domaines = cursor.fetchall()
        
        # Statistiques par langue
        cursor.execute("""
        SELECT langue_source, COUNT(*)
        FROM primitives GROUP BY langue_source
        """)
        langues = cursor.fetchall()
        
        conn.close()
        
        rapport = {
            "titre": "PANLANG - Langue √ârudite Universelle",
            "description": "Primitives extraites de Wikipedia multilingue pour forge d'intuition correcte",
            "domaines": {domaine: {"count": count, "certitude_moy": round(cert, 3)} 
                        for domaine, count, cert in domaines},
            "langues": {lang: count for lang, count in langues},
            "total_primitives": sum(count for _, count in langues),
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # Sauvegarde rapport
        with open(self.output_dir / "panlang_report.json", "w", encoding="utf-8") as f:
            json.dump(rapport, f, indent=2, ensure_ascii=False)
            
        return rapport

def main():
    """Fonction principale - Cr√©ation de PanLang"""
    print("üåç CR√âATION PANLANG - LANGUE √âRUDITE UNIVERSELLE")
    print("=" * 60)
    print("Vision: Infuser toute la connaissance humaine")
    print("Objectif: Forger une intuition correcte pour l'√©poque")
    print()
    
    processor = PanLangWikipediaProcessor()
    
    # 1. D√©compression
    files = processor.decompress_files()
    
    # 2. Extraction prioritaire Sanskrit (dhƒÅtu)
    sanskrit_files = [f for f in files["xml"] if "sawiki" in f.name]
    if sanskrit_files:
        dhatus = processor.extract_sanskrit_dhatus(sanskrit_files[0])
        processor.save_primitives(dhatus)
        print(f"‚úÖ Racines dhƒÅtu int√©gr√©es: {len(dhatus)}")
    
    # 3. Extraction taxonomies multilingues
    other_xml = [f for f in files["xml"] if "sawiki" not in f.name]
    if other_xml:
        taxonomies = processor.extract_taxonomies(other_xml[:3])  # 3 langues pour d√©mo
        processor.save_primitives(taxonomies)
        print(f"‚úÖ Taxonomies int√©gr√©es: {len(taxonomies)}")
    
    # 4. Rapport final
    rapport = processor.generate_panlang_report()
    
    print("\nüìä RAPPORT PANLANG")
    print("-" * 20)
    print(f"üìö Total primitives: {rapport['total_primitives']}")
    print(f"üåê Langues couvertes: {len(rapport['langues'])}")
    print(f"üî¨ Domaines: {len(rapport['domaines'])}")
    
    for domaine, stats in rapport["domaines"].items():
        print(f"   {domaine}: {stats['count']} primitives (certitude: {stats['certitude_moy']})")
    
    print(f"\nüíæ Base de donn√©es: {processor.db_path}")
    print(f"üìÑ Rapport complet: {processor.output_dir}/panlang_report.json")
    print("\nüéØ PanLang - Fondations √©tablies pour la langue √©rudite universelle!")

if __name__ == "__main__":
    main()