#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üîß CORRECTEUR DETECTION CORPUS - HILL CLIMBING PANLANG
=====================================================
R√©pare la d√©tection des corpus disponibles pour l'optimisation continue

PROBL√àME IDENTIFI√â:
- 32+ corpus JSON disponibles dans le workspace  
- Syst√®me d√©tecte 0 nouveaux corpus
- Patterns de recherche d√©faillants

SOLUTION:
- R√©vision compl√®te de la d√©tection
- Expansion des patterns de recherche
- Logique de filtrage am√©lior√©e
- Test imm√©diat et int√©gration

Auteur: Diagnostic & R√©paration Syst√®me PanLang
Date: 2025-09-27
"""

import json
import logging
from pathlib import Path
from typing import List, Set
import glob

class CorrecteurDetectionCorpus:
    """Correcteur pour la d√©tection de corpus disponibles"""
    
    def __init__(self):
        self.workspace_root = Path.cwd()
        self.corpus_integres = set()  # Pour simulation
        self.logger = self._setup_logger()
    
    def _setup_logger(self):
        """Configuration logging pour diagnostic"""
        logging.basicConfig(level=logging.INFO)
        return logging.getLogger(__name__)
    
    def detecter_tous_corpus_disponibles(self) -> List[str]:
        """D√©tection exhaustive de TOUS les corpus disponibles"""
        
        self.logger.info("üîç DIAGNOSTIC COMPLET - D√©tection corpus")
        
        # 1. PATTERNS EXHAUSTIFS - Plus large que l'original
        patterns_exhaustifs = [
            # Patterns originaux (possiblement d√©faillants)
            "corpus_*.json",
            "data/*corpus*.json", 
            "**/*corpus*.json",
            
            # Patterns suppl√©mentaires bas√©s sur file_search
            "**/corpus.json",                    # Fichiers nomm√©s simplement "corpus.json"
            "**/corpus_*.json",                  # Variantes avec pr√©fixes
            "**/*_corpus_*.json",               # Variantes avec corpus au milieu
            "**/integration_corpus_*.json",     # Corpus d'int√©gration sp√©cifiques
            "**/scientific_corpus_*.json",      # Corpus scientifiques
            "**/toy_corpus.json",               # Corpus de test
            
            # Donn√©es brutes d√©tect√©es
            "data/*.json",                      # Tous JSON dans data/
            "**/data/*.json",                   # Tous JSON dans sous-dossiers data/
            "PaniniFS-Research/**/*.json",      # Sous-r√©pertoire PaniniFS-Research
            "tech/**/*.json",                   # R√©pertoire technique
            
            # Formats alternatifs
            "*.txt",                            # Fichiers texte
            "data/*.txt",                       # Textes dans data/
            "**/*.md",                          # Markdown avec potentiel contenu
            
            # Patterns sp√©cialis√©s d√©couverts
            "**/*multilingue*.json",
            "**/*scientifique*.json", 
            "**/*prescolaire*.json",
            "**/*unifie*.json",
            "expansion_*/**/*.json"
        ]
        
        tous_corpus = []
        corpus_par_pattern = {}
        
        for pattern in patterns_exhaustifs:
            corpus_pattern = []
            
            try:
                # Utilisation de glob pour recherche r√©cursive
                chemins_trouves = list(self.workspace_root.glob(pattern))
                
                for chemin in chemins_trouves:
                    if self._est_corpus_valide(chemin):
                        chemin_str = str(chemin)
                        if chemin_str not in tous_corpus:
                            tous_corpus.append(chemin_str)
                            corpus_pattern.append(chemin_str)
                
                if corpus_pattern:
                    corpus_par_pattern[pattern] = len(corpus_pattern)
                    
            except Exception as e:
                self.logger.error(f"‚ùå Erreur pattern {pattern}: {e}")
        
        # 2. RAPPORT DIAGNOSTIC D√âTAILL√â
        self.logger.info(f"üìä R√âSULTATS D√âTECTION:")
        self.logger.info(f"   üéØ Total corpus trouv√©s: {len(tous_corpus)}")
        
        for pattern, nb in corpus_par_pattern.items():
            self.logger.info(f"   üìÅ {pattern}: {nb} corpus")
        
        # 3. AFFICHAGE DES PREMIERS CORPUS TROUV√âS
        self.logger.info(f"üîç √âCHANTILLON CORPUS D√âTECT√âS:")
        for i, corpus in enumerate(tous_corpus[:10]):
            self.logger.info(f"   {i+1:2d}. {Path(corpus).name} ({Path(corpus).parent})")
        
        if len(tous_corpus) > 10:
            self.logger.info(f"   ... et {len(tous_corpus) - 10} autres")
        
        return tous_corpus
    
    def _est_corpus_valide(self, chemin: Path) -> bool:
        """Validation am√©lior√©e d'un corpus"""
        
        try:
            if not chemin.is_file():
                return False
            
            # Filtrage des fichiers syst√®me/temporaires
            nom_fichier = chemin.name.lower()
            if any(exclusion in nom_fichier for exclusion in [
                'node_modules', '.git', '__pycache__', '.pytest_cache',
                'package-lock', '.log', '.tmp', '.cache'
            ]):
                return False
            
            # Validation JSON
            if chemin.suffix == '.json':
                with open(chemin, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Corpus valide = dict avec contenu substantiel
                if isinstance(data, dict) and len(data) >= 3:
                    # V√©rification contenu non-vide
                    contenu_total = sum(len(str(v)) for v in data.values() if isinstance(v, (str, dict, list)))
                    return contenu_total > 100  # Minimum 100 caract√®res de contenu
            
            # Validation TXT/MD
            elif chemin.suffix in ['.txt', '.md']:
                taille = chemin.stat().st_size
                return 500 <= taille <= 10_000_000  # Entre 500 bytes et 10MB
            
            return False
            
        except Exception as e:
            self.logger.debug(f"‚ö†Ô∏è Erreur validation {chemin}: {e}")
            return False
    
    def simuler_detection_originale(self) -> List[str]:
        """Simulation de la d√©tection originale d√©faillante"""
        
        self.logger.info("üîÑ SIMULATION D√âTECTION ORIGINALE")
        
        # Code original approximatif
        sources_potentielles = [
            "corpus_*.json",
            "data/*corpus*.json", 
            "**/*corpus*.json",
            "*.txt",
            "data/*.txt",
            "corpus_*/**/*.json"
        ]
        
        nouveaux_corpus = []
        workspace_root = Path.cwd()
        
        for pattern in sources_potentielles:
            for chemin in workspace_root.glob(pattern):
                if chemin.is_file() and str(chemin) not in self.corpus_integres:
                    try:
                        if chemin.suffix == '.json':
                            with open(chemin, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                if isinstance(data, dict) and len(data) > 5:
                                    nouveaux_corpus.append(str(chemin))
                        elif chemin.suffix == '.txt':
                            if chemin.stat().st_size > 1000:
                                nouveaux_corpus.append(str(chemin))
                    except:
                        continue
        
        self.logger.info(f"üìä Simulation originale: {len(nouveaux_corpus)} corpus")
        return nouveaux_corpus[:5]
    
    def comparer_detections(self):
        """Comparaison entre d√©tection exhaustive et originale"""
        
        print("üÜö COMPARAISON M√âTHODES D√âTECTION")
        print("="*50)
        
        # D√©tection exhaustive
        corpus_exhaustifs = self.detecter_tous_corpus_disponibles()
        
        # Simulation originale  
        corpus_originaux = self.simuler_detection_originale()
        
        print(f"üìä R√âSULTATS:")
        print(f"   üîç D√©tection exhaustive: {len(corpus_exhaustifs)} corpus")
        print(f"   üîÑ Simulation originale: {len(corpus_originaux)} corpus")
        print(f"   üìà Diff√©rence: +{len(corpus_exhaustifs) - len(corpus_originaux)} corpus")
        
        # Analyse de la diff√©rence
        if len(corpus_exhaustifs) > len(corpus_originaux):
            print(f"\n‚úÖ PROBL√àME IDENTIFI√â: D√©tection originale TROP RESTRICTIVE")
            print(f"üí° SOLUTION: Utiliser d√©tection exhaustive")
        else:
            print(f"\n‚ùì Probl√®me non √©vident - investigation approfondie requise")
        
        return corpus_exhaustifs, corpus_originaux
    
    def generer_correcteur_integration(self, corpus_disponibles: List[str]):
        """G√©n√®re le code correcteur pour l'int√©gration"""
        
        code_correcteur = f'''
def detecter_nouveaux_corpus_CORRIGE(self) -> List[str]:
    """D√©tection CORRIG√âE des nouveaux corpus disponibles"""
    
    # PATTERNS EXHAUSTIFS (vs originaux restrictifs)
    patterns_exhaustifs = [
        "**/*corpus*.json",     # R√©cursif tous corpus
        "**/corpus.json",       # Fichiers corpus simples  
        "data/**/*.json",       # Tous JSON dans data/
        "**/*multilingue*.json", # Corpus sp√©cialis√©s
        "**/*scientifique*.json",
        "**/*prescolaire*.json",
        "**/*unifie*.json",
        "tech/**/*.json",       # Corpus techniques
        "*.txt",                # Fichiers texte racine
        "data/*.txt"            # Textes dans data/
    ]
    
    nouveaux_corpus = []
    workspace_root = Path.cwd()
    
    for pattern in patterns_exhaustifs:
        for chemin in workspace_root.glob(pattern):
            if chemin.is_file() and str(chemin) not in self.corpus_integres:
                if self._valider_corpus_ameliore(chemin):
                    nouveaux_corpus.append(str(chemin))
    
    # Limite raisonnable pour √©viter surcharge
    return nouveaux_corpus[:15]  # vs 5 original

def _valider_corpus_ameliore(self, chemin: Path) -> bool:
    """Validation am√©lior√©e vs originale trop stricte"""
    
    try:
        # Exclusions syst√®me
        if any(excl in str(chemin) for excl in ['node_modules', '.git', '__pycache__']):
            return False
            
        if chemin.suffix == '.json':
            with open(chemin, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Seuil abaiss√©: 3 vs 5 original
                return isinstance(data, dict) and len(data) >= 3
                
        elif chemin.suffix == '.txt':
            # Seuil abaiss√©: 500 vs 1000 original  
            return chemin.stat().st_size >= 500
            
        return False
        
    except:
        return False

# üéØ CORPUS IMM√âDIATEMENT DISPONIBLES:
# {len(corpus_disponibles)} corpus d√©tect√©s !
# 
# Premiers 10:
{chr(10).join(f"# - {Path(c).name}" for c in corpus_disponibles[:10])}
'''
        
        print("üîß CODE CORRECTEUR G√âN√âR√â:")
        print(code_correcteur)
        
        return code_correcteur
    
    def recommandations_integration(self):
        """Recommandations pour corriger le syst√®me hill-climbing"""
        
        print(f"\nüí° RECOMMANDATIONS CORRECTION:")
        print(f"="*40)
        print(f"1. üîÑ REMPLACER detecter_nouveaux_corpus() dans optimiseur")
        print(f"2. üìà UTILISER patterns exhaustifs vs restrictifs") 
        print(f"3. üéØ ABAISSER seuils validation (3 vs 5, 500 vs 1000)")
        print(f"4. üöÄ RED√âMARRER optimisation avec correction")
        print(f"5. ‚úÖ V√âRIFIER int√©gration effective des nouveaux corpus")

def main():
    """Test du correcteur de d√©tection"""
    
    correcteur = CorrecteurDetectionCorpus()
    
    print("üîß DIAGNOSTIC COMPLET - D√âTECTION CORPUS")
    print("="*50)
    
    # Comparaison des m√©thodes
    corpus_exhaustifs, corpus_originaux = correcteur.comparer_detections()
    
    # G√©n√©ration du correcteur
    correcteur.generer_correcteur_integration(corpus_exhaustifs)
    
    # Recommandations
    correcteur.recommandations_integration()
    
    print(f"\nüéØ R√âSUM√â:")
    print(f"   Problem: Syst√®me d√©tecte 0 corpus sur {len(corpus_exhaustifs)} disponibles")
    print(f"   Cause: Patterns restrictifs + seuils trop √©lev√©s")
    print(f"   Solution: Code correcteur g√©n√©r√© ‚òùÔ∏è")

if __name__ == "__main__":
    main()